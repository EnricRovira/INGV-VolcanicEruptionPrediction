{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 1. Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers, metrics, losses, optimizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import imageio\n",
    "from skimage import color, io\n",
    "\n",
    "import albumentations\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 2. Paths & Global Variables\n",
    "\n",
    "## 2.1 Paths\n",
    "\n",
    "path = '../../01_Data/'\n",
    "path_sequences = path + '01_GeneratedSequences/'\n",
    "path_spectograms = path + '02_GeneratedSpectograms_MelFeatures/'\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(path + 'train.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv') \n",
    "\n",
    "train_paths = glob.glob(path + 'train/*')\n",
    "test_paths = glob.glob(path + 'test/*')\n",
    "\n",
    "unique_segments_id_train = set(df_train['segment_id'])\n",
    "unique_segments_id_test = set(df_sample_submission['segment_id'])\n",
    "\n",
    "dict_unique_segments_id = { v : k for k, v in enumerate(unique_segments_id_train)}\n",
    "dict_unique_segments_id_inv = { k : v for k, v in enumerate(unique_segments_id_train)}\n",
    "\n",
    "## 2.2 Global Variables\n",
    "\n",
    "SEQ_LENGTH = 60_001\n",
    "IMG_SIZE = (40, 118)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 3. Global Functions\n",
    "\n",
    "\n",
    "def scale(x, mean_, std_):\n",
    "    return (x - mean_) / std_\n",
    "\n",
    "\n",
    "def unscale(x, mean_, std_):\n",
    "    return (x * std_) + mean_\n",
    "\n",
    "def getdDictsSpectoGramsNulls(dict_segments_paths):\n",
    "    dict_results = {}\n",
    "    for segment in tqdm(dict_segments_paths, total=len(dict_segments_paths), position=0):\n",
    "        df = pd.read_csv(dict_segments_paths[segment])\n",
    "        dict_results[segment] = df.isna().sum().values/SEQ_LENGTH      \n",
    "    return dict_results\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 4. Preprocess\n",
    "\n",
    "\n",
    "dict_segment_paths_train = {\n",
    "    segment : path + 'train/' + str(segment) + '.csv' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segment_paths_test = {\n",
    "    segment : path + 'test/' + str(segment) + '.csv' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "dict_segments_spectograms_paths_train = {\n",
    "    segment : path_spectograms + 'train/' + str(segment) + '/'\n",
    "    for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_spectograms_paths_test = {\n",
    "    segment : path_spectograms + 'test/' + str(segment) + '/'\n",
    "    for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "# mean_time_to_eruption, std_time_to_eruption = df_train['time_to_eruption'].mean(), df_train['time_to_eruption'].std()\n",
    "\n",
    "df_train['time_to_eruption'] = df_train['time_to_eruption']/(10**6)\n",
    "# df_train['time_to_eruption'] = df_train['time_to_eruption'].apply(lambda x: scale(x, mean_time_to_eruption, std_time_to_eruption))\n",
    "\n",
    "median_time_to_eruptions = np.quantile(df_train['time_to_eruption'], 0.5)\n",
    "\n",
    "dict_labels = {\n",
    "    segment: df_train['time_to_eruption'][df_train['segment_id']==segment].values.flatten()\n",
    "    for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 5. Build Generator\n",
    "\n",
    "def displayImage(image, figsize=(20, 15)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 5.1 - Data Augmentation\n",
    "\n",
    "def getTrainTransforms():\n",
    "    return albumentations.Compose([\n",
    "            albumentations.OneOf([\n",
    "                albumentations.GaussNoise(p=0.2),\n",
    "                albumentations.Cutout(num_holes=4, max_h_size=6, max_w_size=6, fill_value=0, p=0.2),\n",
    "            ], p=0.3),\n",
    "#         albumentations.OpticalDistortion(p=0.3)\n",
    "#         albumentations.ShiftScaleRotate(shift_limit=0.05, rotate_limit=1, p=0.5),\n",
    "#         albumentations.RandomCrop(IMG_SIZE[0]-2, IMG_SIZE[1]-5, p=0.5),\n",
    "#         albumentations.PadIfNeeded(min_height=IMG_SIZE[0], min_width=IMG_SIZE[1], value=0, p=1.0)\n",
    "    ])\n",
    "\n",
    "\n",
    "# 5.2 - Dataset\n",
    "\n",
    "class VolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_spectograms, batch_size, dict_labels, transforms, training=True):\n",
    "        super(VolcanoSequencesGenerator, self).__init__()\n",
    "        \n",
    "        self.segments = segments\n",
    "        self.path_spectograms = path_spectograms\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.training = training\n",
    "        self.transforms = transforms\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        array_spectograms_s0 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_0.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s1 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_1.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s2 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_2.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s3 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_3.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s4 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_4.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s5 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_5.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s6 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_6.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s7 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_7.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s8 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_8.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s9 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_9.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        \n",
    "        if self.transforms:\n",
    "            data_s0, data_s1 = {'image':array_spectograms_s0}, {'image':array_spectograms_s1}\n",
    "            data_s2, data_s3 = {'image':array_spectograms_s2}, {'image':array_spectograms_s3}\n",
    "            data_s4, data_s5 = {'image':array_spectograms_s4}, {'image':array_spectograms_s5}\n",
    "            data_s6, data_s7 = {'image':array_spectograms_s6}, {'image':array_spectograms_s7}\n",
    "            data_s8, data_s9 = {'image':array_spectograms_s8}, {'image':array_spectograms_s9}\n",
    "            \n",
    "            array_spectograms_s0 = np.stack([self.transforms(image=x)['image'] for x in data_s0['image']], axis=0)\n",
    "            array_spectograms_s1 = np.stack([self.transforms(image=x)['image'] for x in data_s1['image']], axis=0)\n",
    "            array_spectograms_s2 = np.stack([self.transforms(image=x)['image'] for x in data_s2['image']], axis=0)\n",
    "            array_spectograms_s3 = np.stack([self.transforms(image=x)['image'] for x in data_s3['image']], axis=0)\n",
    "            array_spectograms_s4 = np.stack([self.transforms(image=x)['image'] for x in data_s4['image']], axis=0)\n",
    "            array_spectograms_s5 = np.stack([self.transforms(image=x)['image'] for x in data_s5['image']], axis=0)\n",
    "            array_spectograms_s6 = np.stack([self.transforms(image=x)['image'] for x in data_s6['image']], axis=0)\n",
    "            array_spectograms_s7 = np.stack([self.transforms(image=x)['image'] for x in data_s7['image']], axis=0)\n",
    "            array_spectograms_s8 = np.stack([self.transforms(image=x)['image'] for x in data_s8['image']], axis=0)\n",
    "            array_spectograms_s9 = np.stack([self.transforms(image=x)['image'] for x in data_s9['image']], axis=0)\n",
    "                 \n",
    "        batch = (array_spectograms_s0/255, array_spectograms_s1/255, array_spectograms_s2/255, array_spectograms_s3/255, \n",
    "                 array_spectograms_s4/255, array_spectograms_s5/255, array_spectograms_s6/255, array_spectograms_s7/255, \n",
    "                 array_spectograms_s8/255, array_spectograms_s9/255)    \n",
    "            \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "        if self.training:\n",
    "            return batch, array_labels\n",
    "        else:\n",
    "            return batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        \n",
    "        \n",
    "    def generateOrderedSequences(self, list_segments):\n",
    "        array_spectograms_s0 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_0.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s1 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_1.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s2 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_2.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s3 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_3.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s4 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_4.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s5 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_5.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s6 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_6.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s7 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_7.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s8 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_8.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "        array_spectograms_s9 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_9.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8)\n",
    "                 \n",
    "        batch = (array_spectograms_s0/255, array_spectograms_s1/255, array_spectograms_s2/255, array_spectograms_s3/255, \n",
    "                 array_spectograms_s4/255, array_spectograms_s5/255, array_spectograms_s6/255, array_spectograms_s7/255, \n",
    "                 array_spectograms_s8/255, array_spectograms_s9/255)   \n",
    "        \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_segments])\n",
    "        if self.training:\n",
    "            return batch, array_labels\n",
    "        else:\n",
    "            return batch\n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_gen = VolcanoSequencesGenerator(list(unique_segments_id_train), dict_segments_spectograms_paths_train,\n",
    "#                                                 batch_size=2, dict_labels=dict_labels, \n",
    "#                                                 transforms=getTrainTransforms(),\n",
    "#                                                 training=True)\n",
    "\n",
    "# for batch in tmp_gen:\n",
    "#     break\n",
    "    \n",
    "# print(batch[0][0].shape)\n",
    "# displayImage(batch[0][0][0], figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 6. Models\n",
    "\n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)     \n",
    "\n",
    "# Custom Loss\n",
    "\n",
    "def quantileLoss(y_true, y_pred):\n",
    "    quantiles = tf.constant([0.4, 0.5, 0.6])\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(quantiles * e, (quantiles-1) * e)\n",
    "    return tf.reduce_mean(v)\n",
    "\n",
    "\n",
    "# Conv Model\n",
    "\n",
    "class ConvModelSensor(models.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvModelSensor, self).__init__()\n",
    "        \n",
    "        self.block0 = IdentityBlock(filters=[32, 32, 32], kernel_size=(7, 7), reg_factor=1e-4)\n",
    "        self.block1 = IdentityBlock(filters=[64, 64, 64], kernel_size=(7, 7), reg_factor=1e-4)\n",
    "        self.block2 = IdentityBlock(filters=[64, 64, 64], kernel_size=(5, 5), reg_factor=1e-4)\n",
    "        self.block3 = IdentityBlock(filters=[128, 128, 128], kernel_size=(3, 3), reg_factor=1e-4)\n",
    "        \n",
    "        self.conv0 = layers.Conv2D(filters=32, kernel_size=(9, 9), \n",
    "                                  padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.bn0 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(filters=32, kernel_size=(7, 7), \n",
    "                                  padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.conv2 = layers.Conv2D(filters=64, kernel_size=(5, 5), \n",
    "                                  padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.conv3 = layers.Conv2D(filters=64, kernel_size=(5, 5), \n",
    "                          padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.conv4 = layers.Conv2D(filters=128, kernel_size=(3, 3), \n",
    "                          padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        \n",
    "        self.drop0 = layers.Dropout(0.2)\n",
    "        self.drop1 = layers.Dropout(0.2)\n",
    "        self.drop2 = layers.Dropout(0.2)\n",
    "        self.drop3 = layers.Dropout(0.2)\n",
    "        \n",
    "        self.max_pool0 = layers.MaxPool2D((1, 2))\n",
    "        self.max_pool1 = layers.MaxPool2D(2)\n",
    "        self.max_pool2 = layers.MaxPool2D(2)\n",
    "        self.max_pool3 = layers.MaxPool2D(2)\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        \n",
    "        x = self.conv0(inputs)\n",
    "        x = self.bn0(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.max_pool0(x)\n",
    "        \n",
    "        x = self.drop0(x, training=training)\n",
    "        x = self.conv1(x)\n",
    "        x = self.block0(x)\n",
    "        x = self.max_pool1(x)\n",
    "        \n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.max_pool2(x)\n",
    "        \n",
    "        x = self.drop2(x, training=training)\n",
    "        x = self.conv3(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.max_pool3(x)\n",
    "        \n",
    "        x = self.drop3(x, training=training)\n",
    "        x = self.conv4(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class IdentityBlock(models.Model):\n",
    "    def __init__(self, filters=[32, 32, 32], kernel_size=(3, 3), reg_factor=1e-4):\n",
    "            super(IdentityBlock, self).__init__()\n",
    "\n",
    "            self.l0 = layers.Conv2D(filters=filters[0], kernel_size=(1, 1), \n",
    "                              padding='valid', kernel_regularizer=regularizers.l2(reg_factor))\n",
    "            self.l1 = layers.BatchNormalization()\n",
    "\n",
    "            self.l2 = layers.Conv2D(filters=filters[1], kernel_size=kernel_size, \n",
    "                              padding='same', kernel_regularizer=regularizers.l2(reg_factor))\n",
    "            self.l3 = layers.BatchNormalization()\n",
    "\n",
    "            self.l4 = layers.Conv2D(filters=filters[2], kernel_size=(1, 1), \n",
    "                              padding='valid', kernel_regularizer=regularizers.l2(reg_factor))\n",
    "            self.l5 = layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        x_residual = inputs\n",
    "        \n",
    "        x = self.l0(inputs)\n",
    "        x = self.l1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        \n",
    "        x = tf.add(x, x_residual)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "\n",
    "def buildModel(size, summary=False):\n",
    "    \n",
    "    block0 = ConvModelSensor()\n",
    "    block1 = ConvModelSensor()\n",
    "    block2 = ConvModelSensor()\n",
    "    block3 = ConvModelSensor()\n",
    "    block4 = ConvModelSensor()\n",
    "    block5 = ConvModelSensor()\n",
    "    block6 = ConvModelSensor()\n",
    "    block7 = ConvModelSensor()\n",
    "    block8 = ConvModelSensor()\n",
    "    block9 = ConvModelSensor()\n",
    "    \n",
    "    in_0 = layers.Input(shape=(40, 118, 1))\n",
    "    in_1 = layers.Input(shape=(40, 118, 1))\n",
    "    in_2 = layers.Input(shape=(40, 118, 1))\n",
    "    in_3 = layers.Input(shape=(40, 118, 1))\n",
    "    in_4 = layers.Input(shape=(40, 118, 1))\n",
    "    in_5 = layers.Input(shape=(40, 118, 1))\n",
    "    in_6 = layers.Input(shape=(40, 118, 1))\n",
    "    in_7 = layers.Input(shape=(40, 118, 1))\n",
    "    in_8 = layers.Input(shape=(40, 118, 1))\n",
    "    in_9 = layers.Input(shape=(40, 118, 1))\n",
    "    \n",
    "    x0 = block0(in_0)\n",
    "    x1 = block1(in_1)\n",
    "    x2 = block2(in_2)\n",
    "    x3 = block3(in_3)\n",
    "    x4 = block4(in_4)\n",
    "    x5 = block5(in_5)\n",
    "    x6 = block6(in_6)\n",
    "    x7 = block7(in_7)\n",
    "    x8 = block8(in_8)\n",
    "    x9 = block9(in_9)\n",
    "    \n",
    "    ##################################################\n",
    "    \n",
    "    x0 = layers.GlobalAveragePooling2D()(x0)\n",
    "    x1 = layers.GlobalAveragePooling2D()(x1)\n",
    "    x2 = layers.GlobalAveragePooling2D()(x2)\n",
    "    x3 = layers.GlobalAveragePooling2D()(x3)\n",
    "    x4 = layers.GlobalAveragePooling2D()(x4)\n",
    "    x5 = layers.GlobalAveragePooling2D()(x5)\n",
    "    x6 = layers.GlobalAveragePooling2D()(x6)\n",
    "    x7 = layers.GlobalAveragePooling2D()(x7)\n",
    "    x8 = layers.GlobalAveragePooling2D()(x8)\n",
    "    x9 = layers.GlobalAveragePooling2D()(x9)\n",
    "    \n",
    "    x = layers.concatenate([x0, x1, x2, x3, x4, x5, x6, x7, x8, x9])\n",
    "\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(300, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(64, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    ##################################################\n",
    "    \n",
    "    out_1 = layers.Dense(1, activation='relu', name='time_to_eruption')(x)\n",
    "    out_2 = layers.Dense(3, activation='relu', name='quantile')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[in_0, in_1, in_2, in_3, in_4,\n",
    "                                 in_5, in_6, in_7, in_8, in_9], \n",
    "                         outputs=[out_1, out_2])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=8e-4), \n",
    "                  \n",
    "                  loss=[losses.MeanAbsoluteError(), quantileLoss],\n",
    "                  loss_weights=[4, 1],\n",
    "                  metrics=['mae'])\n",
    "    if summary:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0:\n",
    "        return lr*0.9\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Fold: 1\n",
      "Train segments: 3544 Val segments: 887\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 40, 118, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor (ConvModelSen (None, 5, 7, 128)    836800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_1 (ConvModelS (None, 5, 7, 128)    836800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_2 (ConvModelS (None, 5, 7, 128)    836800      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_3 (ConvModelS (None, 5, 7, 128)    836800      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_4 (ConvModelS (None, 5, 7, 128)    836800      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_5 (ConvModelS (None, 5, 7, 128)    836800      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_6 (ConvModelS (None, 5, 7, 128)    836800      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_7 (ConvModelS (None, 5, 7, 128)    836800      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_8 (ConvModelS (None, 5, 7, 128)    836800      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_9 (ConvModelS (None, 5, 7, 128)    836800      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 128)          0           conv_model_sensor[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 128)          0           conv_model_sensor_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 128)          0           conv_model_sensor_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 128)          0           conv_model_sensor_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 128)          0           conv_model_sensor_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 128)          0           conv_model_sensor_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 128)          0           conv_model_sensor_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 128)          0           conv_model_sensor_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glo (None, 128)          0           conv_model_sensor_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_9 (Glo (None, 128)          0           conv_model_sensor_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1280)         0           global_average_pooling2d[0][0]   \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "                                                                 global_average_pooling2d_3[0][0] \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "                                                                 global_average_pooling2d_5[0][0] \n",
      "                                                                 global_average_pooling2d_6[0][0] \n",
      "                                                                 global_average_pooling2d_7[0][0] \n",
      "                                                                 global_average_pooling2d_8[0][0] \n",
      "                                                                 global_average_pooling2d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 1280)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300)          384300      dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 300)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 300)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           19264       dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 64)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_to_eruption (Dense)        (None, 1)            65          dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "quantile (Dense)                (None, 3)            195         dropout_42[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,771,824\n",
      "Trainable params: 8,753,904\n",
      "Non-trainable params: 17,920\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 39s 175ms/step - loss: 54.6949 - time_to_eruption_loss: 11.8500 - quantile_loss: 6.1173 - time_to_eruption_mae: 11.8500 - quantile_mae: 12.7191 - val_loss: 52.7729 - val_time_to_eruption_loss: 11.5025 - val_quantile_loss: 5.5665 - val_time_to_eruption_mae: 11.5025 - val_quantile_mae: 11.9128\n",
      "Epoch 2/100\n",
      "222/222 [==============================] - 36s 164ms/step - loss: 51.3640 - time_to_eruption_loss: 11.1497 - quantile_loss: 5.5545 - time_to_eruption_mae: 11.1497 - quantile_mae: 11.5918 - val_loss: 63.1740 - val_time_to_eruption_loss: 13.8525 - val_quantile_loss: 6.5355 - val_time_to_eruption_mae: 13.8525 - val_quantile_mae: 13.7135\n",
      "Epoch 3/100\n",
      "222/222 [==============================] - 37s 165ms/step - loss: 50.2415 - time_to_eruption_loss: 10.8779 - quantile_loss: 5.4884 - time_to_eruption_mae: 10.8779 - quantile_mae: 11.4318 - val_loss: 51.3486 - val_time_to_eruption_loss: 11.1327 - val_quantile_loss: 5.5669 - val_time_to_eruption_mae: 11.1327 - val_quantile_mae: 11.6299\n",
      "Epoch 4/100\n",
      " 21/222 [=>............................] - ETA: 29s - loss: 51.4559 - time_to_eruption_loss: 11.1311 - quantile_loss: 5.6795 - time_to_eruption_mae: 11.1311 - quantile_mae: 11.9229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-50b4de7a1be3>\", line 39, in <module>\n",
      "    verbose=1)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1098, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 807, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2829, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1848, in _filtered_call\n",
      "    cancellation_manager=cancellation_manager)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1924, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 550, in call\n",
      "    ctx=ctx)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 7. Training\n",
    "\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "callback_early_stopping = ReturnBestEarlyStopping(monitor='val_time_to_eruption_loss', patience=15, \n",
    "                                                  verbose=1, restore_best_weights=True)\n",
    "callback_lrsched = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "list_callbacks = [callback_early_stopping, callback_lrsched]\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "list_history, list_models = [], []\n",
    "\n",
    "for num_fold, (train_index, val_index) in enumerate(kf.split(list_segments_train,\n",
    "                                                             np.zeros(len(list_segments_train)))):\n",
    "    segments_train_fold = np.asarray(list_segments_train)[train_index]\n",
    "    segments_val_fold = np.asarray(list_segments_train)[val_index]\n",
    "\n",
    "    X_train_generator = VolcanoSequencesGenerator(segments_train_fold, dict_segments_spectograms_paths_train,\n",
    "                                                  batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                  transforms=getTrainTransforms(), training=True)\n",
    "\n",
    "    X_val_generator = VolcanoSequencesGenerator(segments_val_fold, dict_segments_spectograms_paths_train,\n",
    "                                                 batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                transforms=None, training=True)\n",
    "\n",
    "    print(f'Num Fold: {num_fold + 1}')\n",
    "    print(f'Train segments: {len(train_index)} Val segments: {len(val_index)}')\n",
    "\n",
    "    model = buildModel(size=118, summary=True)\n",
    "\n",
    "    history = model.fit(X_train_generator,\n",
    "                        validation_data=X_val_generator,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=list_callbacks,\n",
    "                        epochs=100,\n",
    "                        verbose=1)\n",
    "\n",
    "    list_history.append(history)\n",
    "    list_models.append(model)\n",
    "\n",
    "    X_val_batch, y_val_target = X_val_generator.generateOrderedSequences(list(segments_val_fold))\n",
    "    y_pred_val = model.predict(X_val_batch, verbose=1)\n",
    "    y_pred_val[0], y_pred_val[1] = y_pred_val[0]*(10**6), y_pred_val[1]*(10**6)\n",
    "    y_val_target = y_val_target*(10**6)\n",
    "    mae = np.abs(y_val_target - y_pred_val[0])\n",
    "\n",
    "    model.save(f'./models/model_cnn2d_Tiny_{num_fold}')\n",
    "\n",
    "    print('***'*20)\n",
    "    print(f'Prediction MAE: {mae.mean()}')\n",
    "    print('***'*20)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [06:00<00:00, 72.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "2915986.86431257\n",
      "2915679.0316216704\n",
      "************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 8. Cross Val Score\n",
    "\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "batch_size = 8\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "df_val_all = pd.DataFrame()\n",
    "\n",
    "for num_fold, (train_index, val_index) in tqdm(enumerate(kf.split(list_segments_train,\n",
    "                                                             np.zeros(len(list_segments_train)))), \n",
    "                                               total=5, position=0):\n",
    "    \n",
    "    \n",
    "    segments_train_fold = np.asarray(list_segments_train)[train_index]\n",
    "    segments_val_fold = np.asarray(list_segments_train)[val_index]\n",
    "\n",
    "    X_val_generator = VolcanoSequencesGenerator(segments_val_fold, dict_segments_spectograms_paths_train,\n",
    "                                                batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                transforms=None, training=True)   \n",
    "\n",
    "    model = models.load_model(f'./models/model_cnn2d_Tiny_{num_fold}', compile=False)\n",
    "\n",
    "    X_val_sequences, y_val_target = X_val_generator.generateOrderedSequences(segments_val_fold)\n",
    "    y_pred_val = model.predict(X_val_sequences)\n",
    "\n",
    "    df_tmp = pd.DataFrame({\n",
    "            'pred' :  y_pred_val[0].squeeze()*(10**6),\n",
    "            'pred_q_30' : y_pred_val[1][:, 0]*(10**6),\n",
    "            'pred_q_50' : y_pred_val[1][:, 1]*(10**6),\n",
    "            'pred_q_70' : y_pred_val[1][:, 2]*(10**6),\n",
    "            'y_true' : y_val_target.flatten()*(10**6)\n",
    "    })\n",
    "\n",
    "    df_val_all = pd.concat([df_val_all, df_tmp], axis=0)\n",
    "\n",
    "print('***'*20)\n",
    "print(np.mean(np.abs(df_tmp['y_true'] - df_tmp['pred'])))\n",
    "print(np.mean(np.abs(df_tmp['y_true'] - df_tmp['pred_q_50'])))\n",
    "print('***'*20)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/283 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 32 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C3E6AA288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 33 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C3E618438> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 283/283 [04:43<00:00,  1.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>time_to_eruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.520000e+03</td>\n",
       "      <td>4.520000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.066993e+09</td>\n",
       "      <td>2.361497e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.162904e+08</td>\n",
       "      <td>1.077405e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.602880e+05</td>\n",
       "      <td>9.239777e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.458995e+08</td>\n",
       "      <td>1.624013e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.060695e+09</td>\n",
       "      <td>2.660927e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.599284e+09</td>\n",
       "      <td>3.114177e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147116e+09</td>\n",
       "      <td>4.847652e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         segment_id  time_to_eruption\n",
       "count  4.520000e+03      4.520000e+03\n",
       "mean   1.066993e+09      2.361497e+07\n",
       "std    6.162904e+08      1.077405e+07\n",
       "min    8.602880e+05      9.239777e+05\n",
       "25%    5.458995e+08      1.624013e+07\n",
       "50%    1.060695e+09      2.660927e+07\n",
       "75%    1.599284e+09      3.114177e+07\n",
       "max    2.147116e+09      4.847652e+07"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################\n",
    "# 9. Inference\n",
    "\n",
    "\n",
    "# del X_val_sequences, y_val_target, list_cv_pred, y_pred_cv, y_cv_target, df_cv, X_cv_sequences, y_cv_target\n",
    "gc.collect()\n",
    "\n",
    "list_models = [models.load_model(f'./models/model_cnn2d_Tiny_{i}', compile=False) for i in range(5)]\n",
    "\n",
    "X_test_generator = VolcanoSequencesGenerator(list(unique_segments_id_test), dict_segments_spectograms_paths_test,\n",
    "                                             batch_size=16, dict_labels=dict_labels, \n",
    "                                             transforms=None, training=False)\n",
    "\n",
    "batch_size_prediction = 16\n",
    "idx = 0\n",
    "num_test_steps = int(np.ceil(len(unique_segments_id_test) / batch_size_prediction))\n",
    "list_test_segments = list(unique_segments_id_test)\n",
    "array_predictions = np.zeros((len(list_test_segments)))\n",
    "\n",
    "for i in tqdm(range(num_test_steps), total=num_test_steps, position=0):\n",
    "    list_tmp_segments = list_test_segments[idx:(idx+batch_size_prediction)]\n",
    "    X_test_sequences = X_test_generator.generateOrderedSequences(list_tmp_segments)\n",
    "     \n",
    "    predictions = [model.predict(X_test_sequences)[0].squeeze() for model in list_models]\n",
    "    predictions = np.mean(np.asarray(predictions), axis=0)*(10**6)\n",
    "    array_predictions[idx:(idx+batch_size_prediction)] = predictions\n",
    "    idx += batch_size_prediction   \n",
    "\n",
    "df_submission = pd.DataFrame({\n",
    "    'segment_id' : list_test_segments,\n",
    "    'time_to_eruption' : array_predictions\n",
    "})\n",
    "\n",
    "df_submission.to_csv('./FinalSubmissions/' + 'submission_tiny_cnn.csv', index=False)\n",
    "df_submission.describe()\n",
    "\n",
    "#############################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
