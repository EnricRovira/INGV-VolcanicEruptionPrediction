{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 1. Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers, metrics, losses, optimizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 2. Paths & Global Variables\n",
    "\n",
    "## 2.1 Paths\n",
    "\n",
    "path = '../../01_Data/'\n",
    "path_sequences = path + '01_GeneratedSequences/'\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(path + 'train.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv') \n",
    "\n",
    "train_paths = glob.glob(path + 'train/*')\n",
    "test_paths = glob.glob(path + 'test/*')\n",
    "\n",
    "unique_segments_id_train = set(df_train['segment_id'])\n",
    "unique_segments_id_test = set(df_sample_submission['segment_id'])\n",
    "\n",
    "dict_unique_segments_id = { v : k for k, v in enumerate(unique_segments_id_train)}\n",
    "dict_unique_segments_id_inv = { k : v for k, v in enumerate(unique_segments_id_train)}\n",
    "\n",
    "## 2.2 Global Variables\n",
    "\n",
    "SEQ_LENGTH = 60_001\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 3. Global Functions\n",
    "\n",
    "def buildSequences(df, dict_segment_paths, training=True, mask_value=-1.0):\n",
    "    x = np.zeros((len(dict_segment_paths), SEQ_LENGTH, 10))\n",
    "    if training:\n",
    "        y = np.zeros(len(dict_segment_paths))\n",
    "    for i, segment in enumerate(tqdm(dict_segment_paths, total=len(dict_segment_paths), position=0)):\n",
    "        segment_path = dict_segment_paths[segment]\n",
    "        df_tmp = pd.read_csv(segment_path)\n",
    "        df_tmp = df_tmp.fillna(mask_value)\n",
    "        x[i] = df_tmp.values[-SEQ_LENGTH:]\n",
    "        if training:\n",
    "            y[i] = df['time_to_eruption'][df['segment_id']==segment].values[0]\n",
    "    if training:\n",
    "        return x, y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def scale(x, mean_, std_):\n",
    "    return (x - mean_) / std_\n",
    "\n",
    "\n",
    "def unscale(x, mean_, std_):\n",
    "    return (x * std_) + mean_\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 4. Preprocess\n",
    "\n",
    "dict_segments_sequences_paths_train = {\n",
    "    segment : path_sequences + 'train/' + str(segment) + '.npy' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_sequences_paths_test = {\n",
    "    segment : path_sequences + 'test/' + str(segment) + '.npy' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "dict_positions_segments = {k : i for i, k in enumerate(dict_segments_sequences_paths_train.keys())}\n",
    "\n",
    "df_train['time_to_eruption'] = df_train['time_to_eruption']/(10**6)\n",
    "\n",
    "dict_labels = {\n",
    "    segment : df_train['time_to_eruption'][df_train['segment_id']==segment].values.flatten()\n",
    "    for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 5. Build Sequences\n",
    "\n",
    "class VolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_sequences, batch_size, dict_labels, augmentations, training=True):\n",
    "        super(VolcanoSequencesGenerator, self).__init__()\n",
    "        \n",
    "        self.dict_means = {0: 0.09421943291597953, 1: 0.9208114415834104, 2: -0.026617075839858038, \n",
    "                           3: 0.09724443370400684, 4: 1.704695380910225, 5: -0.1180321202370159, 6: 0.7667902421713446, \n",
    "                           7: 0.7804286101804458, 8: -0.2075797991904395, 9: 0.014516944212624944} \n",
    "        \n",
    "        self.dict_stds =  {0: 1820.6211174856987, 1: 1931.0901612736805, 2: 1738.1671740163413, \n",
    "                           3: 1669.8837574619292, 4: 568.5221048211192, 5: 1848.4917466767877, 6: 1623.353060255481, \n",
    "                           7: 1618.2714709240895, 8: 1590.9403316558762, 9: 1906.41447528788}\n",
    "        \n",
    "        self.segments = segments\n",
    "        self.path_sequences = path_sequences\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.augmentations = augmentations\n",
    "        self.training = training\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        \n",
    "        array_sequences = np.asarray([np.load(self.path_sequences[segment], allow_pickle=True)[-SEQ_LENGTH:, :]\n",
    "                                     for segment in list_batch_segments])\n",
    "        \n",
    "        if self.augmentations:\n",
    "            array_sequences = self.augmentBatch(array_sequences)\n",
    "        \n",
    "        array_sequences[:, :, 0] = scale(array_sequences[:, :, 0], self.dict_means[0], self.dict_stds[0])\n",
    "        array_sequences[:, :, 1] = scale(array_sequences[:, :, 1], self.dict_means[1], self.dict_stds[1])\n",
    "        array_sequences[:, :, 2] = scale(array_sequences[:, :, 2], self.dict_means[2], self.dict_stds[2])\n",
    "        array_sequences[:, :, 3] = scale(array_sequences[:, :, 3], self.dict_means[3], self.dict_stds[3])\n",
    "        array_sequences[:, :, 4] = scale(array_sequences[:, :, 4], self.dict_means[4], self.dict_stds[4])\n",
    "        array_sequences[:, :, 5] = scale(array_sequences[:, :, 5], self.dict_means[5], self.dict_stds[5])\n",
    "        array_sequences[:, :, 6] = scale(array_sequences[:, :, 6], self.dict_means[6], self.dict_stds[6])\n",
    "        array_sequences[:, :, 7] = scale(array_sequences[:, :, 7], self.dict_means[7], self.dict_stds[7])\n",
    "        array_sequences[:, :, 8] = scale(array_sequences[:, :, 8], self.dict_means[8], self.dict_stds[8])\n",
    "        array_sequences[:, :, 9] = scale(array_sequences[:, :, 9], self.dict_means[9], self.dict_stds[9])\n",
    "        \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "\n",
    "        if self.training:\n",
    "            return array_sequences, array_labels\n",
    "        else:\n",
    "            return array_sequences\n",
    "        \n",
    "        \n",
    "    def noiseInjection(self, batch_sequences, noise_factor=0.075):\n",
    "        noise = np.random.randn(batch_sequences.shape[0], batch_sequences.shape[1], batch_sequences.shape[2])\n",
    "        augmented_data = batch_sequences + noise_factor * noise\n",
    "        return augmented_data\n",
    "    \n",
    "    \n",
    "    def timeShifting(self, batch_sequences, shift_max):\n",
    "        shift = np.random.randint(shift_max)\n",
    "        for sensor in range(10):\n",
    "            batch_sequences[:, :, sensor] = np.roll(batch_sequences[:, :, sensor], shift)\n",
    "        return batch_sequences\n",
    "       \n",
    "    \n",
    "    def augmentBatch(self, batch_sequences):\n",
    "        \n",
    "        # Add random noise\n",
    "        if np.random.random() > 0.5:\n",
    "            batch_sequences = self.noiseInjection(batch_sequences, noise_factor=0.005)   \n",
    "            \n",
    "        # Time shifting\n",
    "        if np.random.random() > 0.5:\n",
    "            batch_sequences = self.timeShifting(batch_sequences, shift_max=600) \n",
    "        \n",
    "#         # Random batch sequence sensors slices to null\n",
    "#         if np.random.random() > 0.5:\n",
    "#             num_random_sensors = np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "#             random_sensors = list(set(np.random.randint(0, 9, size=num_random_sensors)))\n",
    "#             random_ini_position = np.random.randint(0, 60_001, size=num_random_sensors)\n",
    "#             random_length = np.random.randint(random_ini_position, random_ini_position+6_000, size=num_random_sensors)\n",
    "#             random_length -= random_ini_position\n",
    "\n",
    "#             if num_random_sensors!=0:\n",
    "#                 for i, sensor in enumerate(random_sensors):\n",
    "#                     batch_sequences[:, random_ini_position[i]:random_ini_position[i]+random_length[i], sensor] = 0.0\n",
    "                    \n",
    "        # Shut-down sensor\n",
    "        if np.random.random() > 0.5:\n",
    "            sensor = np.random.randint(0, 9)\n",
    "            batch_sequences[:, :, sensor] = 0.0\n",
    "                \n",
    "        return batch_sequences\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        \n",
    "        \n",
    "    def generateOrderedSequences(self, list_segments):\n",
    "        X = np.empty((len(list_segments), SEQ_LENGTH, 10))\n",
    "        \n",
    "        if self.training:\n",
    "            y = np.empty(len(list_segments))\n",
    "            \n",
    "        for i, segment in enumerate(list_segments):\n",
    "            X[i] = np.load(self.path_sequences[segment], allow_pickle=True)[-SEQ_LENGTH:, :]\n",
    "            if self.training:\n",
    "                y[i] = self.dict_labels[segment]\n",
    "                \n",
    "        if self.augmentations:\n",
    "            X = self.augmentBatch(X)\n",
    "            \n",
    "        X[:, :, 0] = scale(X[:, :, 0], self.dict_means[0], self.dict_stds[0])\n",
    "        X[:, :, 1] = scale(X[:, :, 1], self.dict_means[1], self.dict_stds[1])\n",
    "        X[:, :, 2] = scale(X[:, :, 2], self.dict_means[2], self.dict_stds[2])\n",
    "        X[:, :, 3] = scale(X[:, :, 3], self.dict_means[3], self.dict_stds[3])\n",
    "        X[:, :, 4] = scale(X[:, :, 4], self.dict_means[4], self.dict_stds[4])\n",
    "        X[:, :, 5] = scale(X[:, :, 5], self.dict_means[5], self.dict_stds[5])\n",
    "        X[:, :, 6] = scale(X[:, :, 6], self.dict_means[6], self.dict_stds[6])\n",
    "        X[:, :, 7] = scale(X[:, :, 7], self.dict_means[7], self.dict_stds[7])\n",
    "        X[:, :, 8] = scale(X[:, :, 8], self.dict_means[8], self.dict_stds[8])\n",
    "        X[:, :, 9] = scale(X[:, :, 9], self.dict_means[9], self.dict_stds[9])\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.training:\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 6. Models\n",
    "\n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__(name='MultiHeadAttention')\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = layers.Dense(d_model, kernel_regularizer=regularizers.l2(1e-5))\n",
    "        self.wk = layers.Dense(d_model, kernel_regularizer=regularizers.l2(1e-5))\n",
    "        self.wv = layers.Dense(d_model, kernel_regularizer=regularizers.l2(1e-5))\n",
    "\n",
    "        self.dense = layers.Dense(d_model, kernel_regularizer=regularizers.l2(1e-5))\n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    \n",
    "class WaveNet(layers.Layer):\n",
    "    def __init__(self, num_filters, kernel_size, deep_dilations, **kwargs): \n",
    "        super().__init__(**kwargs, name='WaveNet')\n",
    "        \n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.deep_dilations = deep_dilations\n",
    "        \n",
    "        self.list_dilation_rates = [2**i for i in range(self.deep_dilations)]\n",
    "        \n",
    "        self.conv_casual = layers.Conv1D(filters=self.num_filters, kernel_size=1, padding='causal',\n",
    "                                          kernel_regularizer=regularizers.l2(1e-5))\n",
    "        \n",
    "        self.list_conv_tanh = [layers.Conv1D(filters=self.num_filters, kernel_size=self.kernel_size,\n",
    "                                             kernel_regularizer=regularizers.l2(1e-5),\n",
    "                                             padding='same', activation='tanh', dilation_rate=rate) #tanh\n",
    "                               for rate in self.list_dilation_rates]\n",
    "        \n",
    "        self.list_conv_sigm = [layers.Conv1D(filters=self.num_filters, kernel_size=self.kernel_size,\n",
    "                                             kernel_regularizer=regularizers.l2(1e-5),\n",
    "                                             padding='same', activation='sigmoid', dilation_rate=rate) #sigmoid\n",
    "                               for rate in self.list_dilation_rates]\n",
    "        \n",
    "        self.list_conv_bottleneck = [layers.Conv1D(filters=self.num_filters, kernel_size=1, padding='same',\n",
    "                                                  kernel_regularizer=regularizers.l2(1e-5))\n",
    "                                     for rate in self.list_dilation_rates]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv_casual(inputs)\n",
    "        x_residual = x\n",
    "        \n",
    "        for i in range(len(self.list_dilation_rates)):\n",
    "            z1 = self.list_conv_tanh[i](x)\n",
    "            z2 = self.list_conv_sigm[i](x)\n",
    "            x = tf.multiply(z1, z2)\n",
    "            x = self.list_conv_bottleneck[i](x)\n",
    "            x_residual = tf.add(x_residual, x)  \n",
    "\n",
    "        return x_residual\n",
    "    \n",
    "    \n",
    "class ConvWaveNetHead(models.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvWaveNetHead, self).__init__()\n",
    "        self.wave_1 = WaveNet(num_filters=16, kernel_size=3, deep_dilations=9)\n",
    "        self.wave_2 = WaveNet(num_filters=32, kernel_size=3, deep_dilations=7)\n",
    "        self.wave_3 = WaveNet(num_filters=64, kernel_size=3, deep_dilations=6)\n",
    "        \n",
    "        self.avg_pool1 = layers.MaxPooling1D(10)\n",
    "        self.avg_pool2 = layers.MaxPooling1D(10)\n",
    "        self.avg_pool3 = layers.MaxPooling1D(10)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        \n",
    "        x = self.wave_1(inputs)\n",
    "        x = self.avg_pool1(x)\n",
    "        \n",
    "        x = self.wave_2(x)\n",
    "        x = self.avg_pool2(x)\n",
    "        \n",
    "        x = self.wave_3(x)\n",
    "        x = self.avg_pool3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class VolcanoMHAttentionSequenceModel(models.Model): \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(VolcanoMHAttentionSequenceModel, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        x, _ = self.mha(inputs, inputs, inputs, mask=None)\n",
    "#         x = self.layernorm(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# Custom Loss\n",
    "\n",
    "def quantileLoss(y_true, y_pred):\n",
    "    quantiles = tf.constant([0.4, 0.5, 0.6])\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(quantiles * e, (quantiles-1) * e)\n",
    "    return tf.reduce_mean(v)\n",
    "\n",
    "# Model Wrapper\n",
    "\n",
    "def buildmodel(num_heads, d_model=64, summary=False):\n",
    "        \n",
    "    conv_head = ConvWaveNetHead()\n",
    "    att_model = VolcanoMHAttentionSequenceModel(d_model=d_model, num_heads=num_heads)\n",
    "    \n",
    "    in_ = layers.Input(shape=(SEQ_LENGTH, 10))\n",
    "    x = conv_head(in_)\n",
    "    x = att_model(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    out_1 = layers.Dense(1, activation='relu', name='time_to_eruption')(x)\n",
    "    out_2 = layers.Dense(3, activation='relu', name='quantile')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[in_], outputs=[out_1, out_2])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=8e-4, \n",
    "                                           beta_1=0.9, beta_2=0.98, \n",
    "                                           epsilon=1e-9), \n",
    "                  loss=[losses.MeanAbsoluteError(), quantileLoss],\n",
    "                  loss_weights=[4, 1],\n",
    "                  metrics=['mae'])\n",
    "    if summary:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0:\n",
    "        return lr*0.9\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Fold: 1\n",
      "Train segments: 3544 Val segments: 887\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60001, 10)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_wave_net_head (ConvWaveNet (None, 60, 64)       243424      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "volcano_mh_attention_sequence_m (None, None, 300)    148800      conv_wave_net_head[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 300)          0           volcano_mh_attention_sequence_mod\n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           19264       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_to_eruption (Dense)        (None, 1)            65          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "quantile (Dense)                (None, 3)            195         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 411,748\n",
      "Trainable params: 411,748\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "113/443 [======>.......................] - ETA: 1:39 - loss: 57.8509 - time_to_eruption_loss: 12.6504 - quantile_loss: 7.2097 - time_to_eruption_mae: 12.6504 - quantile_mae: 14.4895"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-3c8ebb41b027>\", line 41, in <module>\n",
      "    verbose=1)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1098, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 807, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2829, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1848, in _filtered_call\n",
      "    cancellation_manager=cancellation_manager)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1924, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 550, in call\n",
      "    ctx=ctx)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Enric\\anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 7. Training\n",
    "\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "\n",
    "batch_size = 8\n",
    "num_heads = 5\n",
    "d_model = 300\n",
    "\n",
    "callback_early_stopping = ReturnBestEarlyStopping(monitor='val_time_to_eruption_loss', \n",
    "                                                  patience=20, verbose=1, restore_best_weights=True)\n",
    "callback_lrsched = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "list_callbacks = [callback_early_stopping, callback_lrsched]\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "list_history, list_reg_models = [], []\n",
    "\n",
    "for num_fold, (train_index, val_index) in enumerate(kf.split(list_segments_train,\n",
    "                                                             np.zeros(len(list_segments_train)))):\n",
    "    segments_train_fold = np.asarray(list_segments_train)[train_index]\n",
    "    segments_val_fold = np.asarray(list_segments_train)[val_index]\n",
    "\n",
    "    X_train_generator = VolcanoSequencesGenerator(segments_train_fold, dict_segments_sequences_paths_train,\n",
    "                                                 batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                 augmentations=True, training=True)\n",
    "\n",
    "    X_val_generator = VolcanoSequencesGenerator(segments_val_fold, dict_segments_sequences_paths_train,\n",
    "                                                 batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                 augmentations=False, training=True)\n",
    "\n",
    "    print(f'Num Fold: {num_fold + 1}')\n",
    "    print(f'Train segments: {len(train_index)} Val segments: {len(val_index)}')\n",
    "\n",
    "    model = buildmodel(num_heads=num_heads, d_model=d_model, summary=True)\n",
    "\n",
    "    history = model.fit(X_train_generator,\n",
    "                        validation_data=X_val_generator,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=list_callbacks,\n",
    "                        epochs=100,\n",
    "                        verbose=1)\n",
    "\n",
    "    # Eval\n",
    "\n",
    "    X_val_sequences, y_val_target = X_val_generator.generateOrderedSequences(list(segments_val_fold))\n",
    "\n",
    "    y_pred_val = model.predict(X_val_sequences)\n",
    "    y_pred_val[0], y_pred_val[1] = y_pred_val[0]*(10**6), y_pred_val[1]*(10**6)\n",
    "    y_val_target = y_val_target*(10**6)\n",
    "\n",
    "    df_train_lreg = pd.DataFrame({\n",
    "        'pred_q_20' : y_pred_val[1][:, 0],\n",
    "        'pred_q_50' : y_pred_val[1][:, 1],\n",
    "        'pred_q_80' : y_pred_val[1][:, 2],\n",
    "    })\n",
    "\n",
    "    reg_model = LinearRegression().fit(df_train_lreg.values, y_val_target)\n",
    "\n",
    "    df_val = pd.DataFrame({\n",
    "        'segment_id' : list(segments_val_fold),\n",
    "        'target_time_to_eruption' : y_val_target,\n",
    "        'pred_time_to_eruption' : y_pred_val[0].squeeze(),\n",
    "        'pred_q_20' : y_pred_val[1][:, 0],\n",
    "        'pred_q_50' : y_pred_val[1][:, 1],\n",
    "        'pred_q_80' : y_pred_val[1][:, 2],\n",
    "        'pred_linear_reg' : reg_model.predict(df_train_lreg),\n",
    "        'mae_pred' : np.abs(y_val_target - y_pred_val[0].squeeze()),\n",
    "        'mae_q_50' : np.abs(y_val_target -  y_pred_val[1][:, 1]),\n",
    "        'mae_linear_reg' : np.abs(y_val_target -  reg_model.predict(df_train_lreg)),\n",
    "    })\n",
    "\n",
    "    print('***'*20)\n",
    "    print(f\"Prediction MAE: {df_val['mae_pred'].mean()}\\n \\\n",
    "            Quantile-P50 MAE: {df_val['mae_q_50'].mean()}\\n \\\n",
    "            Linear-Reg MAE: {df_val['mae_linear_reg'].mean()}\\n \\\n",
    "            Mean Prediction&Linear Reg: {((df_val['mae_pred']+df_val['mae_linear_reg'])/2).mean()}\\n \\\n",
    "            Mean Prediction&Quantile-P50: {((df_val['mae_pred']+df_val['mae_q_50'])/2).mean()}\")\n",
    "    print('***'*20)\n",
    "\n",
    "    list_history.append(history)\n",
    "    model.save(f'./models/model_mha_{num_fold}')\n",
    "    print('***'*20)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 8. Cross Val Score\n",
    "\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "batch_size = 16\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "df_val_all = pd.DataFrame()\n",
    "\n",
    "for num_fold, (train_index, val_index) in tqdm(enumerate(kf.split(list_segments_train,\n",
    "                                                             np.zeros(len(list_segments_train)))), \n",
    "                                               total=5, position=0):\n",
    "    \n",
    "    segments_train_fold = np.asarray(list_segments_train)[train_index]\n",
    "    segments_val_fold = np.asarray(list_segments_train)[val_index]\n",
    "\n",
    "    X_val_generator = VolcanoSequencesGenerator(segments_val_fold, dict_segments_sequences_paths_train,\n",
    "                                                batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                augmentations=False, training=True)   \n",
    "\n",
    "    model = models.load_model(f'./models/model_mha_{num_fold}', compile=False)\n",
    "\n",
    "    X_val_sequences, y_val_target = X_val_generator.generateOrderedSequences(segments_val_fold)\n",
    "    y_pred_val = model.predict(X_val_sequences)\n",
    "\n",
    "    df_tmp = pd.DataFrame({\n",
    "            'pred' :  y_pred_val[0].squeeze()*(10**6),\n",
    "            'pred_q_30' : y_pred_val[1][:, 0]*(10**6),\n",
    "            'pred_q_50' : y_pred_val[1][:, 1]*(10**6),\n",
    "            'pred_q_70' : y_pred_val[1][:, 2]*(10**6),\n",
    "            'y_true' : y_val_target*(10**6)\n",
    "    })\n",
    "\n",
    "    df_val_all = pd.concat([df_val_all, df_tmp], axis=0)\n",
    "\n",
    "print('***'*20)\n",
    "print(np.mean(np.abs(df_tmp['y_true'] - df_tmp['pred'])))\n",
    "print(np.mean(np.abs(df_tmp['y_true'] - df_tmp['pred_q_50'])))\n",
    "print('***'*20)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 9. Inference\n",
    "\n",
    "\n",
    "# del X_val_sequences, y_val_target, list_cv_pred, y_pred_cv, y_cv_target, df_cv, X_cv_sequences, y_cv_target\n",
    "gc.collect()\n",
    "\n",
    "list_models = [models.load_model(f'./models/model_mha_{i}', compile=False) for i in range(5)]\n",
    "\n",
    "X_test_generator = VolcanoSequencesGenerator(list(unique_segments_id_test), dict_segments_sequences_paths_test,\n",
    "                                             batch_size=16, dict_labels=dict_labels, \n",
    "                                             augmentations=False, training=False)\n",
    "\n",
    "batch_size_prediction = 16\n",
    "idx = 0\n",
    "num_test_steps = int(np.ceil(len(unique_segments_id_test) / batch_size_prediction))\n",
    "list_test_segments = list(unique_segments_id_test)\n",
    "array_predictions = np.zeros((len(list_test_segments)))\n",
    "for i in tqdm(range(num_test_steps), total=num_test_steps, position=0):\n",
    "    list_tmp_segments = list_test_segments[idx:(idx+batch_size_prediction)]\n",
    "    X_test_sequences = X_test_generator.generateOrderedSequences(list_tmp_segments)\n",
    "     \n",
    "    predictions = [model.predict(X_test_sequences)[0].squeeze() for model in list_models]\n",
    "    predictions = np.mean(np.asarray(predictions), axis=0)*(10**6)\n",
    "    array_predictions[idx:(idx+batch_size_prediction)] = predictions\n",
    "    idx += batch_size_prediction   \n",
    "\n",
    "df_submission = pd.DataFrame({\n",
    "    'segment_id' : list_test_segments,\n",
    "    'time_to_eruption' : array_predictions\n",
    "})\n",
    "\n",
    "df_submission.to_csv('./FinalSubmissions/' + 'submission_mha.csv', index=False)\n",
    "df_submission.describe()\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/111\n",
      "1/111\n",
      "2/111\n",
      "3/111\n",
      "4/111\n",
      "5/111\n",
      "6/111\n",
      "7/111\n",
      "8/111\n",
      "9/111\n",
      "10/111\n",
      "11/111\n",
      "12/111\n",
      "13/111\n",
      "14/111\n",
      "15/111\n",
      "16/111\n",
      "17/111\n",
      "18/111\n",
      "19/111\n",
      "20/111\n",
      "21/111\n",
      "22/111\n",
      "23/111\n",
      "24/111\n",
      "25/111\n",
      "26/111\n",
      "27/111\n",
      "28/111\n",
      "29/111\n",
      "30/111\n",
      "31/111\n",
      "32/111\n",
      "33/111\n",
      "34/111\n",
      "35/111\n",
      "36/111\n",
      "37/111\n",
      "38/111\n",
      "39/111\n",
      "40/111\n",
      "41/111\n",
      "42/111\n",
      "43/111\n",
      "44/111\n",
      "45/111\n",
      "46/111\n",
      "47/111\n",
      "48/111\n",
      "49/111\n",
      "50/111\n",
      "51/111\n",
      "52/111\n",
      "53/111\n",
      "54/111\n",
      "55/111\n",
      "56/111\n",
      "57/111\n",
      "58/111\n",
      "59/111\n",
      "60/111\n",
      "61/111\n",
      "62/111\n",
      "63/111\n",
      "64/111\n",
      "65/111\n",
      "66/111\n",
      "67/111\n",
      "68/111\n",
      "69/111\n",
      "70/111\n",
      "71/111\n",
      "72/111\n",
      "73/111\n",
      "74/111\n",
      "75/111\n",
      "76/111\n",
      "77/111\n",
      "78/111\n",
      "79/111\n",
      "80/111\n",
      "81/111\n",
      "82/111\n",
      "83/111\n",
      "84/111\n",
      "85/111\n",
      "86/111\n",
      "87/111\n",
      "88/111\n",
      "89/111\n",
      "90/111\n",
      "91/111\n",
      "92/111\n",
      "93/111\n",
      "94/111\n",
      "95/111\n",
      "96/111\n",
      "97/111\n",
      "98/111\n",
      "99/111\n",
      "100/111\n",
      "101/111\n",
      "102/111\n",
      "103/111\n",
      "104/111\n",
      "105/111\n",
      "106/111\n",
      "107/111\n",
      "108/111\n",
      "109/111\n",
      "110/111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 1/5 [05:02<20:08, 302.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/111\n",
      "1/111\n",
      "2/111\n",
      "3/111\n",
      "4/111\n",
      "5/111\n",
      "6/111\n",
      "7/111\n",
      "8/111\n",
      "9/111\n",
      "10/111\n",
      "11/111\n",
      "12/111\n",
      "13/111\n",
      "14/111\n",
      "15/111\n",
      "16/111\n",
      "17/111\n",
      "18/111\n",
      "19/111\n",
      "20/111\n",
      "21/111\n",
      "22/111\n",
      "23/111\n",
      "24/111\n",
      "25/111\n",
      "26/111\n",
      "27/111\n",
      "28/111\n",
      "29/111\n",
      "30/111\n",
      "31/111\n",
      "32/111\n",
      "33/111\n",
      "34/111\n",
      "35/111\n",
      "36/111\n",
      "37/111\n",
      "38/111\n",
      "39/111\n",
      "40/111\n",
      "41/111\n",
      "42/111\n",
      "43/111\n",
      "44/111\n",
      "45/111\n",
      "46/111\n",
      "47/111\n",
      "48/111\n",
      "49/111\n",
      "50/111\n",
      "51/111\n",
      "52/111\n",
      "53/111\n",
      "54/111\n",
      "55/111\n",
      "56/111\n",
      "57/111\n",
      "58/111\n",
      "59/111\n",
      "60/111\n",
      "61/111\n",
      "62/111\n",
      "63/111\n",
      "64/111\n",
      "65/111\n",
      "66/111\n",
      "67/111\n",
      "68/111\n",
      "69/111\n",
      "70/111\n",
      "71/111\n",
      "72/111\n",
      "73/111\n",
      "74/111\n",
      "75/111\n",
      "76/111\n",
      "77/111\n",
      "78/111\n",
      "79/111\n",
      "80/111\n",
      "81/111\n",
      "82/111\n",
      "83/111\n",
      "84/111\n",
      "85/111\n",
      "86/111\n",
      "87/111\n",
      "88/111\n",
      "89/111\n",
      "90/111\n",
      "91/111\n",
      "92/111\n",
      "93/111\n",
      "94/111\n",
      "95/111\n",
      "96/111\n",
      "97/111\n",
      "98/111\n",
      "99/111\n",
      "100/111\n",
      "101/111\n",
      "102/111\n",
      "103/111\n",
      "104/111\n",
      "105/111\n",
      "106/111\n",
      "107/111\n",
      "108/111\n",
      "109/111\n",
      "110/111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 2/5 [10:01<15:03, 301.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/111\n",
      "1/111\n",
      "2/111\n",
      "3/111\n",
      "4/111\n",
      "5/111\n",
      "6/111\n",
      "7/111\n",
      "8/111\n",
      "9/111\n",
      "10/111\n",
      "11/111\n",
      "12/111\n",
      "13/111\n",
      "14/111\n",
      "15/111\n",
      "16/111\n",
      "17/111\n",
      "18/111\n",
      "19/111\n",
      "20/111\n",
      "21/111\n",
      "22/111\n",
      "23/111\n",
      "24/111\n",
      "25/111\n",
      "26/111\n",
      "27/111\n",
      "28/111\n",
      "29/111\n",
      "30/111\n",
      "31/111\n",
      "32/111\n",
      "33/111\n",
      "34/111\n",
      "35/111\n",
      "36/111\n",
      "37/111\n",
      "38/111\n",
      "39/111\n",
      "40/111\n",
      "41/111\n",
      "42/111\n",
      "43/111\n",
      "44/111\n",
      "45/111\n",
      "46/111\n",
      "47/111\n",
      "48/111\n",
      "49/111\n",
      "50/111\n",
      "51/111\n",
      "52/111\n",
      "53/111\n",
      "54/111\n",
      "55/111\n",
      "56/111\n",
      "57/111\n",
      "58/111\n",
      "59/111\n",
      "60/111\n",
      "61/111\n",
      "62/111\n",
      "63/111\n",
      "64/111\n",
      "65/111\n",
      "66/111\n",
      "67/111\n",
      "68/111\n",
      "69/111\n",
      "70/111\n",
      "71/111\n",
      "72/111\n",
      "73/111\n",
      "74/111\n",
      "75/111\n",
      "76/111\n",
      "77/111\n",
      "78/111\n",
      "79/111\n",
      "80/111\n",
      "81/111\n",
      "82/111\n",
      "83/111\n",
      "84/111\n",
      "85/111\n",
      "86/111\n",
      "87/111\n",
      "88/111\n",
      "89/111\n",
      "90/111\n",
      "91/111\n",
      "92/111\n",
      "93/111\n",
      "94/111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-538cd8847057>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0my_pred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mlist_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mlist_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 9. Time Test Augmentation\n",
    "## Time Test augmentations on validation set (for training blend model) and  \n",
    "\n",
    "path_output = './02_Files_TTA_Blend/'\n",
    "qt_augmentations = 10\n",
    "batch_size = 8\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "pbar = tqdm(total=5, position=0)\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "dict_predictions = {}\n",
    "\n",
    "for num_fold, (train_index, val_index) in enumerate(kf.split(list_segments_train,\n",
    "                                                     np.zeros(len(list_segments_train)))):\n",
    "    \n",
    "    segments_train_fold = np.asarray(list_segments_train)[train_index]\n",
    "    segments_val_fold = np.asarray(list_segments_train)[val_index]\n",
    "    \n",
    "    X_val_generator = VolcanoSequencesGenerator(segments_val_fold, \n",
    "                                                  dict_segments_sequences_paths_train,\n",
    "                                                  batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                  augmentations=True, training=True)   \n",
    "    \n",
    "    y_val_target = np.asarray([dict_labels[segment] for segment in segments_val_fold])\n",
    "        \n",
    "    idx = 0\n",
    "    num_steps = int(np.ceil(segments_val_fold.shape[0]/ batch_size))\n",
    "    list_segments = list(segments_val_fold)\n",
    "    \n",
    "    array_predictions = np.zeros((len(list_segments)), dtype=np.float32)\n",
    "    array_confidence = np.zeros((len(list_segments)), dtype=np.float32)\n",
    "    array_predictions_std = np.zeros((len(list_segments)), dtype=np.float32)\n",
    "    array_confidence_std = np.zeros((len(list_segments)), dtype=np.float32)\n",
    "    \n",
    "    model = models.load_model(f'./models/model_mha_{num_fold}', compile=False)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        print(f'{i}/{num_steps}')\n",
    "        list_tmp_segments = list_segments[idx:(idx+batch_size)]\n",
    "        list_predictions, list_conf = [], []\n",
    "        for i in range(qt_augmentations):\n",
    "            \n",
    "            X_val_sequences = X_val_generator.generateOrderedSequences(list_tmp_segments)\n",
    "            y_pred_val = model(X_val_sequences)\n",
    "            \n",
    "            list_predictions.append(y_pred_val[0].numpy())\n",
    "            list_conf.append(y_pred_val[1].numpy()[:, 2] - y_pred_val[1].numpy()[:, 0])\n",
    "\n",
    "        array_predictions[idx:(idx+batch_size)] = np.asarray(list_predictions).mean(axis=0).squeeze().astype(np.float32)\n",
    "        array_confidence[idx:(idx+batch_size)] = np.asarray(list_conf).mean(axis=0).squeeze().astype(np.float32)\n",
    "        array_predictions_std[idx:(idx+batch_size)] = np.asarray(list_predictions).std(axis=0).squeeze().astype(np.float32)\n",
    "        array_confidence_std[idx:(idx+batch_size)] = np.asarray(list_conf).std(axis=0).squeeze().astype(np.float32)\n",
    "        idx += batch_size\n",
    "        \n",
    "        \n",
    "    dict_predictions[num_fold] = {\n",
    "        'segment_id' : segments_val_fold, 'y_true' : y_val_target,\n",
    "        'pred_mean': array_predictions, 'pred_std': array_predictions_std,\n",
    "        'conf_mean' : array_confidence, 'conf_std' : array_confidence_std\n",
    "    }\n",
    "    \n",
    "    pbar.update(1)\n",
    "    \n",
    "    \n",
    "pbar.close()\n",
    "\n",
    "# Prepare output\n",
    "\n",
    "columns_names = ['segment_id', 'y_true', 'mha_pred_mean', 'mha_pred_std', 'mha_conf_mean', 'mha_conf_std']\n",
    "dict_df = {}\n",
    "for col in columns_names:\n",
    "    if col.split('_')[0] in ['mha']:\n",
    "        key_1, key_2, key_3 = col.split('_')[0], col.split('_')[1], '_'.join([col.split('_')[2], col.split('_')[3]])\n",
    "        dict_df[col] = dict_predictions[key_1][int(key_2)][key_3]\n",
    "    else:\n",
    "        dict_df[col] = np.concatenate(list(dict_predictions[col].values()))\n",
    "\n",
    "\n",
    "dict_build_df = {\n",
    "    'segment_id' : dict_df['segment_id'].squeeze(),\n",
    "    'mha_pred_mean' : np.concatenate([dict_df['mha_0_pred_mean'], dict_df['mha_1_pred_mean'], dict_df['mha_2_pred_mean'],\n",
    "                                      dict_df['mha_3_pred_mean'], dict_df['mha_4_pred_mean']]).squeeze(),\n",
    "    'mha_pred_std' : np.concatenate([dict_df['mha_0_pred_std'], dict_df['mha_1_pred_std'], dict_df['mha_2_pred_std'],\n",
    "                                     dict_df['mha_3_pred_std'], dict_df['mha_4_pred_std']]).squeeze(),\n",
    "    'mha_conf_mean' : np.concatenate([dict_df['mha_0_conf_mean'], dict_df['mha_1_conf_mean'], dict_df['mha_2_conf_mean'],\n",
    "                                      dict_df['mha_3_conf_mean'], dict_df['mha_4_conf_mean']]).squeeze(),\n",
    "    'mha_conf_std' : np.concatenate([dict_df['mha_0_conf_std'], dict_df['mha_1_conf_std'], dict_df['mha_2_conf_std'],\n",
    "                                     dict_df['mha_3_conf_std'], dict_df['mha_4_conf_std']]).squeeze()\n",
    "}    \n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
