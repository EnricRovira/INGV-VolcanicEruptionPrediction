{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 1. Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers, metrics, losses, optimizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import imageio\n",
    "from skimage import color, io\n",
    "\n",
    "import albumentations\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 2. Paths & Global Variables\n",
    "\n",
    "## 2.1 Paths\n",
    "\n",
    "path = '../../01_Data/'\n",
    "path_sequences = path + '01_GeneratedSequences/'\n",
    "path_spectograms = path + '03_GeneratedSpectograms_Scipy/'\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(path + 'train.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv') \n",
    "\n",
    "train_paths = glob.glob(path + 'train/*')\n",
    "test_paths = glob.glob(path + 'test/*')\n",
    "\n",
    "unique_segments_id_train = set(df_train['segment_id'])\n",
    "unique_segments_id_test = set(df_sample_submission['segment_id'])\n",
    "\n",
    "dict_unique_segments_id = { v : k for k, v in enumerate(unique_segments_id_train)}\n",
    "dict_unique_segments_id_inv = { k : v for k, v in enumerate(unique_segments_id_train)}\n",
    "\n",
    "## 2.2 Global Variables\n",
    "\n",
    "SEQ_LENGTH = 60_001\n",
    "IMG_SIZE = (128, 235)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 3. Global Functions\n",
    "\n",
    "\n",
    "def scale(x, mean_, std_):\n",
    "    return (x - mean_) / std_\n",
    "\n",
    "\n",
    "def unscale(x, mean_, std_):\n",
    "    return (x * std_) + mean_\n",
    "\n",
    "def getdDictsSpectoGramsNulls(dict_segments_paths):\n",
    "    dict_results = {}\n",
    "    for segment in tqdm(dict_segments_paths, total=len(dict_segments_paths), position=0):\n",
    "        df = pd.read_csv(dict_segments_paths[segment])\n",
    "        dict_results[segment] = df.isna().sum().values/SEQ_LENGTH      \n",
    "    return dict_results\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 4. Preprocess\n",
    "\n",
    "\n",
    "dict_segment_paths_train = {\n",
    "    segment : path + 'train/' + str(segment) + '.csv' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segment_paths_test = {\n",
    "    segment : path + 'test/' + str(segment) + '.csv' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "dict_segments_spectograms_paths_train = {\n",
    "    segment : path_spectograms + 'train/' + str(segment) + '/'\n",
    "    for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_spectograms_paths_test = {\n",
    "    segment : path_spectograms + 'test/' + str(segment) + '/'\n",
    "    for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "# mean_time_to_eruption, std_time_to_eruption = df_train['time_to_eruption'].mean(), df_train['time_to_eruption'].std()\n",
    "\n",
    "df_train['time_to_eruption'] = df_train['time_to_eruption']/(10**6)\n",
    "# df_train['time_to_eruption'] = df_train['time_to_eruption'].apply(lambda x: scale(x, mean_time_to_eruption, std_time_to_eruption))\n",
    "\n",
    "median_time_to_eruptions = np.quantile(df_train['time_to_eruption'], 0.5)\n",
    "\n",
    "dict_labels = {\n",
    "    segment: df_train['time_to_eruption'][df_train['segment_id']==segment].values.flatten()\n",
    "    for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 5. Data Generator\n",
    "\n",
    "def displayImage(image, figsize=(20, 15)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 5.1 - Data Augmentation\n",
    "\n",
    "def getTrainTransforms():\n",
    "    return albumentations.Compose([\n",
    "            albumentations.OneOf([\n",
    "                albumentations.GaussNoise(p=0.2),\n",
    "                albumentations.Cutout(num_holes=8, max_h_size=12, max_w_size=12, fill_value=0, p=0.2),\n",
    "            ], p=0.3),\n",
    "        albumentations.OpticalDistortion(p=0.3),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.05, rotate_limit=1, p=0.5),\n",
    "        albumentations.RandomCrop(IMG_SIZE[0]-10, IMG_SIZE[1]-10, p=0.5),\n",
    "        albumentations.PadIfNeeded(min_height=IMG_SIZE[0], min_width=IMG_SIZE[1], value=0, p=1.0)\n",
    "    ])\n",
    "\n",
    "\n",
    "class VolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_spectograms, batch_size, dict_labels, transforms, training=True):\n",
    "        super(VolcanoSequencesGenerator, self).__init__()\n",
    "        \n",
    "        self.segments = segments\n",
    "        self.path_spectograms = path_spectograms\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.transforms = transforms\n",
    "        self.training = training\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        array_spectograms_s0 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_0.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s1 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_1.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s2 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_2.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s3 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_3.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s4 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_4.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s5 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_5.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s6 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_6.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s7 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_7.npy', \n",
    "                                                            allow_pickle=True) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s8 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_8.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s9 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_9.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]) .astype(np.uint8)  \n",
    "        \n",
    "\n",
    "        if self.transforms:\n",
    "            data_s0, data_s1 = {'image':array_spectograms_s0}, {'image':array_spectograms_s1}\n",
    "            data_s2, data_s3 = {'image':array_spectograms_s2}, {'image':array_spectograms_s3}\n",
    "            data_s4, data_s5 = {'image':array_spectograms_s4}, {'image':array_spectograms_s5}\n",
    "            data_s6, data_s7 = {'image':array_spectograms_s6}, {'image':array_spectograms_s7}\n",
    "            data_s8, data_s9 = {'image':array_spectograms_s8}, {'image':array_spectograms_s9}\n",
    "            \n",
    "            array_spectograms_s0 = np.stack([self.transforms(image=x)['image'] for x in data_s0['image']], axis=0)\n",
    "            array_spectograms_s1 = np.stack([self.transforms(image=x)['image'] for x in data_s1['image']], axis=0)\n",
    "            array_spectograms_s2 = np.stack([self.transforms(image=x)['image'] for x in data_s2['image']], axis=0)\n",
    "            array_spectograms_s3 = np.stack([self.transforms(image=x)['image'] for x in data_s3['image']], axis=0)\n",
    "            array_spectograms_s4 = np.stack([self.transforms(image=x)['image'] for x in data_s4['image']], axis=0)\n",
    "            array_spectograms_s5 = np.stack([self.transforms(image=x)['image'] for x in data_s5['image']], axis=0)\n",
    "            array_spectograms_s6 = np.stack([self.transforms(image=x)['image'] for x in data_s6['image']], axis=0)\n",
    "            array_spectograms_s7 = np.stack([self.transforms(image=x)['image'] for x in data_s7['image']], axis=0)\n",
    "            array_spectograms_s8 = np.stack([self.transforms(image=x)['image'] for x in data_s8['image']], axis=0)\n",
    "            array_spectograms_s9 = np.stack([self.transforms(image=x)['image'] for x in data_s9['image']], axis=0)\n",
    "                 \n",
    "        batch = (array_spectograms_s0/255, array_spectograms_s1/255, array_spectograms_s2/255, array_spectograms_s3/255, \n",
    "                 array_spectograms_s4/255, array_spectograms_s5/255, array_spectograms_s6/255, array_spectograms_s7/255, \n",
    "                 array_spectograms_s8/255, array_spectograms_s9/255)   \n",
    "            \n",
    "            \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "            return batch, array_labels\n",
    "        else:\n",
    "            return batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        \n",
    "        \n",
    "    def generateOrderedSequences(self, list_segments):\n",
    "        array_spectograms_s0 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_0.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s1 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_1.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s2 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_2.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s3 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_3.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s4 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_4.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s5 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_5.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s6 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_6.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s7 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_7.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s8 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_8.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        array_spectograms_s9 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_9.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_segments]).astype(np.uint8) \n",
    "        \n",
    "         \n",
    "        batch = (array_spectograms_s0/255, array_spectograms_s1/255, array_spectograms_s2/255, array_spectograms_s3/255, \n",
    "                 array_spectograms_s4/255, array_spectograms_s5/255, array_spectograms_s6/255, array_spectograms_s7/255, \n",
    "                 array_spectograms_s8/255, array_spectograms_s9/255)     \n",
    "        \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_segments])\n",
    "        if self.training:\n",
    "            return batch, array_labels\n",
    "        else:\n",
    "            return batch\n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tmp_gen = VolcanoSequencesGenerator(list(unique_segments_id_train), dict_segments_spectograms_paths_train,\n",
    "#                                                 batch_size=2, dict_labels=dict_labels, \n",
    "#                                                 transforms=getTrainTransforms(),\n",
    "#                                                 training=True)\n",
    "\n",
    "# for batch in tmp_gen:\n",
    "#     break\n",
    "    \n",
    "# print(batch[0][0].shape)\n",
    "# displayImage(batch[0][0][0], figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 6. Models\n",
    "\n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)     \n",
    "\n",
    "# Custom Loss\n",
    "\n",
    "def quantileLoss(y_true, y_pred):\n",
    "    quantiles = tf.constant([0.4, 0.5, 0.6])\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(quantiles * e, (quantiles-1) * e)\n",
    "    return tf.reduce_mean(v)\n",
    "\n",
    "\n",
    "# Conv Model\n",
    "\n",
    "class ConvModelSensor(models.Model):\n",
    "    def __init__(self):\n",
    "        super(ConvModelSensor, self).__init__()\n",
    "        \n",
    "        self.block0 = IdentityBlock(filters=[32, 32, 32], kernel_size=(7, 7), reg_factor=1e-4)\n",
    "        self.block1 = IdentityBlock(filters=[32, 32, 32], kernel_size=(7, 7), reg_factor=1e-4)\n",
    "        self.block2 = IdentityBlock(filters=[64, 64, 64], kernel_size=(5, 5), reg_factor=1e-4)\n",
    "        self.block3 = IdentityBlock(filters=[128, 128, 128], kernel_size=(3, 3), reg_factor=1e-4)\n",
    "        \n",
    "        self.conv0 = layers.Conv2D(filters=16, kernel_size=(9, 9), \n",
    "                                  padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.bn0 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(filters=32, kernel_size=(7, 7), \n",
    "                                  padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.conv2 = layers.Conv2D(filters=32, kernel_size=(5, 5), \n",
    "                                  padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.conv3 = layers.Conv2D(filters=64, kernel_size=(5, 5), \n",
    "                          padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.conv4 = layers.Conv2D(filters=128, kernel_size=(3, 3), \n",
    "                          padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
    "        \n",
    "        self.drop0 = layers.Dropout(0.2)\n",
    "        self.drop1 = layers.Dropout(0.2)\n",
    "        self.drop2 = layers.Dropout(0.2)\n",
    "        self.drop3 = layers.Dropout(0.2)\n",
    "        \n",
    "#         self.max_pool0 = layers.MaxPool2D((2, 1))\n",
    "        self.max_pool0 = layers.MaxPool2D((1, 2))\n",
    "        self.max_pool1 = layers.MaxPool2D(2)\n",
    "        self.max_pool2 = layers.MaxPool2D(2)\n",
    "        self.max_pool3 = layers.MaxPool2D(2)\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        \n",
    "        x = self.conv0(inputs)\n",
    "        x = self.bn0(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.max_pool0(x)\n",
    "        \n",
    "        x = self.drop0(x, training=training)\n",
    "        x = self.conv1(x)\n",
    "        x = self.block0(x)\n",
    "        x = self.max_pool1(x)\n",
    "        \n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.conv2(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.max_pool2(x)\n",
    "        \n",
    "        x = self.drop2(x, training=training)\n",
    "        x = self.conv3(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.max_pool3(x)\n",
    "        \n",
    "        x = self.drop3(x, training=training)\n",
    "        x = self.conv4(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class IdentityBlock(models.Model):\n",
    "    def __init__(self, filters=[32, 32, 32], kernel_size=(3, 3), reg_factor=1e-4):\n",
    "            super(IdentityBlock, self).__init__()\n",
    "\n",
    "            self.l0 = layers.Conv2D(filters=filters[0], kernel_size=(1, 1), \n",
    "                              padding='valid', kernel_regularizer=regularizers.l2(reg_factor))\n",
    "            self.l1 = layers.BatchNormalization()\n",
    "\n",
    "            self.l2 = layers.Conv2D(filters=filters[1], kernel_size=kernel_size, \n",
    "                              padding='same', kernel_regularizer=regularizers.l2(reg_factor))\n",
    "            self.l3 = layers.BatchNormalization()\n",
    "\n",
    "            self.l4 = layers.Conv2D(filters=filters[2], kernel_size=(1, 1), \n",
    "                              padding='valid', kernel_regularizer=regularizers.l2(reg_factor))\n",
    "            self.l5 = layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        x_residual = inputs\n",
    "        \n",
    "        x = self.l0(inputs)\n",
    "        x = self.l1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        \n",
    "        x = tf.add(x, x_residual)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "def buildModel(size, summary=False):\n",
    "    \n",
    "    block0 = ConvModelSensor()\n",
    "    block1 = ConvModelSensor()\n",
    "    block2 = ConvModelSensor()\n",
    "    block3 = ConvModelSensor()\n",
    "    block4 = ConvModelSensor()\n",
    "    block5 = ConvModelSensor()\n",
    "    block6 = ConvModelSensor()\n",
    "    block7 = ConvModelSensor()\n",
    "    block8 = ConvModelSensor()\n",
    "    block9 = ConvModelSensor()\n",
    "    \n",
    "#     mha_model = MHAttentionModel(num_heads, d_model)\n",
    "    \n",
    "    in_0 = layers.Input(shape=(128, 235, 1))\n",
    "    in_1 = layers.Input(shape=(128, 235, 1))\n",
    "    in_2 = layers.Input(shape=(128, 235, 1))\n",
    "    in_3 = layers.Input(shape=(128, 235, 1))\n",
    "    in_4 = layers.Input(shape=(128, 235, 1))\n",
    "    in_5 = layers.Input(shape=(128, 235, 1))\n",
    "    in_6 = layers.Input(shape=(128, 235, 1))\n",
    "    in_7 = layers.Input(shape=(128, 235, 1))\n",
    "    in_8 = layers.Input(shape=(128, 235, 1))\n",
    "    in_9 = layers.Input(shape=(128, 235, 1))\n",
    "    \n",
    "    x0 = block0(in_0)\n",
    "    x1 = block1(in_1)\n",
    "    x2 = block2(in_2)\n",
    "    x3 = block3(in_3)\n",
    "    x4 = block4(in_4)\n",
    "    x5 = block5(in_5)\n",
    "    x6 = block6(in_6)\n",
    "    x7 = block7(in_7)\n",
    "    x8 = block8(in_8)\n",
    "    x9 = block9(in_9)\n",
    "    \n",
    "    ##################################################\n",
    "    \n",
    "    x0 = layers.GlobalAveragePooling2D()(x0)\n",
    "    x1 = layers.GlobalAveragePooling2D()(x1)\n",
    "    x2 = layers.GlobalAveragePooling2D()(x2)\n",
    "    x3 = layers.GlobalAveragePooling2D()(x3)\n",
    "    x4 = layers.GlobalAveragePooling2D()(x4)\n",
    "    x5 = layers.GlobalAveragePooling2D()(x5)\n",
    "    x6 = layers.GlobalAveragePooling2D()(x6)\n",
    "    x7 = layers.GlobalAveragePooling2D()(x7)\n",
    "    x8 = layers.GlobalAveragePooling2D()(x8)\n",
    "    x9 = layers.GlobalAveragePooling2D()(x9)\n",
    "    \n",
    "    x = layers.concatenate([x0, x1, x2, x3, x4, x5, x6, x7, x8, x9])\n",
    "\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(300, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(64, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    ##################################################\n",
    "    \n",
    "    out_1 = layers.Dense(1, activation='relu', name='time_to_eruption')(x)\n",
    "    out_2 = layers.Dense(3, activation='relu', name='quantile')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[in_0, in_1, in_2, in_3, in_4,\n",
    "                                 in_5, in_6, in_7, in_8, in_9], \n",
    "                         outputs=[out_1, out_2])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=8e-4), \n",
    "                  \n",
    "                  loss=[losses.MeanAbsoluteError(), quantileLoss],\n",
    "                  loss_weights=[4, 1],\n",
    "                  metrics=['mae'])\n",
    "    if summary:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0:\n",
    "        return lr*0.9\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Num Fold: 2\n",
      "Train segments: 3545 Val segments: 886\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor (ConvModelSen (None, 16, 14, 128)  576352      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_1 (ConvModelS (None, 16, 14, 128)  576352      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_2 (ConvModelS (None, 16, 14, 128)  576352      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_3 (ConvModelS (None, 16, 14, 128)  576352      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_4 (ConvModelS (None, 16, 14, 128)  576352      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_5 (ConvModelS (None, 16, 14, 128)  576352      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_6 (ConvModelS (None, 16, 14, 128)  576352      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_7 (ConvModelS (None, 16, 14, 128)  576352      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_8 (ConvModelS (None, 16, 14, 128)  576352      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_9 (ConvModelS (None, 16, 14, 128)  576352      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 128)          0           conv_model_sensor[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 128)          0           conv_model_sensor_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 128)          0           conv_model_sensor_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 128)          0           conv_model_sensor_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 128)          0           conv_model_sensor_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 128)          0           conv_model_sensor_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 128)          0           conv_model_sensor_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 128)          0           conv_model_sensor_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glo (None, 128)          0           conv_model_sensor_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_9 (Glo (None, 128)          0           conv_model_sensor_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1280)         0           global_average_pooling2d[0][0]   \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "                                                                 global_average_pooling2d_3[0][0] \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "                                                                 global_average_pooling2d_5[0][0] \n",
      "                                                                 global_average_pooling2d_6[0][0] \n",
      "                                                                 global_average_pooling2d_7[0][0] \n",
      "                                                                 global_average_pooling2d_8[0][0] \n",
      "                                                                 global_average_pooling2d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 1280)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300)          384300      dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 300)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 300)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           19264       dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 64)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_to_eruption (Dense)        (None, 1)            65          dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "quantile (Dense)                (None, 3)            195         dropout_42[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,167,344\n",
      "Trainable params: 6,151,664\n",
      "Non-trainable params: 15,680\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "444/444 [==============================] - 218s 492ms/step - loss: 55.5359 - time_to_eruption_loss: 12.0595 - quantile_loss: 6.2619 - time_to_eruption_mae: 12.0595 - quantile_mae: 12.8584 - val_loss: 80.8045 - val_time_to_eruption_loss: 17.9947 - val_quantile_loss: 7.7656 - val_time_to_eruption_mae: 17.9947 - val_quantile_mae: 16.4484\n",
      "Epoch 2/100\n",
      "444/444 [==============================] - 136s 306ms/step - loss: 51.9920 - time_to_eruption_loss: 11.3085 - quantile_loss: 5.6825 - time_to_eruption_mae: 11.3085 - quantile_mae: 11.8234 - val_loss: 51.2656 - val_time_to_eruption_loss: 11.1775 - val_quantile_loss: 5.4694 - val_time_to_eruption_mae: 11.1775 - val_quantile_mae: 11.3483\n",
      "Epoch 3/100\n",
      "444/444 [==============================] - 136s 306ms/step - loss: 50.4265 - time_to_eruption_loss: 10.9564 - quantile_loss: 5.5055 - time_to_eruption_mae: 10.9564 - quantile_mae: 11.4984 - val_loss: 61.6493 - val_time_to_eruption_loss: 13.5349 - val_quantile_loss: 6.4051 - val_time_to_eruption_mae: 13.5349 - val_quantile_mae: 13.2703\n",
      "Epoch 4/100\n",
      "444/444 [==============================] - 137s 310ms/step - loss: 49.0399 - time_to_eruption_loss: 10.6608 - quantile_loss: 5.2878 - time_to_eruption_mae: 10.6608 - quantile_mae: 10.9885 - val_loss: 52.1931 - val_time_to_eruption_loss: 11.3917 - val_quantile_loss: 5.5109 - val_time_to_eruption_mae: 11.3917 - val_quantile_mae: 11.4041\n",
      "Epoch 5/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 48.5098 - time_to_eruption_loss: 10.5414 - quantile_loss: 5.2217 - time_to_eruption_mae: 10.5414 - quantile_mae: 10.8935 - val_loss: 53.8694 - val_time_to_eruption_loss: 11.7294 - val_quantile_loss: 5.8213 - val_time_to_eruption_mae: 11.7294 - val_quantile_mae: 12.1976\n",
      "Epoch 6/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 47.9093 - time_to_eruption_loss: 10.4000 - quantile_loss: 5.1648 - time_to_eruption_mae: 10.4000 - quantile_mae: 10.7555 - val_loss: 46.9817 - val_time_to_eruption_loss: 10.2029 - val_quantile_loss: 5.0159 - val_time_to_eruption_mae: 10.2029 - val_quantile_mae: 10.4862\n",
      "Epoch 7/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 46.9685 - time_to_eruption_loss: 10.1922 - quantile_loss: 5.0372 - time_to_eruption_mae: 10.1922 - quantile_mae: 10.4856 - val_loss: 40.8178 - val_time_to_eruption_loss: 8.8106 - val_quantile_loss: 4.3999 - val_time_to_eruption_mae: 8.8106 - val_quantile_mae: 9.3609\n",
      "Epoch 8/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 45.7053 - time_to_eruption_loss: 9.9092 - quantile_loss: 4.8868 - time_to_eruption_mae: 9.9092 - quantile_mae: 10.2221 - val_loss: 48.3938 - val_time_to_eruption_loss: 10.4969 - val_quantile_loss: 5.2214 - val_time_to_eruption_mae: 10.4969 - val_quantile_mae: 10.8662\n",
      "Epoch 9/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 42.7931 - time_to_eruption_loss: 9.2572 - quantile_loss: 4.5813 - time_to_eruption_mae: 9.2572 - quantile_mae: 9.5445 - val_loss: 46.8312 - val_time_to_eruption_loss: 10.1454 - val_quantile_loss: 5.0638 - val_time_to_eruption_mae: 10.1454 - val_quantile_mae: 10.4980\n",
      "Epoch 10/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 41.4730 - time_to_eruption_loss: 8.9633 - quantile_loss: 4.4291 - time_to_eruption_mae: 8.9633 - quantile_mae: 9.1991 - val_loss: 42.0282 - val_time_to_eruption_loss: 9.1042 - val_quantile_loss: 4.4219 - val_time_to_eruption_mae: 9.1042 - val_quantile_mae: 9.1016\n",
      "Epoch 11/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 40.6930 - time_to_eruption_loss: 8.7891 - quantile_loss: 4.3486 - time_to_eruption_mae: 8.7891 - quantile_mae: 9.0307 - val_loss: 60.4081 - val_time_to_eruption_loss: 13.2290 - val_quantile_loss: 6.2992 - val_time_to_eruption_mae: 13.2290 - val_quantile_mae: 13.0567\n",
      "Epoch 12/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 38.7109 - time_to_eruption_loss: 8.3481 - quantile_loss: 4.1263 - time_to_eruption_mae: 8.3481 - quantile_mae: 8.5757 - val_loss: 34.7103 - val_time_to_eruption_loss: 7.4580 - val_quantile_loss: 3.6902 - val_time_to_eruption_mae: 7.4580 - val_quantile_mae: 7.7675\n",
      "Epoch 13/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 37.7587 - time_to_eruption_loss: 8.1362 - quantile_loss: 4.0302 - time_to_eruption_mae: 8.1362 - quantile_mae: 8.3873 - val_loss: 42.5281 - val_time_to_eruption_loss: 9.2007 - val_quantile_loss: 4.5469 - val_time_to_eruption_mae: 9.2007 - val_quantile_mae: 9.4970\n",
      "Epoch 14/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 36.0488 - time_to_eruption_loss: 7.7618 - quantile_loss: 3.8274 - time_to_eruption_mae: 7.7618 - quantile_mae: 7.9570 - val_loss: 41.0320 - val_time_to_eruption_loss: 8.8505 - val_quantile_loss: 4.4576 - val_time_to_eruption_mae: 8.8505 - val_quantile_mae: 9.2436\n",
      "Epoch 15/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 37.2900 - time_to_eruption_loss: 8.0399 - quantile_loss: 3.9566 - time_to_eruption_mae: 8.0399 - quantile_mae: 8.2151 - val_loss: 36.2538 - val_time_to_eruption_loss: 7.8053 - val_quantile_loss: 3.8582 - val_time_to_eruption_mae: 7.8053 - val_quantile_mae: 7.9962\n",
      "Epoch 16/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 36.1406 - time_to_eruption_loss: 7.7815 - quantile_loss: 3.8295 - time_to_eruption_mae: 7.7815 - quantile_mae: 7.9473 - val_loss: 36.0150 - val_time_to_eruption_loss: 7.7433 - val_quantile_loss: 3.8518 - val_time_to_eruption_mae: 7.7433 - val_quantile_mae: 7.9753\n",
      "Epoch 17/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 35.5449 - time_to_eruption_loss: 7.6477 - quantile_loss: 3.7581 - time_to_eruption_mae: 7.6477 - quantile_mae: 7.8190 - val_loss: 31.8861 - val_time_to_eruption_loss: 6.8352 - val_quantile_loss: 3.3443 - val_time_to_eruption_mae: 6.8352 - val_quantile_mae: 6.9812\n",
      "Epoch 18/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 36.0724 - time_to_eruption_loss: 7.7629 - quantile_loss: 3.8277 - time_to_eruption_mae: 7.7629 - quantile_mae: 7.9339 - val_loss: 35.7017 - val_time_to_eruption_loss: 7.6842 - val_quantile_loss: 3.7761 - val_time_to_eruption_mae: 7.6842 - val_quantile_mae: 7.9087\n",
      "Epoch 19/100\n",
      "444/444 [==============================] - 137s 310ms/step - loss: 34.2008 - time_to_eruption_loss: 7.3512 - quantile_loss: 3.6180 - time_to_eruption_mae: 7.3512 - quantile_mae: 7.5206 - val_loss: 42.3252 - val_time_to_eruption_loss: 9.1521 - val_quantile_loss: 4.5466 - val_time_to_eruption_mae: 9.1521 - val_quantile_mae: 9.4667\n",
      "Epoch 20/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 34.1482 - time_to_eruption_loss: 7.3442 - quantile_loss: 3.6083 - time_to_eruption_mae: 7.3442 - quantile_mae: 7.5144 - val_loss: 44.3794 - val_time_to_eruption_loss: 9.6258 - val_quantile_loss: 4.7173 - val_time_to_eruption_mae: 9.6258 - val_quantile_mae: 9.7787\n",
      "Epoch 21/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 33.2686 - time_to_eruption_loss: 7.1517 - quantile_loss: 3.5145 - time_to_eruption_mae: 7.1517 - quantile_mae: 7.3144 - val_loss: 38.4054 - val_time_to_eruption_loss: 8.3012 - val_quantile_loss: 4.0642 - val_time_to_eruption_mae: 8.3012 - val_quantile_mae: 8.3176\n",
      "Epoch 22/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 31.9770 - time_to_eruption_loss: 6.8672 - quantile_loss: 3.3798 - time_to_eruption_mae: 6.8672 - quantile_mae: 7.0158 - val_loss: 28.3342 - val_time_to_eruption_loss: 6.0622 - val_quantile_loss: 2.9624 - val_time_to_eruption_mae: 6.0622 - val_quantile_mae: 6.2145\n",
      "Epoch 23/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 31.5057 - time_to_eruption_loss: 6.7671 - quantile_loss: 3.3158 - time_to_eruption_mae: 6.7671 - quantile_mae: 6.8961 - val_loss: 38.6478 - val_time_to_eruption_loss: 8.3492 - val_quantile_loss: 4.1353 - val_time_to_eruption_mae: 8.3492 - val_quantile_mae: 8.4944\n",
      "Epoch 24/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 30.3580 - time_to_eruption_loss: 6.5141 - quantile_loss: 3.1943 - time_to_eruption_mae: 6.5141 - quantile_mae: 6.6404 - val_loss: 27.9236 - val_time_to_eruption_loss: 5.9775 - val_quantile_loss: 2.9107 - val_time_to_eruption_mae: 5.9775 - val_quantile_mae: 6.0491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 30.2037 - time_to_eruption_loss: 6.4820 - quantile_loss: 3.1796 - time_to_eruption_mae: 6.4820 - quantile_mae: 6.6021 - val_loss: 32.2903 - val_time_to_eruption_loss: 6.9597 - val_quantile_loss: 3.3626 - val_time_to_eruption_mae: 6.9597 - val_quantile_mae: 6.9287\n",
      "Epoch 26/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 30.0855 - time_to_eruption_loss: 6.4596 - quantile_loss: 3.1641 - time_to_eruption_mae: 6.4596 - quantile_mae: 6.5842 - val_loss: 27.8869 - val_time_to_eruption_loss: 5.9595 - val_quantile_loss: 2.9711 - val_time_to_eruption_mae: 5.9595 - val_quantile_mae: 6.1619\n",
      "Epoch 27/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 29.6420 - time_to_eruption_loss: 6.3621 - quantile_loss: 3.1183 - time_to_eruption_mae: 6.3621 - quantile_mae: 6.4769 - val_loss: 25.2122 - val_time_to_eruption_loss: 5.3698 - val_quantile_loss: 2.6591 - val_time_to_eruption_mae: 5.3698 - val_quantile_mae: 5.6146\n",
      "Epoch 28/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 29.4008 - time_to_eruption_loss: 6.3075 - quantile_loss: 3.0931 - time_to_eruption_mae: 6.3075 - quantile_mae: 6.4408 - val_loss: 27.1865 - val_time_to_eruption_loss: 5.8059 - val_quantile_loss: 2.8868 - val_time_to_eruption_mae: 5.8059 - val_quantile_mae: 6.0701\n",
      "Epoch 29/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 28.9959 - time_to_eruption_loss: 6.2175 - quantile_loss: 3.0478 - time_to_eruption_mae: 6.2175 - quantile_mae: 6.3385 - val_loss: 36.1249 - val_time_to_eruption_loss: 7.7998 - val_quantile_loss: 3.8360 - val_time_to_eruption_mae: 7.7998 - val_quantile_mae: 7.8910\n",
      "Epoch 30/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 28.8084 - time_to_eruption_loss: 6.1731 - quantile_loss: 3.0286 - time_to_eruption_mae: 6.1731 - quantile_mae: 6.2992 - val_loss: 28.3901 - val_time_to_eruption_loss: 6.0810 - val_quantile_loss: 2.9809 - val_time_to_eruption_mae: 6.0810 - val_quantile_mae: 6.1799\n",
      "Epoch 31/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 27.8364 - time_to_eruption_loss: 5.9576 - quantile_loss: 2.9231 - time_to_eruption_mae: 5.9576 - quantile_mae: 6.0749 - val_loss: 22.2042 - val_time_to_eruption_loss: 4.6952 - val_quantile_loss: 2.3459 - val_time_to_eruption_mae: 4.6952 - val_quantile_mae: 4.9298\n",
      "Epoch 32/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 27.0549 - time_to_eruption_loss: 5.7863 - quantile_loss: 2.8373 - time_to_eruption_mae: 5.7863 - quantile_mae: 5.8938 - val_loss: 25.5633 - val_time_to_eruption_loss: 5.4578 - val_quantile_loss: 2.6640 - val_time_to_eruption_mae: 5.4578 - val_quantile_mae: 5.5817\n",
      "Epoch 33/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 27.1268 - time_to_eruption_loss: 5.8045 - quantile_loss: 2.8462 - time_to_eruption_mae: 5.8045 - quantile_mae: 5.9128 - val_loss: 23.4702 - val_time_to_eruption_loss: 4.9913 - val_quantile_loss: 2.4484 - val_time_to_eruption_mae: 4.9913 - val_quantile_mae: 5.0864\n",
      "Epoch 34/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 26.7798 - time_to_eruption_loss: 5.7298 - quantile_loss: 2.8074 - time_to_eruption_mae: 5.7298 - quantile_mae: 5.8396 - val_loss: 37.4705 - val_time_to_eruption_loss: 8.0975 - val_quantile_loss: 4.0274 - val_time_to_eruption_mae: 8.0975 - val_quantile_mae: 8.2242\n",
      "Epoch 35/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 26.7274 - time_to_eruption_loss: 5.7179 - quantile_loss: 2.8015 - time_to_eruption_mae: 5.7179 - quantile_mae: 5.8208 - val_loss: 34.0874 - val_time_to_eruption_loss: 7.3487 - val_quantile_loss: 3.6349 - val_time_to_eruption_mae: 7.3487 - val_quantile_mae: 7.4556\n",
      "Epoch 36/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 26.4086 - time_to_eruption_loss: 5.6456 - quantile_loss: 2.7710 - time_to_eruption_mae: 5.6456 - quantile_mae: 5.7579 - val_loss: 23.2478 - val_time_to_eruption_loss: 4.9420 - val_quantile_loss: 2.4275 - val_time_to_eruption_mae: 4.9420 - val_quantile_mae: 5.0197\n",
      "Epoch 37/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 26.1319 - time_to_eruption_loss: 5.5837 - quantile_loss: 2.7419 - time_to_eruption_mae: 5.5837 - quantile_mae: 5.6843 - val_loss: 23.3897 - val_time_to_eruption_loss: 4.9728 - val_quantile_loss: 2.4448 - val_time_to_eruption_mae: 4.9728 - val_quantile_mae: 5.0532\n",
      "Epoch 38/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 25.3310 - time_to_eruption_loss: 5.4074 - quantile_loss: 2.6521 - time_to_eruption_mae: 5.4074 - quantile_mae: 5.5086 - val_loss: 31.6622 - val_time_to_eruption_loss: 6.8143 - val_quantile_loss: 3.3603 - val_time_to_eruption_mae: 6.8143 - val_quantile_mae: 6.9775\n",
      "Epoch 39/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 25.3142 - time_to_eruption_loss: 5.4043 - quantile_loss: 2.6511 - time_to_eruption_mae: 5.4043 - quantile_mae: 5.5112 - val_loss: 20.2780 - val_time_to_eruption_loss: 4.2752 - val_quantile_loss: 2.1281 - val_time_to_eruption_mae: 4.2752 - val_quantile_mae: 4.5006\n",
      "Epoch 40/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 25.0804 - time_to_eruption_loss: 5.3532 - quantile_loss: 2.6213 - time_to_eruption_mae: 5.3532 - quantile_mae: 5.4542 - val_loss: 27.2921 - val_time_to_eruption_loss: 5.8428 - val_quantile_loss: 2.8751 - val_time_to_eruption_mae: 5.8428 - val_quantile_mae: 5.9349\n",
      "Epoch 41/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 24.8344 - time_to_eruption_loss: 5.2989 - quantile_loss: 2.5944 - time_to_eruption_mae: 5.2989 - quantile_mae: 5.3939 - val_loss: 21.3838 - val_time_to_eruption_loss: 4.5238 - val_quantile_loss: 2.2465 - val_time_to_eruption_mae: 4.5238 - val_quantile_mae: 4.7036\n",
      "Epoch 42/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 24.4935 - time_to_eruption_loss: 5.2226 - quantile_loss: 2.5620 - time_to_eruption_mae: 5.2226 - quantile_mae: 5.3305 - val_loss: 35.0828 - val_time_to_eruption_loss: 7.5671 - val_quantile_loss: 3.7709 - val_time_to_eruption_mae: 7.5671 - val_quantile_mae: 7.7743\n",
      "Epoch 43/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 24.3061 - time_to_eruption_loss: 5.1812 - quantile_loss: 2.5400 - time_to_eruption_mae: 5.1812 - quantile_mae: 5.2754 - val_loss: 20.0268 - val_time_to_eruption_loss: 4.2320 - val_quantile_loss: 2.0542 - val_time_to_eruption_mae: 4.2320 - val_quantile_mae: 4.2990\n",
      "Epoch 44/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 23.3804 - time_to_eruption_loss: 4.9738 - quantile_loss: 2.4395 - time_to_eruption_mae: 4.9738 - quantile_mae: 5.0695 - val_loss: 29.8712 - val_time_to_eruption_loss: 6.4233 - val_quantile_loss: 3.1348 - val_time_to_eruption_mae: 6.4233 - val_quantile_mae: 6.4180\n",
      "Epoch 45/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 23.7605 - time_to_eruption_loss: 5.0598 - quantile_loss: 2.4782 - time_to_eruption_mae: 5.0598 - quantile_mae: 5.1495 - val_loss: 19.7683 - val_time_to_eruption_loss: 4.1700 - val_quantile_loss: 2.0485 - val_time_to_eruption_mae: 4.1700 - val_quantile_mae: 4.2707\n",
      "Epoch 46/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 23.5256 - time_to_eruption_loss: 5.0083 - quantile_loss: 2.4539 - time_to_eruption_mae: 5.0083 - quantile_mae: 5.0944 - val_loss: 18.0420 - val_time_to_eruption_loss: 3.7861 - val_quantile_loss: 1.8634 - val_time_to_eruption_mae: 3.7861 - val_quantile_mae: 3.9025\n",
      "Epoch 47/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 22.5611 - time_to_eruption_loss: 4.7943 - quantile_loss: 2.3516 - time_to_eruption_mae: 4.7943 - quantile_mae: 4.8771 - val_loss: 19.0376 - val_time_to_eruption_loss: 4.0050 - val_quantile_loss: 1.9844 - val_time_to_eruption_mae: 4.0050 - val_quantile_mae: 4.1567\n",
      "Epoch 48/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 22.7793 - time_to_eruption_loss: 4.8430 - quantile_loss: 2.3723 - time_to_eruption_mae: 4.8430 - quantile_mae: 4.9330 - val_loss: 21.4336 - val_time_to_eruption_loss: 4.5477 - val_quantile_loss: 2.2043 - val_time_to_eruption_mae: 4.5477 - val_quantile_mae: 4.5998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "444/444 [==============================] - 138s 312ms/step - loss: 23.4858 - time_to_eruption_loss: 5.0000 - quantile_loss: 2.4472 - time_to_eruption_mae: 5.0000 - quantile_mae: 5.0951 - val_loss: 19.4605 - val_time_to_eruption_loss: 4.0999 - val_quantile_loss: 2.0233 - val_time_to_eruption_mae: 4.0999 - val_quantile_mae: 4.2318\n",
      "Epoch 50/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 22.6709 - time_to_eruption_loss: 4.8181 - quantile_loss: 2.3629 - time_to_eruption_mae: 4.8181 - quantile_mae: 4.9152 - val_loss: 18.4581 - val_time_to_eruption_loss: 3.8772 - val_quantile_loss: 1.9143 - val_time_to_eruption_mae: 3.8772 - val_quantile_mae: 4.0218\n",
      "Epoch 51/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 21.9583 - time_to_eruption_loss: 4.6603 - quantile_loss: 2.2830 - time_to_eruption_mae: 4.6603 - quantile_mae: 4.7594 - val_loss: 20.9249 - val_time_to_eruption_loss: 4.4289 - val_quantile_loss: 2.1772 - val_time_to_eruption_mae: 4.4289 - val_quantile_mae: 4.5129\n",
      "Epoch 52/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 22.0275 - time_to_eruption_loss: 4.6762 - quantile_loss: 2.2916 - time_to_eruption_mae: 4.6762 - quantile_mae: 4.7687 - val_loss: 18.4118 - val_time_to_eruption_loss: 3.8694 - val_quantile_loss: 1.9044 - val_time_to_eruption_mae: 3.8694 - val_quantile_mae: 4.0082\n",
      "Epoch 53/100\n",
      "444/444 [==============================] - 138s 312ms/step - loss: 21.6189 - time_to_eruption_loss: 4.5861 - quantile_loss: 2.2475 - time_to_eruption_mae: 4.5861 - quantile_mae: 4.6726 - val_loss: 22.0318 - val_time_to_eruption_loss: 4.6785 - val_quantile_loss: 2.2929 - val_time_to_eruption_mae: 4.6785 - val_quantile_mae: 4.7355\n",
      "Epoch 54/100\n",
      "444/444 [==============================] - 138s 312ms/step - loss: 21.8357 - time_to_eruption_loss: 4.6347 - quantile_loss: 2.2723 - time_to_eruption_mae: 4.6347 - quantile_mae: 4.7060 - val_loss: 17.3542 - val_time_to_eruption_loss: 3.6330 - val_quantile_loss: 1.7952 - val_time_to_eruption_mae: 3.6330 - val_quantile_mae: 3.7721\n",
      "Epoch 55/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 21.9603 - time_to_eruption_loss: 4.6622 - quantile_loss: 2.2826 - time_to_eruption_mae: 4.6622 - quantile_mae: 4.7542 - val_loss: 20.3103 - val_time_to_eruption_loss: 4.2956 - val_quantile_loss: 2.0980 - val_time_to_eruption_mae: 4.2956 - val_quantile_mae: 4.3766\n",
      "Epoch 56/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 21.5514 - time_to_eruption_loss: 4.5705 - quantile_loss: 2.2405 - time_to_eruption_mae: 4.5705 - quantile_mae: 4.6610 - val_loss: 19.0839 - val_time_to_eruption_loss: 4.0218 - val_quantile_loss: 1.9667 - val_time_to_eruption_mae: 4.0218 - val_quantile_mae: 4.1134\n",
      "Epoch 57/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 21.4758 - time_to_eruption_loss: 4.5538 - quantile_loss: 2.2297 - time_to_eruption_mae: 4.5538 - quantile_mae: 4.6397 - val_loss: 19.6881 - val_time_to_eruption_loss: 4.1505 - val_quantile_loss: 2.0532 - val_time_to_eruption_mae: 4.1505 - val_quantile_mae: 4.2830\n",
      "Epoch 58/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 20.7185 - time_to_eruption_loss: 4.3851 - quantile_loss: 2.1473 - time_to_eruption_mae: 4.3851 - quantile_mae: 4.4688 - val_loss: 18.3517 - val_time_to_eruption_loss: 3.8626 - val_quantile_loss: 1.8720 - val_time_to_eruption_mae: 3.8626 - val_quantile_mae: 3.9007\n",
      "Epoch 59/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 21.0531 - time_to_eruption_loss: 4.4590 - quantile_loss: 2.1880 - time_to_eruption_mae: 4.4590 - quantile_mae: 4.5398 - val_loss: 29.0079 - val_time_to_eruption_loss: 6.2336 - val_quantile_loss: 3.0437 - val_time_to_eruption_mae: 6.2336 - val_quantile_mae: 6.2177\n",
      "Epoch 60/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 20.0243 - time_to_eruption_loss: 4.2298 - quantile_loss: 2.0754 - time_to_eruption_mae: 4.2298 - quantile_mae: 4.3159 - val_loss: 20.1125 - val_time_to_eruption_loss: 4.2549 - val_quantile_loss: 2.0624 - val_time_to_eruption_mae: 4.2549 - val_quantile_mae: 4.2528\n",
      "Epoch 61/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 20.5010 - time_to_eruption_loss: 4.3359 - quantile_loss: 2.1256 - time_to_eruption_mae: 4.3359 - quantile_mae: 4.4202 - val_loss: 15.5559 - val_time_to_eruption_loss: 3.2334 - val_quantile_loss: 1.5921 - val_time_to_eruption_mae: 3.2334 - val_quantile_mae: 3.3645\n",
      "Epoch 62/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 20.2583 - time_to_eruption_loss: 4.2828 - quantile_loss: 2.0980 - time_to_eruption_mae: 4.2828 - quantile_mae: 4.3734 - val_loss: 26.9503 - val_time_to_eruption_loss: 5.7669 - val_quantile_loss: 2.8533 - val_time_to_eruption_mae: 5.7669 - val_quantile_mae: 5.8283\n",
      "Epoch 63/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 20.2285 - time_to_eruption_loss: 4.2760 - quantile_loss: 2.0949 - time_to_eruption_mae: 4.2760 - quantile_mae: 4.3581 - val_loss: 22.5060 - val_time_to_eruption_loss: 4.7818 - val_quantile_loss: 2.3499 - val_time_to_eruption_mae: 4.7818 - val_quantile_mae: 4.9129\n",
      "Epoch 64/100\n",
      "444/444 [==============================] - 140s 316ms/step - loss: 20.3578 - time_to_eruption_loss: 4.3049 - quantile_loss: 2.1108 - time_to_eruption_mae: 4.3049 - quantile_mae: 4.3918 - val_loss: 17.4426 - val_time_to_eruption_loss: 3.6554 - val_quantile_loss: 1.7934 - val_time_to_eruption_mae: 3.6554 - val_quantile_mae: 3.7327\n",
      "Epoch 65/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.9835 - time_to_eruption_loss: 4.2220 - quantile_loss: 2.0698 - time_to_eruption_mae: 4.2220 - quantile_mae: 4.3069 - val_loss: 19.5481 - val_time_to_eruption_loss: 4.1217 - val_quantile_loss: 2.0366 - val_time_to_eruption_mae: 4.1217 - val_quantile_mae: 4.2148\n",
      "Epoch 66/100\n",
      "444/444 [==============================] - 140s 314ms/step - loss: 19.9893 - time_to_eruption_loss: 4.2247 - quantile_loss: 2.0665 - time_to_eruption_mae: 4.2247 - quantile_mae: 4.3058 - val_loss: 17.5842 - val_time_to_eruption_loss: 3.6855 - val_quantile_loss: 1.8186 - val_time_to_eruption_mae: 3.6855 - val_quantile_mae: 3.7913\n",
      "Epoch 67/100\n",
      "444/444 [==============================] - 141s 317ms/step - loss: 19.6247 - time_to_eruption_loss: 4.1420 - quantile_loss: 2.0306 - time_to_eruption_mae: 4.1420 - quantile_mae: 4.2210 - val_loss: 19.7971 - val_time_to_eruption_loss: 4.1949 - val_quantile_loss: 1.9900 - val_time_to_eruption_mae: 4.1949 - val_quantile_mae: 4.1361\n",
      "Epoch 68/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.6516 - time_to_eruption_loss: 4.1475 - quantile_loss: 2.0332 - time_to_eruption_mae: 4.1475 - quantile_mae: 4.2335 - val_loss: 13.3946 - val_time_to_eruption_loss: 2.7526 - val_quantile_loss: 1.3570 - val_time_to_eruption_mae: 2.7526 - val_quantile_mae: 2.8652\n",
      "Epoch 69/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 19.1036 - time_to_eruption_loss: 4.0257 - quantile_loss: 1.9738 - time_to_eruption_mae: 4.0257 - quantile_mae: 4.1001 - val_loss: 19.8930 - val_time_to_eruption_loss: 4.2047 - val_quantile_loss: 2.0475 - val_time_to_eruption_mae: 4.2047 - val_quantile_mae: 4.2194\n",
      "Epoch 70/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 19.2232 - time_to_eruption_loss: 4.0518 - quantile_loss: 1.9882 - time_to_eruption_mae: 4.0518 - quantile_mae: 4.1296 - val_loss: 22.5237 - val_time_to_eruption_loss: 4.7853 - val_quantile_loss: 2.3544 - val_time_to_eruption_mae: 4.7853 - val_quantile_mae: 4.8379\n",
      "Epoch 71/100\n",
      "444/444 [==============================] - 137s 310ms/step - loss: 18.7302 - time_to_eruption_loss: 3.9424 - quantile_loss: 1.9345 - time_to_eruption_mae: 3.9424 - quantile_mae: 4.0164 - val_loss: 20.6043 - val_time_to_eruption_loss: 4.3617 - val_quantile_loss: 2.1330 - val_time_to_eruption_mae: 4.3617 - val_quantile_mae: 4.4079\n",
      "Epoch 72/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 19.3412 - time_to_eruption_loss: 4.0800 - quantile_loss: 1.9984 - time_to_eruption_mae: 4.0800 - quantile_mae: 4.1595 - val_loss: 17.4577 - val_time_to_eruption_loss: 3.6573 - val_quantile_loss: 1.8052 - val_time_to_eruption_mae: 3.6573 - val_quantile_mae: 3.8018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 19.0751 - time_to_eruption_loss: 4.0208 - quantile_loss: 1.9713 - time_to_eruption_mae: 4.0208 - quantile_mae: 4.1034 - val_loss: 16.6647 - val_time_to_eruption_loss: 3.4850 - val_quantile_loss: 1.7047 - val_time_to_eruption_mae: 3.4850 - val_quantile_mae: 3.5550\n",
      "Epoch 74/100\n",
      "444/444 [==============================] - 137s 310ms/step - loss: 18.8772 - time_to_eruption_loss: 3.9777 - quantile_loss: 1.9459 - time_to_eruption_mae: 3.9777 - quantile_mae: 4.0529 - val_loss: 13.1427 - val_time_to_eruption_loss: 2.6992 - val_quantile_loss: 1.3261 - val_time_to_eruption_mae: 2.6992 - val_quantile_mae: 2.7827\n",
      "Epoch 75/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 18.0792 - time_to_eruption_loss: 3.7994 - quantile_loss: 1.8623 - time_to_eruption_mae: 3.7994 - quantile_mae: 3.8728 - val_loss: 15.5078 - val_time_to_eruption_loss: 3.2244 - val_quantile_loss: 1.5911 - val_time_to_eruption_mae: 3.2244 - val_quantile_mae: 3.3175\n",
      "Epoch 76/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 18.3063 - time_to_eruption_loss: 3.8504 - quantile_loss: 1.8855 - time_to_eruption_mae: 3.8504 - quantile_mae: 3.9239 - val_loss: 15.2825 - val_time_to_eruption_loss: 3.1764 - val_quantile_loss: 1.5582 - val_time_to_eruption_mae: 3.1764 - val_quantile_mae: 3.2595\n",
      "Epoch 77/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 18.0786 - time_to_eruption_loss: 3.7990 - quantile_loss: 1.8646 - time_to_eruption_mae: 3.7990 - quantile_mae: 3.8807 - val_loss: 14.8721 - val_time_to_eruption_loss: 3.0816 - val_quantile_loss: 1.5276 - val_time_to_eruption_mae: 3.0816 - val_quantile_mae: 3.1895\n",
      "Epoch 78/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 18.2856 - time_to_eruption_loss: 3.8448 - quantile_loss: 1.8869 - time_to_eruption_mae: 3.8448 - quantile_mae: 3.9262 - val_loss: 14.5075 - val_time_to_eruption_loss: 3.0000 - val_quantile_loss: 1.4861 - val_time_to_eruption_mae: 3.0000 - val_quantile_mae: 3.1152\n",
      "Epoch 79/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 18.3195 - time_to_eruption_loss: 3.8527 - quantile_loss: 1.8882 - time_to_eruption_mae: 3.8527 - quantile_mae: 3.9301 - val_loss: 12.7339 - val_time_to_eruption_loss: 2.6084 - val_quantile_loss: 1.2806 - val_time_to_eruption_mae: 2.6084 - val_quantile_mae: 2.6916\n",
      "Epoch 80/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 17.9397 - time_to_eruption_loss: 3.7678 - quantile_loss: 1.8486 - time_to_eruption_mae: 3.7678 - quantile_mae: 3.8430 - val_loss: 16.7314 - val_time_to_eruption_loss: 3.5043 - val_quantile_loss: 1.6944 - val_time_to_eruption_mae: 3.5043 - val_quantile_mae: 3.5292\n",
      "Epoch 81/100\n",
      "444/444 [==============================] - 140s 315ms/step - loss: 17.4159 - time_to_eruption_loss: 3.6516 - quantile_loss: 1.7904 - time_to_eruption_mae: 3.6516 - quantile_mae: 3.7239 - val_loss: 14.9473 - val_time_to_eruption_loss: 3.1059 - val_quantile_loss: 1.5043 - val_time_to_eruption_mae: 3.1059 - val_quantile_mae: 3.1549\n",
      "Epoch 82/100\n",
      "444/444 [==============================] - 143s 322ms/step - loss: 17.5188 - time_to_eruption_loss: 3.6750 - quantile_loss: 1.8015 - time_to_eruption_mae: 3.6750 - quantile_mae: 3.7507 - val_loss: 15.1886 - val_time_to_eruption_loss: 3.1573 - val_quantile_loss: 1.5427 - val_time_to_eruption_mae: 3.1573 - val_quantile_mae: 3.2077\n",
      "Epoch 83/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 17.2409 - time_to_eruption_loss: 3.6136 - quantile_loss: 1.7706 - time_to_eruption_mae: 3.6136 - quantile_mae: 3.6840 - val_loss: 17.1914 - val_time_to_eruption_loss: 3.5930 - val_quantile_loss: 1.8039 - val_time_to_eruption_mae: 3.5930 - val_quantile_mae: 3.7544\n",
      "Epoch 84/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 17.2909 - time_to_eruption_loss: 3.6242 - quantile_loss: 1.7775 - time_to_eruption_mae: 3.6242 - quantile_mae: 3.7000 - val_loss: 14.7358 - val_time_to_eruption_loss: 3.0467 - val_quantile_loss: 1.5330 - val_time_to_eruption_mae: 3.0467 - val_quantile_mae: 3.2278\n",
      "Epoch 85/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 17.7282 - time_to_eruption_loss: 3.7226 - quantile_loss: 1.8226 - time_to_eruption_mae: 3.7226 - quantile_mae: 3.8045 - val_loss: 11.6305 - val_time_to_eruption_loss: 2.3626 - val_quantile_loss: 1.1656 - val_time_to_eruption_mae: 2.3626 - val_quantile_mae: 2.4650\n",
      "Epoch 86/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 17.5995 - time_to_eruption_loss: 3.6941 - quantile_loss: 1.8089 - time_to_eruption_mae: 3.6941 - quantile_mae: 3.7641 - val_loss: 16.4993 - val_time_to_eruption_loss: 3.4492 - val_quantile_loss: 1.6882 - val_time_to_eruption_mae: 3.4492 - val_quantile_mae: 3.5003\n",
      "Epoch 87/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 17.3472 - time_to_eruption_loss: 3.6367 - quantile_loss: 1.7855 - time_to_eruption_mae: 3.6367 - quantile_mae: 3.7184 - val_loss: 12.5923 - val_time_to_eruption_loss: 2.5748 - val_quantile_loss: 1.2784 - val_time_to_eruption_mae: 2.5748 - val_quantile_mae: 2.6976\n",
      "Epoch 88/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 17.1219 - time_to_eruption_loss: 3.5876 - quantile_loss: 1.7566 - time_to_eruption_mae: 3.5876 - quantile_mae: 3.6585 - val_loss: 16.9339 - val_time_to_eruption_loss: 3.5440 - val_quantile_loss: 1.7433 - val_time_to_eruption_mae: 3.5440 - val_quantile_mae: 3.6063\n",
      "Epoch 89/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 16.6502 - time_to_eruption_loss: 3.4823 - quantile_loss: 1.7072 - time_to_eruption_mae: 3.4823 - quantile_mae: 3.5516 - val_loss: 12.3971 - val_time_to_eruption_loss: 2.5335 - val_quantile_loss: 1.2488 - val_time_to_eruption_mae: 2.5335 - val_quantile_mae: 2.6429\n",
      "Epoch 90/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 16.7924 - time_to_eruption_loss: 3.5136 - quantile_loss: 1.7242 - time_to_eruption_mae: 3.5136 - quantile_mae: 3.5875 - val_loss: 14.2125 - val_time_to_eruption_loss: 2.9349 - val_quantile_loss: 1.4599 - val_time_to_eruption_mae: 2.9349 - val_quantile_mae: 3.0750\n",
      "Epoch 91/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 16.8531 - time_to_eruption_loss: 3.5276 - quantile_loss: 1.7309 - time_to_eruption_mae: 3.5276 - quantile_mae: 3.5969 - val_loss: 12.3598 - val_time_to_eruption_loss: 2.5258 - val_quantile_loss: 1.2455 - val_time_to_eruption_mae: 2.5258 - val_quantile_mae: 2.6124\n",
      "Epoch 92/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 16.6384 - time_to_eruption_loss: 3.4805 - quantile_loss: 1.7056 - time_to_eruption_mae: 3.4805 - quantile_mae: 3.5520 - val_loss: 13.3755 - val_time_to_eruption_loss: 2.7530 - val_quantile_loss: 1.3528 - val_time_to_eruption_mae: 2.7530 - val_quantile_mae: 2.8366\n",
      "Epoch 93/100\n",
      "444/444 [==============================] - 140s 315ms/step - loss: 17.1230 - time_to_eruption_loss: 3.5883 - quantile_loss: 1.7581 - time_to_eruption_mae: 3.5883 - quantile_mae: 3.6635 - val_loss: 13.4435 - val_time_to_eruption_loss: 2.7749 - val_quantile_loss: 1.3323 - val_time_to_eruption_mae: 2.7749 - val_quantile_mae: 2.7956\n",
      "Epoch 94/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 16.9617 - time_to_eruption_loss: 3.5525 - quantile_loss: 1.7402 - time_to_eruption_mae: 3.5525 - quantile_mae: 3.6266 - val_loss: 11.7966 - val_time_to_eruption_loss: 2.4019 - val_quantile_loss: 1.1776 - val_time_to_eruption_mae: 2.4019 - val_quantile_mae: 2.4865\n",
      "Epoch 95/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 17.0134 - time_to_eruption_loss: 3.5641 - quantile_loss: 1.7456 - time_to_eruption_mae: 3.5641 - quantile_mae: 3.6337 - val_loss: 12.7202 - val_time_to_eruption_loss: 2.6064 - val_quantile_loss: 1.2826 - val_time_to_eruption_mae: 2.6064 - val_quantile_mae: 2.7120\n",
      "Epoch 96/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.2778 - time_to_eruption_loss: 3.3994 - quantile_loss: 1.6676 - time_to_eruption_mae: 3.3994 - quantile_mae: 3.4717 - val_loss: 16.3911 - val_time_to_eruption_loss: 3.4214 - val_quantile_loss: 1.6940 - val_time_to_eruption_mae: 3.4214 - val_quantile_mae: 3.5059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "444/444 [==============================] - 137s 310ms/step - loss: 16.6285 - time_to_eruption_loss: 3.4774 - quantile_loss: 1.7058 - time_to_eruption_mae: 3.4774 - quantile_mae: 3.5458 - val_loss: 10.9294 - val_time_to_eruption_loss: 2.2044 - val_quantile_loss: 1.0988 - val_time_to_eruption_mae: 2.2044 - val_quantile_mae: 2.3469\n",
      "Epoch 98/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 16.1886 - time_to_eruption_loss: 3.3793 - quantile_loss: 1.6591 - time_to_eruption_mae: 3.3793 - quantile_mae: 3.4522 - val_loss: 29.7686 - val_time_to_eruption_loss: 6.3939 - val_quantile_loss: 3.1821 - val_time_to_eruption_mae: 6.3939 - val_quantile_mae: 6.4768\n",
      "Epoch 99/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 16.2567 - time_to_eruption_loss: 3.3958 - quantile_loss: 1.6638 - time_to_eruption_mae: 3.3958 - quantile_mae: 3.4594 - val_loss: 13.1146 - val_time_to_eruption_loss: 2.6954 - val_quantile_loss: 1.3231 - val_time_to_eruption_mae: 2.6954 - val_quantile_mae: 2.7888\n",
      "Epoch 100/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 16.0383 - time_to_eruption_loss: 3.3463 - quantile_loss: 1.6425 - time_to_eruption_mae: 3.3463 - quantile_mae: 3.4132 - val_loss: 11.9225 - val_time_to_eruption_loss: 2.4293 - val_quantile_loss: 1.1953 - val_time_to_eruption_mae: 2.4293 - val_quantile_mae: 2.5102\n",
      "Restoring model weights from the end of the best epoch.\n",
      " 1/28 [>.............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0210s vs `on_predict_batch_end` time: 0.1985s). Check your callbacks.\n",
      "28/28 [==============================] - 6s 216ms/step\n",
      "WARNING:tensorflow:From C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./models/model_cnn2d_Big_1\\assets\n",
      "************************************************************\n",
      "Prediction MAE: 2204381.605410393\n",
      "************************************************************\n",
      "Num Fold: 3\n",
      "Train segments: 3545 Val segments: 886\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_10 (ConvModel (None, 16, 14, 128)  576352      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_11 (ConvModel (None, 16, 14, 128)  576352      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_12 (ConvModel (None, 16, 14, 128)  576352      input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_13 (ConvModel (None, 16, 14, 128)  576352      input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_14 (ConvModel (None, 16, 14, 128)  576352      input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_15 (ConvModel (None, 16, 14, 128)  576352      input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_16 (ConvModel (None, 16, 14, 128)  576352      input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_17 (ConvModel (None, 16, 14, 128)  576352      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_18 (ConvModel (None, 16, 14, 128)  576352      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_19 (ConvModel (None, 16, 14, 128)  576352      input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_10 (Gl (None, 128)          0           conv_model_sensor_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_11 (Gl (None, 128)          0           conv_model_sensor_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_12 (Gl (None, 128)          0           conv_model_sensor_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_13 (Gl (None, 128)          0           conv_model_sensor_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_14 (Gl (None, 128)          0           conv_model_sensor_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_15 (Gl (None, 128)          0           conv_model_sensor_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_16 (Gl (None, 128)          0           conv_model_sensor_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_17 (Gl (None, 128)          0           conv_model_sensor_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_18 (Gl (None, 128)          0           conv_model_sensor_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_19 (Gl (None, 128)          0           conv_model_sensor_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1280)         0           global_average_pooling2d_10[0][0]\n",
      "                                                                 global_average_pooling2d_11[0][0]\n",
      "                                                                 global_average_pooling2d_12[0][0]\n",
      "                                                                 global_average_pooling2d_13[0][0]\n",
      "                                                                 global_average_pooling2d_14[0][0]\n",
      "                                                                 global_average_pooling2d_15[0][0]\n",
      "                                                                 global_average_pooling2d_16[0][0]\n",
      "                                                                 global_average_pooling2d_17[0][0]\n",
      "                                                                 global_average_pooling2d_18[0][0]\n",
      "                                                                 global_average_pooling2d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 1280)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 300)          384300      dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 300)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 300)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           19264       dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 64)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_to_eruption (Dense)        (None, 1)            65          dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "quantile (Dense)                (None, 3)            195         dropout_85[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,167,344\n",
      "Trainable params: 6,151,664\n",
      "Non-trainable params: 15,680\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 139s 312ms/step - loss: 56.0661 - time_to_eruption_loss: 12.2090 - quantile_loss: 6.1924 - time_to_eruption_mae: 12.2090 - quantile_mae: 12.8833 - val_loss: 56.2476 - val_time_to_eruption_loss: 12.3215 - val_quantile_loss: 5.8985 - val_time_to_eruption_mae: 12.3215 - val_quantile_mae: 12.1048\n",
      "Epoch 2/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 53.7055 - time_to_eruption_loss: 11.6899 - quantile_loss: 5.8618 - time_to_eruption_mae: 11.6899 - quantile_mae: 12.2206 - val_loss: 49.6827 - val_time_to_eruption_loss: 10.8186 - val_quantile_loss: 5.3109 - val_time_to_eruption_mae: 10.8186 - val_quantile_mae: 11.1814\n",
      "Epoch 3/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 52.0221 - time_to_eruption_loss: 11.3062 - quantile_loss: 5.6849 - time_to_eruption_mae: 11.3062 - quantile_mae: 11.8962 - val_loss: 51.8362 - val_time_to_eruption_loss: 11.2867 - val_quantile_loss: 5.5642 - val_time_to_eruption_mae: 11.2867 - val_quantile_mae: 11.4715\n",
      "Epoch 4/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 50.9476 - time_to_eruption_loss: 11.0777 - quantile_loss: 5.5068 - time_to_eruption_mae: 11.0777 - quantile_mae: 11.4986 - val_loss: 47.8202 - val_time_to_eruption_loss: 10.3915 - val_quantile_loss: 5.1209 - val_time_to_eruption_mae: 10.3915 - val_quantile_mae: 10.7259\n",
      "Epoch 5/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 50.3212 - time_to_eruption_loss: 10.9333 - quantile_loss: 5.4520 - time_to_eruption_mae: 10.9333 - quantile_mae: 11.3481 - val_loss: 49.8477 - val_time_to_eruption_loss: 10.8760 - val_quantile_loss: 5.2024 - val_time_to_eruption_mae: 10.8760 - val_quantile_mae: 10.7810\n",
      "Epoch 6/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 48.2713 - time_to_eruption_loss: 10.4793 - quantile_loss: 5.2124 - time_to_eruption_mae: 10.4793 - quantile_mae: 10.8484 - val_loss: 45.1613 - val_time_to_eruption_loss: 9.8076 - val_quantile_loss: 4.7832 - val_time_to_eruption_mae: 9.8076 - val_quantile_mae: 10.1119\n",
      "Epoch 7/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 47.6184 - time_to_eruption_loss: 10.3368 - quantile_loss: 5.1180 - time_to_eruption_mae: 10.3368 - quantile_mae: 10.6509 - val_loss: 56.2878 - val_time_to_eruption_loss: 12.2207 - val_quantile_loss: 6.2492 - val_time_to_eruption_mae: 12.2207 - val_quantile_mae: 13.0820\n",
      "Epoch 8/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 47.7494 - time_to_eruption_loss: 10.3727 - quantile_loss: 5.1090 - time_to_eruption_mae: 10.3727 - quantile_mae: 10.6696 - val_loss: 44.6585 - val_time_to_eruption_loss: 9.6808 - val_quantile_loss: 4.7908 - val_time_to_eruption_mae: 9.6808 - val_quantile_mae: 10.0731\n",
      "Epoch 9/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 45.7845 - time_to_eruption_loss: 9.9287 - quantile_loss: 4.9246 - time_to_eruption_mae: 9.9287 - quantile_mae: 10.2546 - val_loss: 43.2906 - val_time_to_eruption_loss: 9.3830 - val_quantile_loss: 4.6128 - val_time_to_eruption_mae: 9.3830 - val_quantile_mae: 9.6053\n",
      "Epoch 10/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 45.1428 - time_to_eruption_loss: 9.7927 - quantile_loss: 4.8215 - time_to_eruption_mae: 9.7927 - quantile_mae: 10.0153 - val_loss: 41.0701 - val_time_to_eruption_loss: 8.8832 - val_quantile_loss: 4.3804 - val_time_to_eruption_mae: 8.8832 - val_quantile_mae: 9.1381\n",
      "Epoch 11/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 42.5891 - time_to_eruption_loss: 9.2232 - quantile_loss: 4.5409 - time_to_eruption_mae: 9.2232 - quantile_mae: 9.4313 - val_loss: 37.4630 - val_time_to_eruption_loss: 8.0871 - val_quantile_loss: 3.9624 - val_time_to_eruption_mae: 8.0871 - val_quantile_mae: 8.2340\n",
      "Epoch 12/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 41.0073 - time_to_eruption_loss: 8.8708 - quantile_loss: 4.3757 - time_to_eruption_mae: 8.8708 - quantile_mae: 9.0846 - val_loss: 37.9288 - val_time_to_eruption_loss: 8.1858 - val_quantile_loss: 4.0410 - val_time_to_eruption_mae: 8.1858 - val_quantile_mae: 8.4561\n",
      "Epoch 13/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 39.5899 - time_to_eruption_loss: 8.5555 - quantile_loss: 4.2191 - time_to_eruption_mae: 8.5555 - quantile_mae: 8.7613 - val_loss: 31.4958 - val_time_to_eruption_loss: 6.7571 - val_quantile_loss: 3.3209 - val_time_to_eruption_mae: 6.7571 - val_quantile_mae: 6.9683\n",
      "Epoch 14/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 38.4701 - time_to_eruption_loss: 8.3098 - quantile_loss: 4.0916 - time_to_eruption_mae: 8.3098 - quantile_mae: 8.4866 - val_loss: 36.5344 - val_time_to_eruption_loss: 7.8803 - val_quantile_loss: 3.8798 - val_time_to_eruption_mae: 7.8803 - val_quantile_mae: 8.0884\n",
      "Epoch 15/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 37.7635 - time_to_eruption_loss: 8.1558 - quantile_loss: 4.0109 - time_to_eruption_mae: 8.1558 - quantile_mae: 8.3261 - val_loss: 30.4804 - val_time_to_eruption_loss: 6.5360 - val_quantile_loss: 3.2117 - val_time_to_eruption_mae: 6.5360 - val_quantile_mae: 6.7501\n",
      "Epoch 16/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 36.3356 - time_to_eruption_loss: 7.8407 - quantile_loss: 3.8573 - time_to_eruption_mae: 7.8407 - quantile_mae: 8.0134 - val_loss: 38.8665 - val_time_to_eruption_loss: 8.4145 - val_quantile_loss: 4.0996 - val_time_to_eruption_mae: 8.4145 - val_quantile_mae: 8.4363\n",
      "Epoch 17/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 35.2131 - time_to_eruption_loss: 7.5946 - quantile_loss: 3.7284 - time_to_eruption_mae: 7.5946 - quantile_mae: 7.7408 - val_loss: 43.3959 - val_time_to_eruption_loss: 9.4225 - val_quantile_loss: 4.5999 - val_time_to_eruption_mae: 9.4225 - val_quantile_mae: 9.4393\n",
      "Epoch 18/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 34.2044 - time_to_eruption_loss: 7.3712 - quantile_loss: 3.6152 - time_to_eruption_mae: 7.3712 - quantile_mae: 7.4995 - val_loss: 31.8843 - val_time_to_eruption_loss: 6.8575 - val_quantile_loss: 3.3510 - val_time_to_eruption_mae: 6.8575 - val_quantile_mae: 6.9385\n",
      "Epoch 19/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 33.8207 - time_to_eruption_loss: 7.2841 - quantile_loss: 3.5782 - time_to_eruption_mae: 7.2841 - quantile_mae: 7.4269 - val_loss: 30.6427 - val_time_to_eruption_loss: 6.5651 - val_quantile_loss: 3.2715 - val_time_to_eruption_mae: 6.5651 - val_quantile_mae: 6.7672\n",
      "Epoch 20/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 33.1021 - time_to_eruption_loss: 7.1249 - quantile_loss: 3.4963 - time_to_eruption_mae: 7.1249 - quantile_mae: 7.2434 - val_loss: 30.4425 - val_time_to_eruption_loss: 6.5339 - val_quantile_loss: 3.2028 - val_time_to_eruption_mae: 6.5339 - val_quantile_mae: 6.6809\n",
      "Epoch 21/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 32.4234 - time_to_eruption_loss: 6.9750 - quantile_loss: 3.4235 - time_to_eruption_mae: 6.9750 - quantile_mae: 7.1033 - val_loss: 52.1518 - val_time_to_eruption_loss: 11.3614 - val_quantile_loss: 5.6111 - val_time_to_eruption_mae: 11.3614 - val_quantile_mae: 11.4076\n",
      "Epoch 22/100\n",
      "444/444 [==============================] - 141s 317ms/step - loss: 31.6100 - time_to_eruption_loss: 6.7970 - quantile_loss: 3.3312 - time_to_eruption_mae: 6.7970 - quantile_mae: 6.9247 - val_loss: 27.7382 - val_time_to_eruption_loss: 5.9357 - val_quantile_loss: 2.9069 - val_time_to_eruption_mae: 5.9357 - val_quantile_mae: 6.0728\n",
      "Epoch 23/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 31.1303 - time_to_eruption_loss: 6.6892 - quantile_loss: 3.2849 - time_to_eruption_mae: 6.6892 - quantile_mae: 6.8111 - val_loss: 27.4167 - val_time_to_eruption_loss: 5.8556 - val_quantile_loss: 2.9056 - val_time_to_eruption_mae: 5.8556 - val_quantile_mae: 6.0242\n",
      "Epoch 24/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 30.3363 - time_to_eruption_loss: 6.5149 - quantile_loss: 3.1912 - time_to_eruption_mae: 6.5149 - quantile_mae: 6.6406 - val_loss: 32.0557 - val_time_to_eruption_loss: 6.8977 - val_quantile_loss: 3.3787 - val_time_to_eruption_mae: 6.8977 - val_quantile_mae: 6.9885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 29.3225 - time_to_eruption_loss: 6.2895 - quantile_loss: 3.0792 - time_to_eruption_mae: 6.2895 - quantile_mae: 6.4082 - val_loss: 35.2127 - val_time_to_eruption_loss: 7.5956 - val_quantile_loss: 3.7524 - val_time_to_eruption_mae: 7.5956 - val_quantile_mae: 7.8053\n",
      "Epoch 26/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 28.8397 - time_to_eruption_loss: 6.1825 - quantile_loss: 3.0305 - time_to_eruption_mae: 6.1825 - quantile_mae: 6.2947 - val_loss: 26.1869 - val_time_to_eruption_loss: 5.5951 - val_quantile_loss: 2.7266 - val_time_to_eruption_mae: 5.5951 - val_quantile_mae: 5.6668\n",
      "Epoch 27/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 29.1724 - time_to_eruption_loss: 6.2560 - quantile_loss: 3.0629 - time_to_eruption_mae: 6.2560 - quantile_mae: 6.3839 - val_loss: 24.6935 - val_time_to_eruption_loss: 5.2532 - val_quantile_loss: 2.5932 - val_time_to_eruption_mae: 5.2532 - val_quantile_mae: 5.4648\n",
      "Epoch 28/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 27.6020 - time_to_eruption_loss: 5.9064 - quantile_loss: 2.8947 - time_to_eruption_mae: 5.9064 - quantile_mae: 6.0272 - val_loss: 26.1670 - val_time_to_eruption_loss: 5.5811 - val_quantile_loss: 2.7638 - val_time_to_eruption_mae: 5.5811 - val_quantile_mae: 5.8320\n",
      "Epoch 29/100\n",
      "444/444 [==============================] - 137s 309ms/step - loss: 28.0123 - time_to_eruption_loss: 5.9983 - quantile_loss: 2.9441 - time_to_eruption_mae: 5.9983 - quantile_mae: 6.1185 - val_loss: 23.4421 - val_time_to_eruption_loss: 4.9819 - val_quantile_loss: 2.4409 - val_time_to_eruption_mae: 4.9819 - val_quantile_mae: 5.0927\n",
      "Epoch 30/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 26.9099 - time_to_eruption_loss: 5.7542 - quantile_loss: 2.8244 - time_to_eruption_mae: 5.7542 - quantile_mae: 5.8487 - val_loss: 32.4860 - val_time_to_eruption_loss: 6.9984 - val_quantile_loss: 3.4265 - val_time_to_eruption_mae: 6.9984 - val_quantile_mae: 7.0484\n",
      "Epoch 31/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 26.6545 - time_to_eruption_loss: 5.6988 - quantile_loss: 2.7966 - time_to_eruption_mae: 5.6988 - quantile_mae: 5.8156 - val_loss: 22.9876 - val_time_to_eruption_loss: 4.8797 - val_quantile_loss: 2.4100 - val_time_to_eruption_mae: 4.8797 - val_quantile_mae: 5.0266\n",
      "Epoch 32/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 26.5997 - time_to_eruption_loss: 5.6890 - quantile_loss: 2.7845 - time_to_eruption_mae: 5.6890 - quantile_mae: 5.7985 - val_loss: 22.0551 - val_time_to_eruption_loss: 4.6621 - val_quantile_loss: 2.3532 - val_time_to_eruption_mae: 4.6621 - val_quantile_mae: 4.9747\n",
      "Epoch 33/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 26.3743 - time_to_eruption_loss: 5.6401 - quantile_loss: 2.7647 - time_to_eruption_mae: 5.6401 - quantile_mae: 5.7413 - val_loss: 33.8031 - val_time_to_eruption_loss: 7.3089 - val_quantile_loss: 3.5211 - val_time_to_eruption_mae: 7.3089 - val_quantile_mae: 7.2449\n",
      "Epoch 34/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 25.4578 - time_to_eruption_loss: 5.4368 - quantile_loss: 2.6659 - time_to_eruption_mae: 5.4368 - quantile_mae: 5.5493 - val_loss: 25.8570 - val_time_to_eruption_loss: 5.5214 - val_quantile_loss: 2.7280 - val_time_to_eruption_mae: 5.5214 - val_quantile_mae: 5.6964\n",
      "Epoch 35/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 25.3235 - time_to_eruption_loss: 5.4081 - quantile_loss: 2.6498 - time_to_eruption_mae: 5.4081 - quantile_mae: 5.5127 - val_loss: 21.4799 - val_time_to_eruption_loss: 4.5494 - val_quantile_loss: 2.2436 - val_time_to_eruption_mae: 4.5494 - val_quantile_mae: 4.6708\n",
      "Epoch 36/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 25.0673 - time_to_eruption_loss: 5.3522 - quantile_loss: 2.6236 - time_to_eruption_mae: 5.3522 - quantile_mae: 5.4512 - val_loss: 22.9002 - val_time_to_eruption_loss: 4.8728 - val_quantile_loss: 2.3760 - val_time_to_eruption_mae: 4.8728 - val_quantile_mae: 4.9783\n",
      "Epoch 37/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 24.6925 - time_to_eruption_loss: 5.2691 - quantile_loss: 2.5819 - time_to_eruption_mae: 5.2691 - quantile_mae: 5.3748 - val_loss: 34.8677 - val_time_to_eruption_loss: 7.5231 - val_quantile_loss: 3.7266 - val_time_to_eruption_mae: 7.5231 - val_quantile_mae: 7.6769\n",
      "Epoch 38/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 25.1584 - time_to_eruption_loss: 5.3696 - quantile_loss: 2.6331 - time_to_eruption_mae: 5.3696 - quantile_mae: 5.4669 - val_loss: 33.0660 - val_time_to_eruption_loss: 7.1247 - val_quantile_loss: 3.5209 - val_time_to_eruption_mae: 7.1247 - val_quantile_mae: 7.2108\n",
      "Epoch 39/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 24.3988 - time_to_eruption_loss: 5.2009 - quantile_loss: 2.5449 - time_to_eruption_mae: 5.2009 - quantile_mae: 5.2876 - val_loss: 21.0606 - val_time_to_eruption_loss: 4.4543 - val_quantile_loss: 2.1922 - val_time_to_eruption_mae: 4.4543 - val_quantile_mae: 4.5608\n",
      "Epoch 40/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 23.9498 - time_to_eruption_loss: 5.0994 - quantile_loss: 2.4996 - time_to_eruption_mae: 5.0994 - quantile_mae: 5.1900 - val_loss: 19.4396 - val_time_to_eruption_loss: 4.0913 - val_quantile_loss: 2.0245 - val_time_to_eruption_mae: 4.0913 - val_quantile_mae: 4.2456\n",
      "Epoch 41/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 23.5693 - time_to_eruption_loss: 5.0158 - quantile_loss: 2.4590 - time_to_eruption_mae: 5.0158 - quantile_mae: 5.1136 - val_loss: 18.6321 - val_time_to_eruption_loss: 3.9153 - val_quantile_loss: 1.9278 - val_time_to_eruption_mae: 3.9153 - val_quantile_mae: 4.0286\n",
      "Epoch 42/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 23.3852 - time_to_eruption_loss: 4.9754 - quantile_loss: 2.4437 - time_to_eruption_mae: 4.9754 - quantile_mae: 5.0700 - val_loss: 21.5094 - val_time_to_eruption_loss: 4.5578 - val_quantile_loss: 2.2394 - val_time_to_eruption_mae: 4.5578 - val_quantile_mae: 4.6335\n",
      "Epoch 43/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 23.1984 - time_to_eruption_loss: 4.9349 - quantile_loss: 2.4210 - time_to_eruption_mae: 4.9349 - quantile_mae: 5.0292 - val_loss: 20.7458 - val_time_to_eruption_loss: 4.3903 - val_quantile_loss: 2.1450 - val_time_to_eruption_mae: 4.3903 - val_quantile_mae: 4.4318\n",
      "Epoch 44/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 22.8021 - time_to_eruption_loss: 4.8464 - quantile_loss: 2.3770 - time_to_eruption_mae: 4.8464 - quantile_mae: 4.9433 - val_loss: 25.2587 - val_time_to_eruption_loss: 5.3963 - val_quantile_loss: 2.6327 - val_time_to_eruption_mae: 5.3963 - val_quantile_mae: 5.4596\n",
      "Epoch 45/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 22.8816 - time_to_eruption_loss: 4.8631 - quantile_loss: 2.3860 - time_to_eruption_mae: 4.8631 - quantile_mae: 4.9540 - val_loss: 27.3523 - val_time_to_eruption_loss: 5.8525 - val_quantile_loss: 2.8968 - val_time_to_eruption_mae: 5.8525 - val_quantile_mae: 5.9536\n",
      "Epoch 46/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 21.8999 - time_to_eruption_loss: 4.6454 - quantile_loss: 2.2756 - time_to_eruption_mae: 4.6454 - quantile_mae: 4.7337 - val_loss: 19.1264 - val_time_to_eruption_loss: 4.0296 - val_quantile_loss: 1.9697 - val_time_to_eruption_mae: 4.0296 - val_quantile_mae: 4.1556\n",
      "Epoch 47/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 22.3116 - time_to_eruption_loss: 4.7368 - quantile_loss: 2.3210 - time_to_eruption_mae: 4.7368 - quantile_mae: 4.8304 - val_loss: 19.7998 - val_time_to_eruption_loss: 4.1823 - val_quantile_loss: 2.0266 - val_time_to_eruption_mae: 4.1823 - val_quantile_mae: 4.2420\n",
      "Epoch 48/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 21.8442 - time_to_eruption_loss: 4.6325 - quantile_loss: 2.2719 - time_to_eruption_mae: 4.6325 - quantile_mae: 4.7211 - val_loss: 19.8030 - val_time_to_eruption_loss: 4.1807 - val_quantile_loss: 2.0394 - val_time_to_eruption_mae: 4.1807 - val_quantile_mae: 4.2433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 21.8030 - time_to_eruption_loss: 4.6239 - quantile_loss: 2.2673 - time_to_eruption_mae: 4.6239 - quantile_mae: 4.7152 - val_loss: 22.9422 - val_time_to_eruption_loss: 4.8772 - val_quantile_loss: 2.3945 - val_time_to_eruption_mae: 4.8772 - val_quantile_mae: 4.9438\n",
      "Epoch 50/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 20.8870 - time_to_eruption_loss: 4.4199 - quantile_loss: 2.1707 - time_to_eruption_mae: 4.4199 - quantile_mae: 4.5122 - val_loss: 21.9811 - val_time_to_eruption_loss: 4.6703 - val_quantile_loss: 2.2635 - val_time_to_eruption_mae: 4.6703 - val_quantile_mae: 4.6659\n",
      "Epoch 51/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 20.9889 - time_to_eruption_loss: 4.4437 - quantile_loss: 2.1793 - time_to_eruption_mae: 4.4437 - quantile_mae: 4.5247 - val_loss: 26.2817 - val_time_to_eruption_loss: 5.6191 - val_quantile_loss: 2.7713 - val_time_to_eruption_mae: 5.6191 - val_quantile_mae: 5.7095\n",
      "Epoch 52/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 20.3158 - time_to_eruption_loss: 4.2951 - quantile_loss: 2.1044 - time_to_eruption_mae: 4.2951 - quantile_mae: 4.3804 - val_loss: 16.9331 - val_time_to_eruption_loss: 3.5391 - val_quantile_loss: 1.7480 - val_time_to_eruption_mae: 3.5391 - val_quantile_mae: 3.6306\n",
      "Epoch 53/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 20.7783 - time_to_eruption_loss: 4.3995 - quantile_loss: 2.1555 - time_to_eruption_mae: 4.3995 - quantile_mae: 4.4821 - val_loss: 14.7397 - val_time_to_eruption_loss: 3.0488 - val_quantile_loss: 1.5232 - val_time_to_eruption_mae: 3.0488 - val_quantile_mae: 3.2524\n",
      "Epoch 54/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 20.4260 - time_to_eruption_loss: 4.3192 - quantile_loss: 2.1224 - time_to_eruption_mae: 4.3192 - quantile_mae: 4.4036 - val_loss: 23.5793 - val_time_to_eruption_loss: 5.0279 - val_quantile_loss: 2.4366 - val_time_to_eruption_mae: 5.0279 - val_quantile_mae: 5.0253\n",
      "Epoch 55/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 20.3295 - time_to_eruption_loss: 4.2979 - quantile_loss: 2.1080 - time_to_eruption_mae: 4.2979 - quantile_mae: 4.3785 - val_loss: 19.8364 - val_time_to_eruption_loss: 4.1866 - val_quantile_loss: 2.0615 - val_time_to_eruption_mae: 4.1866 - val_quantile_mae: 4.2803\n",
      "Epoch 56/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 20.4853 - time_to_eruption_loss: 4.3337 - quantile_loss: 2.1214 - time_to_eruption_mae: 4.3337 - quantile_mae: 4.4187 - val_loss: 20.7566 - val_time_to_eruption_loss: 4.3989 - val_quantile_loss: 2.1279 - val_time_to_eruption_mae: 4.3989 - val_quantile_mae: 4.4663\n",
      "Epoch 57/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 20.7123 - time_to_eruption_loss: 4.3831 - quantile_loss: 2.1475 - time_to_eruption_mae: 4.3831 - quantile_mae: 4.4708 - val_loss: 16.5929 - val_time_to_eruption_loss: 3.4688 - val_quantile_loss: 1.6883 - val_time_to_eruption_mae: 3.4688 - val_quantile_mae: 3.5359\n",
      "Epoch 58/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 19.7052 - time_to_eruption_loss: 4.1592 - quantile_loss: 2.0395 - time_to_eruption_mae: 4.1592 - quantile_mae: 4.2428 - val_loss: 19.2394 - val_time_to_eruption_loss: 4.0587 - val_quantile_loss: 1.9772 - val_time_to_eruption_mae: 4.0587 - val_quantile_mae: 4.1779\n",
      "Epoch 59/100\n",
      "444/444 [==============================] - 137s 307ms/step - loss: 19.6905 - time_to_eruption_loss: 4.1564 - quantile_loss: 2.0386 - time_to_eruption_mae: 4.1564 - quantile_mae: 4.2443 - val_loss: 19.8115 - val_time_to_eruption_loss: 4.1769 - val_quantile_loss: 2.0789 - val_time_to_eruption_mae: 4.1769 - val_quantile_mae: 4.2866\n",
      "Epoch 60/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 20.0687 - time_to_eruption_loss: 4.2398 - quantile_loss: 2.0779 - time_to_eruption_mae: 4.2398 - quantile_mae: 4.3212 - val_loss: 26.3285 - val_time_to_eruption_loss: 5.6302 - val_quantile_loss: 2.7705 - val_time_to_eruption_mae: 5.6302 - val_quantile_mae: 5.6973\n",
      "Epoch 61/100\n",
      "444/444 [==============================] - 136s 307ms/step - loss: 19.2956 - time_to_eruption_loss: 4.0668 - quantile_loss: 1.9939 - time_to_eruption_mae: 4.0668 - quantile_mae: 4.1557 - val_loss: 35.3146 - val_time_to_eruption_loss: 7.6290 - val_quantile_loss: 3.7670 - val_time_to_eruption_mae: 7.6290 - val_quantile_mae: 7.6857\n",
      "Epoch 62/100\n",
      "444/444 [==============================] - 137s 308ms/step - loss: 19.3909 - time_to_eruption_loss: 4.0899 - quantile_loss: 2.0029 - time_to_eruption_mae: 4.0899 - quantile_mae: 4.1764 - val_loss: 14.2495 - val_time_to_eruption_loss: 2.9388 - val_quantile_loss: 1.4688 - val_time_to_eruption_mae: 2.9388 - val_quantile_mae: 3.1133\n",
      "Epoch 63/100\n",
      "444/444 [==============================] - 138s 310ms/step - loss: 19.5192 - time_to_eruption_loss: 4.1190 - quantile_loss: 2.0205 - time_to_eruption_mae: 4.1190 - quantile_mae: 4.2067 - val_loss: 20.7286 - val_time_to_eruption_loss: 4.3993 - val_quantile_loss: 2.1103 - val_time_to_eruption_mae: 4.3993 - val_quantile_mae: 4.3539\n",
      "Epoch 64/100\n",
      "444/444 [==============================] - 154s 348ms/step - loss: 18.7591 - time_to_eruption_loss: 3.9511 - quantile_loss: 1.9362 - time_to_eruption_mae: 3.9511 - quantile_mae: 4.0269 - val_loss: 18.2510 - val_time_to_eruption_loss: 3.8398 - val_quantile_loss: 1.8746 - val_time_to_eruption_mae: 3.8398 - val_quantile_mae: 3.8617\n",
      "Epoch 65/100\n",
      "444/444 [==============================] - 164s 369ms/step - loss: 18.6860 - time_to_eruption_loss: 3.9349 - quantile_loss: 1.9308 - time_to_eruption_mae: 3.9349 - quantile_mae: 4.0164 - val_loss: 13.4754 - val_time_to_eruption_loss: 2.7761 - val_quantile_loss: 1.3554 - val_time_to_eruption_mae: 2.7761 - val_quantile_mae: 2.8615\n",
      "Epoch 66/100\n",
      "444/444 [==============================] - 175s 393ms/step - loss: 18.8869 - time_to_eruption_loss: 3.9805 - quantile_loss: 1.9502 - time_to_eruption_mae: 3.9805 - quantile_mae: 4.0604 - val_loss: 18.1004 - val_time_to_eruption_loss: 3.8127 - val_quantile_loss: 1.8358 - val_time_to_eruption_mae: 3.8127 - val_quantile_mae: 3.8109\n",
      "Epoch 67/100\n",
      "444/444 [==============================] - 191s 429ms/step - loss: 18.8079 - time_to_eruption_loss: 3.9631 - quantile_loss: 1.9416 - time_to_eruption_mae: 3.9631 - quantile_mae: 4.0380 - val_loss: 16.5780 - val_time_to_eruption_loss: 3.4638 - val_quantile_loss: 1.7065 - val_time_to_eruption_mae: 3.4638 - val_quantile_mae: 3.5195\n",
      "Epoch 68/100\n",
      "444/444 [==============================] - 190s 429ms/step - loss: 18.7775 - time_to_eruption_loss: 3.9553 - quantile_loss: 1.9411 - time_to_eruption_mae: 3.9553 - quantile_mae: 4.0372 - val_loss: 14.9549 - val_time_to_eruption_loss: 3.0993 - val_quantile_loss: 1.5441 - val_time_to_eruption_mae: 3.0993 - val_quantile_mae: 3.2541\n",
      "Epoch 69/100\n",
      "444/444 [==============================] - 183s 412ms/step - loss: 18.5949 - time_to_eruption_loss: 3.9155 - quantile_loss: 1.9202 - time_to_eruption_mae: 3.9155 - quantile_mae: 3.9975 - val_loss: 16.5987 - val_time_to_eruption_loss: 3.4678 - val_quantile_loss: 1.7166 - val_time_to_eruption_mae: 3.4678 - val_quantile_mae: 3.6197\n",
      "Epoch 70/100\n",
      "444/444 [==============================] - 163s 368ms/step - loss: 18.8147 - time_to_eruption_loss: 3.9652 - quantile_loss: 1.9424 - time_to_eruption_mae: 3.9652 - quantile_mae: 4.0466 - val_loss: 18.9117 - val_time_to_eruption_loss: 3.9997 - val_quantile_loss: 1.8996 - val_time_to_eruption_mae: 3.9997 - val_quantile_mae: 3.9831\n",
      "Epoch 71/100\n",
      "444/444 [==============================] - 163s 368ms/step - loss: 18.0102 - time_to_eruption_loss: 3.7852 - quantile_loss: 1.8579 - time_to_eruption_mae: 3.7852 - quantile_mae: 3.8635 - val_loss: 15.4676 - val_time_to_eruption_loss: 3.2196 - val_quantile_loss: 1.5809 - val_time_to_eruption_mae: 3.2196 - val_quantile_mae: 3.3121\n",
      "Epoch 72/100\n",
      "444/444 [==============================] - 163s 368ms/step - loss: 17.9472 - time_to_eruption_loss: 3.7725 - quantile_loss: 1.8501 - time_to_eruption_mae: 3.7725 - quantile_mae: 3.8529 - val_loss: 16.1493 - val_time_to_eruption_loss: 3.3705 - val_quantile_loss: 1.6552 - val_time_to_eruption_mae: 3.3705 - val_quantile_mae: 3.4761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "444/444 [==============================] - 164s 368ms/step - loss: 18.0333 - time_to_eruption_loss: 3.7910 - quantile_loss: 1.8577 - time_to_eruption_mae: 3.7910 - quantile_mae: 3.8710 - val_loss: 13.3160 - val_time_to_eruption_loss: 2.7379 - val_quantile_loss: 1.3544 - val_time_to_eruption_mae: 2.7379 - val_quantile_mae: 2.8758\n",
      "Epoch 74/100\n",
      "444/444 [==============================] - 163s 368ms/step - loss: 17.7356 - time_to_eruption_loss: 3.7255 - quantile_loss: 1.8253 - time_to_eruption_mae: 3.7255 - quantile_mae: 3.7997 - val_loss: 17.8855 - val_time_to_eruption_loss: 3.7590 - val_quantile_loss: 1.8422 - val_time_to_eruption_mae: 3.7590 - val_quantile_mae: 3.8821\n",
      "Epoch 75/100\n",
      "444/444 [==============================] - 165s 373ms/step - loss: 17.6455 - time_to_eruption_loss: 3.7052 - quantile_loss: 1.8184 - time_to_eruption_mae: 3.7052 - quantile_mae: 3.7824 - val_loss: 25.1549 - val_time_to_eruption_loss: 5.3847 - val_quantile_loss: 2.6107 - val_time_to_eruption_mae: 5.3847 - val_quantile_mae: 5.3490\n",
      "Epoch 76/100\n",
      "444/444 [==============================] - 168s 379ms/step - loss: 18.1546 - time_to_eruption_loss: 3.8194 - quantile_loss: 1.8703 - time_to_eruption_mae: 3.8194 - quantile_mae: 3.8946 - val_loss: 14.3301 - val_time_to_eruption_loss: 2.9556 - val_quantile_loss: 1.5018 - val_time_to_eruption_mae: 2.9556 - val_quantile_mae: 3.1628\n",
      "Epoch 77/100\n",
      "444/444 [==============================] - 166s 374ms/step - loss: 17.4296 - time_to_eruption_loss: 3.6578 - quantile_loss: 1.7944 - time_to_eruption_mae: 3.6578 - quantile_mae: 3.7333 - val_loss: 14.7241 - val_time_to_eruption_loss: 3.0552 - val_quantile_loss: 1.5009 - val_time_to_eruption_mae: 3.0552 - val_quantile_mae: 3.1283\n",
      "Epoch 78/100\n",
      "444/444 [==============================] - 166s 374ms/step - loss: 17.5586 - time_to_eruption_loss: 3.6873 - quantile_loss: 1.8080 - time_to_eruption_mae: 3.6873 - quantile_mae: 3.7634 - val_loss: 13.0545 - val_time_to_eruption_loss: 2.6826 - val_quantile_loss: 1.3230 - val_time_to_eruption_mae: 2.6826 - val_quantile_mae: 2.7716\n",
      "Epoch 79/100\n",
      "444/444 [==============================] - 166s 374ms/step - loss: 17.1532 - time_to_eruption_loss: 3.5978 - quantile_loss: 1.7625 - time_to_eruption_mae: 3.5978 - quantile_mae: 3.6718 - val_loss: 12.4435 - val_time_to_eruption_loss: 2.5484 - val_quantile_loss: 1.2501 - val_time_to_eruption_mae: 2.5484 - val_quantile_mae: 2.6404\n",
      "Epoch 80/100\n",
      "444/444 [==============================] - 166s 374ms/step - loss: 17.4163 - time_to_eruption_loss: 3.6569 - quantile_loss: 1.7898 - time_to_eruption_mae: 3.6569 - quantile_mae: 3.7305 - val_loss: 17.5328 - val_time_to_eruption_loss: 3.6734 - val_quantile_loss: 1.8404 - val_time_to_eruption_mae: 3.6734 - val_quantile_mae: 3.8529\n",
      "Epoch 81/100\n",
      "444/444 [==============================] - 166s 374ms/step - loss: 17.2598 - time_to_eruption_loss: 3.6212 - quantile_loss: 1.7771 - time_to_eruption_mae: 3.6212 - quantile_mae: 3.6986 - val_loss: 15.6262 - val_time_to_eruption_loss: 3.2397 - val_quantile_loss: 1.6702 - val_time_to_eruption_mae: 3.2397 - val_quantile_mae: 3.4982\n",
      "Epoch 82/100\n",
      "444/444 [==============================] - 166s 374ms/step - loss: 16.6501 - time_to_eruption_loss: 3.4864 - quantile_loss: 1.7086 - time_to_eruption_mae: 3.4864 - quantile_mae: 3.5566 - val_loss: 16.6901 - val_time_to_eruption_loss: 3.5048 - val_quantile_loss: 1.6771 - val_time_to_eruption_mae: 3.5048 - val_quantile_mae: 3.5070\n",
      "Epoch 83/100\n",
      "444/444 [==============================] - 166s 373ms/step - loss: 16.8556 - time_to_eruption_loss: 3.5330 - quantile_loss: 1.7319 - time_to_eruption_mae: 3.5330 - quantile_mae: 3.6045 - val_loss: 15.2247 - val_time_to_eruption_loss: 3.1724 - val_quantile_loss: 1.5448 - val_time_to_eruption_mae: 3.1724 - val_quantile_mae: 3.2460\n",
      "Epoch 84/100\n",
      "444/444 [==============================] - 164s 369ms/step - loss: 16.7549 - time_to_eruption_loss: 3.5108 - quantile_loss: 1.7227 - time_to_eruption_mae: 3.5108 - quantile_mae: 3.5851 - val_loss: 17.0136 - val_time_to_eruption_loss: 3.5739 - val_quantile_loss: 1.7292 - val_time_to_eruption_mae: 3.5739 - val_quantile_mae: 3.6171\n",
      "Epoch 85/100\n",
      "444/444 [==============================] - 164s 369ms/step - loss: 16.5899 - time_to_eruption_loss: 3.4751 - quantile_loss: 1.7006 - time_to_eruption_mae: 3.4751 - quantile_mae: 3.5439 - val_loss: 13.3625 - val_time_to_eruption_loss: 2.7552 - val_quantile_loss: 1.3544 - val_time_to_eruption_mae: 2.7552 - val_quantile_mae: 2.8412\n",
      "Epoch 86/100\n",
      "444/444 [==============================] - 164s 370ms/step - loss: 16.8019 - time_to_eruption_loss: 3.5234 - quantile_loss: 1.7227 - time_to_eruption_mae: 3.5234 - quantile_mae: 3.5990 - val_loss: 13.0055 - val_time_to_eruption_loss: 2.6798 - val_quantile_loss: 1.3007 - val_time_to_eruption_mae: 2.6798 - val_quantile_mae: 2.7444\n",
      "Epoch 87/100\n",
      "444/444 [==============================] - 906s 2s/step - loss: 16.9481 - time_to_eruption_loss: 3.5539 - quantile_loss: 1.7428 - time_to_eruption_mae: 3.5539 - quantile_mae: 3.6224 - val_loss: 14.5488 - val_time_to_eruption_loss: 3.0171 - val_quantile_loss: 1.4862 - val_time_to_eruption_mae: 3.0171 - val_quantile_mae: 3.1143\n",
      "Epoch 88/100\n",
      "444/444 [==============================] - 606s 1s/step - loss: 16.9432 - time_to_eruption_loss: 3.5526 - quantile_loss: 1.7415 - time_to_eruption_mae: 3.5526 - quantile_mae: 3.6241 - val_loss: 13.0854 - val_time_to_eruption_loss: 2.6902 - val_quantile_loss: 1.3343 - val_time_to_eruption_mae: 2.6902 - val_quantile_mae: 2.8227\n",
      "Epoch 89/100\n",
      "444/444 [==============================] - 816s 2s/step - loss: 16.6888 - time_to_eruption_loss: 3.4962 - quantile_loss: 1.7147 - time_to_eruption_mae: 3.4962 - quantile_mae: 3.5701 - val_loss: 12.3887 - val_time_to_eruption_loss: 2.5440 - val_quantile_loss: 1.2231 - val_time_to_eruption_mae: 2.5440 - val_quantile_mae: 2.5738\n",
      "Epoch 90/100\n",
      "444/444 [==============================] - 714s 2s/step - loss: 16.5980 - time_to_eruption_loss: 3.4768 - quantile_loss: 1.7022 - time_to_eruption_mae: 3.4768 - quantile_mae: 3.5430 - val_loss: 13.2499 - val_time_to_eruption_loss: 2.7299 - val_quantile_loss: 1.3427 - val_time_to_eruption_mae: 2.7299 - val_quantile_mae: 2.8164\n",
      "Epoch 91/100\n",
      "444/444 [==============================] - 734s 2s/step - loss: 16.3525 - time_to_eruption_loss: 3.4221 - quantile_loss: 1.6783 - time_to_eruption_mae: 3.4221 - quantile_mae: 3.4930 - val_loss: 12.5458 - val_time_to_eruption_loss: 2.5770 - val_quantile_loss: 1.2545 - val_time_to_eruption_mae: 2.5770 - val_quantile_mae: 2.6286\n",
      "Epoch 92/100\n",
      "444/444 [==============================] - 734s 2s/step - loss: 16.0136 - time_to_eruption_loss: 3.3477 - quantile_loss: 1.6405 - time_to_eruption_mae: 3.3477 - quantile_mae: 3.4089 - val_loss: 13.0126 - val_time_to_eruption_loss: 2.6896 - val_quantile_loss: 1.2731 - val_time_to_eruption_mae: 2.6896 - val_quantile_mae: 2.6882\n",
      "Epoch 93/100\n",
      "444/444 [==============================] - 812s 2s/step - loss: 16.0533 - time_to_eruption_loss: 3.3565 - quantile_loss: 1.6465 - time_to_eruption_mae: 3.3565 - quantile_mae: 3.4321 - val_loss: 16.4435 - val_time_to_eruption_loss: 3.4443 - val_quantile_loss: 1.6867 - val_time_to_eruption_mae: 3.4443 - val_quantile_mae: 3.5042\n",
      "Epoch 94/100\n",
      "444/444 [==============================] - 645s 1s/step - loss: 15.9805 - time_to_eruption_loss: 3.3413 - quantile_loss: 1.6359 - time_to_eruption_mae: 3.3413 - quantile_mae: 3.4127 - val_loss: 12.2187 - val_time_to_eruption_loss: 2.5003 - val_quantile_loss: 1.2393 - val_time_to_eruption_mae: 2.5003 - val_quantile_mae: 2.6466\n",
      "Epoch 95/100\n",
      "444/444 [==============================] - 747s 2s/step - loss: 15.8488 - time_to_eruption_loss: 3.3120 - quantile_loss: 1.6234 - time_to_eruption_mae: 3.3120 - quantile_mae: 3.3833 - val_loss: 16.2852 - val_time_to_eruption_loss: 3.4089 - val_quantile_loss: 1.6723 - val_time_to_eruption_mae: 3.4089 - val_quantile_mae: 3.4672\n",
      "Epoch 96/100\n",
      "444/444 [==============================] - 527s 1s/step - loss: 16.0331 - time_to_eruption_loss: 3.3537 - quantile_loss: 1.6419 - time_to_eruption_mae: 3.3537 - quantile_mae: 3.4223 - val_loss: 11.6953 - val_time_to_eruption_loss: 2.3826 - val_quantile_loss: 1.1892 - val_time_to_eruption_mae: 2.3826 - val_quantile_mae: 2.5380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "444/444 [==============================] - 710s 2s/step - loss: 15.8268 - time_to_eruption_loss: 3.3078 - quantile_loss: 1.6214 - time_to_eruption_mae: 3.3078 - quantile_mae: 3.3733 - val_loss: 12.3414 - val_time_to_eruption_loss: 2.5335 - val_quantile_loss: 1.2339 - val_time_to_eruption_mae: 2.5335 - val_quantile_mae: 2.5816\n",
      "Epoch 98/100\n",
      "444/444 [==============================] - 685s 2s/step - loss: 16.0562 - time_to_eruption_loss: 3.3591 - quantile_loss: 1.6460 - time_to_eruption_mae: 3.3591 - quantile_mae: 3.4210 - val_loss: 11.0117 - val_time_to_eruption_loss: 2.2316 - val_quantile_loss: 1.1105 - val_time_to_eruption_mae: 2.2316 - val_quantile_mae: 2.3685\n",
      "Epoch 99/100\n",
      "444/444 [==============================] - 641s 1s/step - loss: 15.6863 - time_to_eruption_loss: 3.2762 - quantile_loss: 1.6074 - time_to_eruption_mae: 3.2762 - quantile_mae: 3.3416 - val_loss: 11.9141 - val_time_to_eruption_loss: 2.4365 - val_quantile_loss: 1.1947 - val_time_to_eruption_mae: 2.4365 - val_quantile_mae: 2.5135\n",
      "Epoch 100/100\n",
      "444/444 [==============================] - 642s 1s/step - loss: 16.3631 - time_to_eruption_loss: 3.4268 - quantile_loss: 1.6818 - time_to_eruption_mae: 3.4268 - quantile_mae: 3.4969 - val_loss: 12.9887 - val_time_to_eruption_loss: 2.6744 - val_quantile_loss: 1.3168 - val_time_to_eruption_mae: 2.6744 - val_quantile_mae: 2.7599\n",
      "Restoring model weights from the end of the best epoch.\n",
      " 2/28 [=>............................] - ETA: 6sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0219s vs `on_predict_batch_end` time: 0.5445s). Check your callbacks.\n",
      "28/28 [==============================] - 15s 533ms/step\n",
      "INFO:tensorflow:Assets written to: ./models/model_cnn2d_Big_2\\assets\n",
      "************************************************************\n",
      "Prediction MAE: 2231595.6248999997\n",
      "************************************************************\n",
      "Num Fold: 4\n",
      "Train segments: 3545 Val segments: 886\n",
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_20 (ConvModel (None, 16, 14, 128)  576352      input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_21 (ConvModel (None, 16, 14, 128)  576352      input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_22 (ConvModel (None, 16, 14, 128)  576352      input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_23 (ConvModel (None, 16, 14, 128)  576352      input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_24 (ConvModel (None, 16, 14, 128)  576352      input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_25 (ConvModel (None, 16, 14, 128)  576352      input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_26 (ConvModel (None, 16, 14, 128)  576352      input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_27 (ConvModel (None, 16, 14, 128)  576352      input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_28 (ConvModel (None, 16, 14, 128)  576352      input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_29 (ConvModel (None, 16, 14, 128)  576352      input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_20 (Gl (None, 128)          0           conv_model_sensor_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_21 (Gl (None, 128)          0           conv_model_sensor_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_22 (Gl (None, 128)          0           conv_model_sensor_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_23 (Gl (None, 128)          0           conv_model_sensor_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_24 (Gl (None, 128)          0           conv_model_sensor_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_25 (Gl (None, 128)          0           conv_model_sensor_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_26 (Gl (None, 128)          0           conv_model_sensor_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_27 (Gl (None, 128)          0           conv_model_sensor_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_28 (Gl (None, 128)          0           conv_model_sensor_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_29 (Gl (None, 128)          0           conv_model_sensor_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1280)         0           global_average_pooling2d_20[0][0]\n",
      "                                                                 global_average_pooling2d_21[0][0]\n",
      "                                                                 global_average_pooling2d_22[0][0]\n",
      "                                                                 global_average_pooling2d_23[0][0]\n",
      "                                                                 global_average_pooling2d_24[0][0]\n",
      "                                                                 global_average_pooling2d_25[0][0]\n",
      "                                                                 global_average_pooling2d_26[0][0]\n",
      "                                                                 global_average_pooling2d_27[0][0]\n",
      "                                                                 global_average_pooling2d_28[0][0]\n",
      "                                                                 global_average_pooling2d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_126 (Dropout)           (None, 1280)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 300)          384300      dropout_126[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 300)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)           (None, 300)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           19264       dropout_127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_128 (Dropout)           (None, 64)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_to_eruption (Dense)        (None, 1)            65          dropout_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "quantile (Dense)                (None, 3)            195         dropout_128[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,167,344\n",
      "Trainable params: 6,151,664\n",
      "Non-trainable params: 15,680\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 154s 346ms/step - loss: 55.5999 - time_to_eruption_loss: 12.0616 - quantile_loss: 6.3164 - time_to_eruption_mae: 12.0616 - quantile_mae: 13.2058 - val_loss: 91.3969 - val_time_to_eruption_loss: 20.3992 - val_quantile_loss: 8.7387 - val_time_to_eruption_mae: 20.3992 - val_quantile_mae: 18.2647\n",
      "Epoch 2/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 52.7513 - time_to_eruption_loss: 11.4729 - quantile_loss: 5.7794 - time_to_eruption_mae: 11.4729 - quantile_mae: 12.0818 - val_loss: 59.0060 - val_time_to_eruption_loss: 13.0038 - val_quantile_loss: 5.8956 - val_time_to_eruption_mae: 13.0038 - val_quantile_mae: 12.3625\n",
      "Epoch 3/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 51.4683 - time_to_eruption_loss: 11.1991 - quantile_loss: 5.5629 - time_to_eruption_mae: 11.1991 - quantile_mae: 11.6018 - val_loss: 50.2094 - val_time_to_eruption_loss: 10.9261 - val_quantile_loss: 5.3804 - val_time_to_eruption_mae: 10.9261 - val_quantile_mae: 11.3296\n",
      "Epoch 4/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 49.3668 - time_to_eruption_loss: 10.7215 - quantile_loss: 5.3508 - time_to_eruption_mae: 10.7215 - quantile_mae: 11.1547 - val_loss: 56.2005 - val_time_to_eruption_loss: 12.2691 - val_quantile_loss: 5.9858 - val_time_to_eruption_mae: 12.2691 - val_quantile_mae: 12.3513\n",
      "Epoch 5/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 50.3925 - time_to_eruption_loss: 10.9467 - quantile_loss: 5.4526 - time_to_eruption_mae: 10.9467 - quantile_mae: 11.3916 - val_loss: 49.9490 - val_time_to_eruption_loss: 10.8401 - val_quantile_loss: 5.4262 - val_time_to_eruption_mae: 10.8401 - val_quantile_mae: 11.3412\n",
      "Epoch 6/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 49.8993 - time_to_eruption_loss: 10.8377 - quantile_loss: 5.3801 - time_to_eruption_mae: 10.8377 - quantile_mae: 11.1753 - val_loss: 50.4132 - val_time_to_eruption_loss: 10.9679 - val_quantile_loss: 5.3697 - val_time_to_eruption_mae: 10.9679 - val_quantile_mae: 11.2895\n",
      "Epoch 7/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 49.6934 - time_to_eruption_loss: 10.7972 - quantile_loss: 5.3277 - time_to_eruption_mae: 10.7972 - quantile_mae: 11.1143 - val_loss: 59.9687 - val_time_to_eruption_loss: 13.0670 - val_quantile_loss: 6.5278 - val_time_to_eruption_mae: 13.0670 - val_quantile_mae: 13.5234\n",
      "Epoch 8/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 47.9716 - time_to_eruption_loss: 10.4166 - quantile_loss: 5.1362 - time_to_eruption_mae: 10.4166 - quantile_mae: 10.6955 - val_loss: 50.7808 - val_time_to_eruption_loss: 11.0605 - val_quantile_loss: 5.3757 - val_time_to_eruption_mae: 11.0605 - val_quantile_mae: 11.1757\n",
      "Epoch 9/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 46.9798 - time_to_eruption_loss: 10.1929 - quantile_loss: 5.0499 - time_to_eruption_mae: 10.1929 - quantile_mae: 10.5211 - val_loss: 46.5009 - val_time_to_eruption_loss: 10.1046 - val_quantile_loss: 4.9305 - val_time_to_eruption_mae: 10.1046 - val_quantile_mae: 10.2118\n",
      "Epoch 10/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 46.0382 - time_to_eruption_loss: 9.9964 - quantile_loss: 4.9094 - time_to_eruption_mae: 9.9964 - quantile_mae: 10.2216 - val_loss: 42.7947 - val_time_to_eruption_loss: 9.2815 - val_quantile_loss: 4.5352 - val_time_to_eruption_mae: 9.2815 - val_quantile_mae: 9.5017\n",
      "Epoch 11/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 45.4083 - time_to_eruption_loss: 9.8506 - quantile_loss: 4.8722 - time_to_eruption_mae: 9.8506 - quantile_mae: 10.1218 - val_loss: 52.6447 - val_time_to_eruption_loss: 11.4145 - val_quantile_loss: 5.8580 - val_time_to_eruption_mae: 11.4145 - val_quantile_mae: 11.9970\n",
      "Epoch 12/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 44.7343 - time_to_eruption_loss: 9.7027 - quantile_loss: 4.7976 - time_to_eruption_mae: 9.7027 - quantile_mae: 9.9494 - val_loss: 55.6911 - val_time_to_eruption_loss: 12.1563 - val_quantile_loss: 5.9403 - val_time_to_eruption_mae: 12.1563 - val_quantile_mae: 12.3831\n",
      "Epoch 13/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 44.2323 - time_to_eruption_loss: 9.5951 - quantile_loss: 4.7322 - time_to_eruption_mae: 9.5951 - quantile_mae: 9.8281 - val_loss: 41.3195 - val_time_to_eruption_loss: 8.9630 - val_quantile_loss: 4.3480 - val_time_to_eruption_mae: 8.9630 - val_quantile_mae: 9.1107\n",
      "Epoch 14/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 42.9716 - time_to_eruption_loss: 9.3177 - quantile_loss: 4.5855 - time_to_eruption_mae: 9.3177 - quantile_mae: 9.5491 - val_loss: 46.1230 - val_time_to_eruption_loss: 10.0290 - val_quantile_loss: 4.8959 - val_time_to_eruption_mae: 10.0290 - val_quantile_mae: 10.1204\n",
      "Epoch 15/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 42.1901 - time_to_eruption_loss: 9.1418 - quantile_loss: 4.5110 - time_to_eruption_mae: 9.1418 - quantile_mae: 9.3773 - val_loss: 47.8764 - val_time_to_eruption_loss: 10.4035 - val_quantile_loss: 5.1505 - val_time_to_eruption_mae: 10.4035 - val_quantile_mae: 10.5329\n",
      "Epoch 16/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 40.8727 - time_to_eruption_loss: 8.8510 - quantile_loss: 4.3579 - time_to_eruption_mae: 8.8510 - quantile_mae: 9.0491 - val_loss: 50.0172 - val_time_to_eruption_loss: 10.8878 - val_quantile_loss: 5.3570 - val_time_to_eruption_mae: 10.8878 - val_quantile_mae: 11.0729\n",
      "Epoch 17/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 41.0844 - time_to_eruption_loss: 8.9002 - quantile_loss: 4.3734 - time_to_eruption_mae: 8.9002 - quantile_mae: 9.0852 - val_loss: 46.6087 - val_time_to_eruption_loss: 10.1300 - val_quantile_loss: 4.9758 - val_time_to_eruption_mae: 10.1300 - val_quantile_mae: 10.1910\n",
      "Epoch 18/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 39.4978 - time_to_eruption_loss: 8.5464 - quantile_loss: 4.2017 - time_to_eruption_mae: 8.5464 - quantile_mae: 8.7369 - val_loss: 36.6445 - val_time_to_eruption_loss: 7.9147 - val_quantile_loss: 3.8816 - val_time_to_eruption_mae: 7.9147 - val_quantile_mae: 8.1128\n",
      "Epoch 19/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 39.4603 - time_to_eruption_loss: 8.5434 - quantile_loss: 4.1899 - time_to_eruption_mae: 8.5434 - quantile_mae: 8.7264 - val_loss: 38.6517 - val_time_to_eruption_loss: 8.3658 - val_quantile_loss: 4.1000 - val_time_to_eruption_mae: 8.3658 - val_quantile_mae: 8.4873\n",
      "Epoch 20/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 38.0661 - time_to_eruption_loss: 8.2330 - quantile_loss: 4.0465 - time_to_eruption_mae: 8.2330 - quantile_mae: 8.4238 - val_loss: 36.9485 - val_time_to_eruption_loss: 7.9873 - val_quantile_loss: 3.9159 - val_time_to_eruption_mae: 7.9873 - val_quantile_mae: 8.1864\n",
      "Epoch 21/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 35.9628 - time_to_eruption_loss: 7.7654 - quantile_loss: 3.8260 - time_to_eruption_mae: 7.7654 - quantile_mae: 7.9604 - val_loss: 33.9754 - val_time_to_eruption_loss: 7.3239 - val_quantile_loss: 3.6109 - val_time_to_eruption_mae: 7.3239 - val_quantile_mae: 7.4681\n",
      "Epoch 22/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 34.7866 - time_to_eruption_loss: 7.5073 - quantile_loss: 3.6899 - time_to_eruption_mae: 7.5073 - quantile_mae: 7.6587 - val_loss: 37.0583 - val_time_to_eruption_loss: 8.0179 - val_quantile_loss: 3.9229 - val_time_to_eruption_mae: 8.0179 - val_quantile_mae: 8.0800\n",
      "Epoch 23/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 33.8998 - time_to_eruption_loss: 7.3136 - quantile_loss: 3.5879 - time_to_eruption_mae: 7.3136 - quantile_mae: 7.4554 - val_loss: 32.5548 - val_time_to_eruption_loss: 7.0258 - val_quantile_loss: 3.3973 - val_time_to_eruption_mae: 7.0258 - val_quantile_mae: 7.0874\n",
      "Epoch 24/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 35.8284 - time_to_eruption_loss: 7.7445 - quantile_loss: 3.7886 - time_to_eruption_mae: 7.7445 - quantile_mae: 7.8903 - val_loss: 38.3543 - val_time_to_eruption_loss: 8.2998 - val_quantile_loss: 4.0928 - val_time_to_eruption_mae: 8.2998 - val_quantile_mae: 8.5894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 34.0273 - time_to_eruption_loss: 7.3386 - quantile_loss: 3.6066 - time_to_eruption_mae: 7.3386 - quantile_mae: 7.4885 - val_loss: 33.9402 - val_time_to_eruption_loss: 7.3163 - val_quantile_loss: 3.6039 - val_time_to_eruption_mae: 7.3163 - val_quantile_mae: 7.4711\n",
      "Epoch 26/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 34.2748 - time_to_eruption_loss: 7.3914 - quantile_loss: 3.6326 - time_to_eruption_mae: 7.3914 - quantile_mae: 7.5439 - val_loss: 31.4209 - val_time_to_eruption_loss: 6.7464 - val_quantile_loss: 3.3613 - val_time_to_eruption_mae: 6.7464 - val_quantile_mae: 7.0879\n",
      "Epoch 27/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 31.5305 - time_to_eruption_loss: 6.7795 - quantile_loss: 3.3246 - time_to_eruption_mae: 6.7795 - quantile_mae: 6.9041 - val_loss: 31.2439 - val_time_to_eruption_loss: 6.7004 - val_quantile_loss: 3.3488 - val_time_to_eruption_mae: 6.7004 - val_quantile_mae: 7.0090\n",
      "Epoch 28/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 31.1171 - time_to_eruption_loss: 6.6873 - quantile_loss: 3.2802 - time_to_eruption_mae: 6.6873 - quantile_mae: 6.8142 - val_loss: 31.7505 - val_time_to_eruption_loss: 6.8315 - val_quantile_loss: 3.3396 - val_time_to_eruption_mae: 6.8315 - val_quantile_mae: 6.9372\n",
      "Epoch 29/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 30.2728 - time_to_eruption_loss: 6.5012 - quantile_loss: 3.1883 - time_to_eruption_mae: 6.5012 - quantile_mae: 6.6298 - val_loss: 30.1207 - val_time_to_eruption_loss: 6.4711 - val_quantile_loss: 3.1609 - val_time_to_eruption_mae: 6.4711 - val_quantile_mae: 6.5325\n",
      "Epoch 30/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 29.6138 - time_to_eruption_loss: 6.3571 - quantile_loss: 3.1127 - time_to_eruption_mae: 6.3571 - quantile_mae: 6.4692 - val_loss: 34.5039 - val_time_to_eruption_loss: 7.4395 - val_quantile_loss: 3.6740 - val_time_to_eruption_mae: 7.4395 - val_quantile_mae: 7.5396\n",
      "Epoch 31/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 28.8505 - time_to_eruption_loss: 6.1872 - quantile_loss: 3.0326 - time_to_eruption_mae: 6.1872 - quantile_mae: 6.3085 - val_loss: 27.0527 - val_time_to_eruption_loss: 5.7928 - val_quantile_loss: 2.8158 - val_time_to_eruption_mae: 5.7928 - val_quantile_mae: 5.9271\n",
      "Epoch 32/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 28.8371 - time_to_eruption_loss: 6.1849 - quantile_loss: 3.0352 - time_to_eruption_mae: 6.1849 - quantile_mae: 6.3099 - val_loss: 28.5170 - val_time_to_eruption_loss: 6.1144 - val_quantile_loss: 2.9985 - val_time_to_eruption_mae: 6.1144 - val_quantile_mae: 6.2362\n",
      "Epoch 33/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 28.8132 - time_to_eruption_loss: 6.1808 - quantile_loss: 3.0282 - time_to_eruption_mae: 6.1808 - quantile_mae: 6.3056 - val_loss: 26.9951 - val_time_to_eruption_loss: 5.7666 - val_quantile_loss: 2.8616 - val_time_to_eruption_mae: 5.7666 - val_quantile_mae: 6.0150\n",
      "Epoch 34/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 28.5956 - time_to_eruption_loss: 6.1294 - quantile_loss: 3.0067 - time_to_eruption_mae: 6.1294 - quantile_mae: 6.2366 - val_loss: 27.9395 - val_time_to_eruption_loss: 5.9873 - val_quantile_loss: 2.9163 - val_time_to_eruption_mae: 5.9873 - val_quantile_mae: 6.1181\n",
      "Epoch 35/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 27.3808 - time_to_eruption_loss: 5.8593 - quantile_loss: 2.8736 - time_to_eruption_mae: 5.8593 - quantile_mae: 5.9720 - val_loss: 23.8219 - val_time_to_eruption_loss: 5.0682 - val_quantile_loss: 2.4819 - val_time_to_eruption_mae: 5.0682 - val_quantile_mae: 5.2401\n",
      "Epoch 36/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 26.9722 - time_to_eruption_loss: 5.7698 - quantile_loss: 2.8279 - time_to_eruption_mae: 5.7698 - quantile_mae: 5.8799 - val_loss: 33.3277 - val_time_to_eruption_loss: 7.1933 - val_quantile_loss: 3.4901 - val_time_to_eruption_mae: 7.1933 - val_quantile_mae: 7.1636\n",
      "Epoch 37/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 26.3866 - time_to_eruption_loss: 5.6396 - quantile_loss: 2.7620 - time_to_eruption_mae: 5.6396 - quantile_mae: 5.7466 - val_loss: 27.0179 - val_time_to_eruption_loss: 5.7743 - val_quantile_loss: 2.8526 - val_time_to_eruption_mae: 5.7743 - val_quantile_mae: 5.9393\n",
      "Epoch 38/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 26.5978 - time_to_eruption_loss: 5.6860 - quantile_loss: 2.7853 - time_to_eruption_mae: 5.6860 - quantile_mae: 5.7944 - val_loss: 26.6684 - val_time_to_eruption_loss: 5.6957 - val_quantile_loss: 2.8127 - val_time_to_eruption_mae: 5.6957 - val_quantile_mae: 5.8990\n",
      "Epoch 39/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 26.5373 - time_to_eruption_loss: 5.6705 - quantile_loss: 2.7787 - time_to_eruption_mae: 5.6705 - quantile_mae: 5.7822 - val_loss: 22.6047 - val_time_to_eruption_loss: 4.7913 - val_quantile_loss: 2.3582 - val_time_to_eruption_mae: 4.7913 - val_quantile_mae: 4.9242\n",
      "Epoch 40/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 26.6351 - time_to_eruption_loss: 5.6919 - quantile_loss: 2.7862 - time_to_eruption_mae: 5.6919 - quantile_mae: 5.7892 - val_loss: 29.2255 - val_time_to_eruption_loss: 6.2870 - val_quantile_loss: 2.9936 - val_time_to_eruption_mae: 6.2870 - val_quantile_mae: 6.2406\n",
      "Epoch 41/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 25.5294 - time_to_eruption_loss: 5.4439 - quantile_loss: 2.6707 - time_to_eruption_mae: 5.4439 - quantile_mae: 5.5527 - val_loss: 30.1442 - val_time_to_eruption_loss: 6.4772 - val_quantile_loss: 3.1533 - val_time_to_eruption_mae: 6.4772 - val_quantile_mae: 6.4939\n",
      "Epoch 42/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 24.6852 - time_to_eruption_loss: 5.2580 - quantile_loss: 2.5751 - time_to_eruption_mae: 5.2580 - quantile_mae: 5.3521 - val_loss: 30.1248 - val_time_to_eruption_loss: 6.4744 - val_quantile_loss: 3.1502 - val_time_to_eruption_mae: 6.4744 - val_quantile_mae: 6.5229\n",
      "Epoch 43/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 24.6604 - time_to_eruption_loss: 5.2522 - quantile_loss: 2.5741 - time_to_eruption_mae: 5.2522 - quantile_mae: 5.3476 - val_loss: 24.4725 - val_time_to_eruption_loss: 5.2061 - val_quantile_loss: 2.5649 - val_time_to_eruption_mae: 5.2061 - val_quantile_mae: 5.2897\n",
      "Epoch 44/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 24.5124 - time_to_eruption_loss: 5.2197 - quantile_loss: 2.5529 - time_to_eruption_mae: 5.2197 - quantile_mae: 5.3168 - val_loss: 24.5751 - val_time_to_eruption_loss: 5.2300 - val_quantile_loss: 2.5779 - val_time_to_eruption_mae: 5.2300 - val_quantile_mae: 5.4193\n",
      "Epoch 45/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 24.9839 - time_to_eruption_loss: 5.3243 - quantile_loss: 2.6091 - time_to_eruption_mae: 5.3243 - quantile_mae: 5.4386 - val_loss: 18.9873 - val_time_to_eruption_loss: 3.9887 - val_quantile_loss: 1.9547 - val_time_to_eruption_mae: 3.9887 - val_quantile_mae: 4.1226\n",
      "Epoch 46/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 24.1788 - time_to_eruption_loss: 5.1451 - quantile_loss: 2.5218 - time_to_eruption_mae: 5.1451 - quantile_mae: 5.2382 - val_loss: 28.3011 - val_time_to_eruption_loss: 6.0571 - val_quantile_loss: 2.9915 - val_time_to_eruption_mae: 6.0571 - val_quantile_mae: 6.1800\n",
      "Epoch 47/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 24.1086 - time_to_eruption_loss: 5.1275 - quantile_loss: 2.5125 - time_to_eruption_mae: 5.1275 - quantile_mae: 5.2132 - val_loss: 22.0355 - val_time_to_eruption_loss: 4.6649 - val_quantile_loss: 2.2927 - val_time_to_eruption_mae: 4.6649 - val_quantile_mae: 4.7556\n",
      "Epoch 48/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 23.7172 - time_to_eruption_loss: 5.0408 - quantile_loss: 2.4703 - time_to_eruption_mae: 5.0408 - quantile_mae: 5.1269 - val_loss: 23.2993 - val_time_to_eruption_loss: 4.9413 - val_quantile_loss: 2.4515 - val_time_to_eruption_mae: 4.9413 - val_quantile_mae: 5.0865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 23.3080 - time_to_eruption_loss: 4.9499 - quantile_loss: 2.4243 - time_to_eruption_mae: 4.9499 - quantile_mae: 5.0434 - val_loss: 19.3448 - val_time_to_eruption_loss: 4.0627 - val_quantile_loss: 2.0109 - val_time_to_eruption_mae: 4.0627 - val_quantile_mae: 4.2447\n",
      "Epoch 50/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 23.1510 - time_to_eruption_loss: 4.9151 - quantile_loss: 2.4074 - time_to_eruption_mae: 4.9151 - quantile_mae: 5.0144 - val_loss: 19.6486 - val_time_to_eruption_loss: 4.1365 - val_quantile_loss: 2.0207 - val_time_to_eruption_mae: 4.1365 - val_quantile_mae: 4.2340\n",
      "Epoch 51/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 22.5395 - time_to_eruption_loss: 4.7796 - quantile_loss: 2.3424 - time_to_eruption_mae: 4.7796 - quantile_mae: 4.8735 - val_loss: 19.5784 - val_time_to_eruption_loss: 4.1196 - val_quantile_loss: 2.0243 - val_time_to_eruption_mae: 4.1196 - val_quantile_mae: 4.2371\n",
      "Epoch 52/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 22.7568 - time_to_eruption_loss: 4.8286 - quantile_loss: 2.3665 - time_to_eruption_mae: 4.8286 - quantile_mae: 4.9152 - val_loss: 19.5339 - val_time_to_eruption_loss: 4.1064 - val_quantile_loss: 2.0281 - val_time_to_eruption_mae: 4.1064 - val_quantile_mae: 4.2203\n",
      "Epoch 53/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 21.8911 - time_to_eruption_loss: 4.6349 - quantile_loss: 2.2718 - time_to_eruption_mae: 4.6349 - quantile_mae: 4.7151 - val_loss: 21.9442 - val_time_to_eruption_loss: 4.6470 - val_quantile_loss: 2.2763 - val_time_to_eruption_mae: 4.6470 - val_quantile_mae: 4.7368\n",
      "Epoch 54/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 21.8391 - time_to_eruption_loss: 4.6230 - quantile_loss: 2.2647 - time_to_eruption_mae: 4.6230 - quantile_mae: 4.7122 - val_loss: 25.6087 - val_time_to_eruption_loss: 5.4613 - val_quantile_loss: 2.6835 - val_time_to_eruption_mae: 5.4613 - val_quantile_mae: 5.5620\n",
      "Epoch 55/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 22.7702 - time_to_eruption_loss: 4.8313 - quantile_loss: 2.3647 - time_to_eruption_mae: 4.8313 - quantile_mae: 4.9305 - val_loss: 20.6332 - val_time_to_eruption_loss: 4.3534 - val_quantile_loss: 2.1373 - val_time_to_eruption_mae: 4.3534 - val_quantile_mae: 4.4750\n",
      "Epoch 56/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 22.1724 - time_to_eruption_loss: 4.6976 - quantile_loss: 2.2977 - time_to_eruption_mae: 4.6976 - quantile_mae: 4.7829 - val_loss: 21.4780 - val_time_to_eruption_loss: 4.5454 - val_quantile_loss: 2.2113 - val_time_to_eruption_mae: 4.5454 - val_quantile_mae: 4.6262\n",
      "Epoch 57/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 21.4115 - time_to_eruption_loss: 4.5278 - quantile_loss: 2.2175 - time_to_eruption_mae: 4.5278 - quantile_mae: 4.6163 - val_loss: 18.1315 - val_time_to_eruption_loss: 3.7953 - val_quantile_loss: 1.8687 - val_time_to_eruption_mae: 3.7953 - val_quantile_mae: 3.9144\n",
      "Epoch 58/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 21.7780 - time_to_eruption_loss: 4.6092 - quantile_loss: 2.2592 - time_to_eruption_mae: 4.6092 - quantile_mae: 4.6956 - val_loss: 22.3549 - val_time_to_eruption_loss: 4.7368 - val_quantile_loss: 2.3253 - val_time_to_eruption_mae: 4.7368 - val_quantile_mae: 4.8323\n",
      "Epoch 59/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 21.5601 - time_to_eruption_loss: 4.5614 - quantile_loss: 2.2316 - time_to_eruption_mae: 4.5614 - quantile_mae: 4.6461 - val_loss: 20.8776 - val_time_to_eruption_loss: 4.4089 - val_quantile_loss: 2.1578 - val_time_to_eruption_mae: 4.4089 - val_quantile_mae: 4.4924\n",
      "Epoch 60/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 21.1956 - time_to_eruption_loss: 4.4781 - quantile_loss: 2.1935 - time_to_eruption_mae: 4.4781 - quantile_mae: 4.5613 - val_loss: 29.2085 - val_time_to_eruption_loss: 6.2584 - val_quantile_loss: 3.0850 - val_time_to_eruption_mae: 6.2584 - val_quantile_mae: 6.3341\n",
      "Epoch 61/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 20.5414 - time_to_eruption_loss: 4.3337 - quantile_loss: 2.1212 - time_to_eruption_mae: 4.3337 - quantile_mae: 4.4153 - val_loss: 19.2882 - val_time_to_eruption_loss: 4.0559 - val_quantile_loss: 1.9810 - val_time_to_eruption_mae: 4.0559 - val_quantile_mae: 4.1371\n",
      "Epoch 62/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 20.1702 - time_to_eruption_loss: 4.2518 - quantile_loss: 2.0824 - time_to_eruption_mae: 4.2518 - quantile_mae: 4.3357 - val_loss: 29.8650 - val_time_to_eruption_loss: 6.4070 - val_quantile_loss: 3.1581 - val_time_to_eruption_mae: 6.4070 - val_quantile_mae: 6.5124\n",
      "Epoch 63/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 20.5107 - time_to_eruption_loss: 4.3292 - quantile_loss: 2.1167 - time_to_eruption_mae: 4.3292 - quantile_mae: 4.4065 - val_loss: 15.4344 - val_time_to_eruption_loss: 3.1922 - val_quantile_loss: 1.5909 - val_time_to_eruption_mae: 3.1922 - val_quantile_mae: 3.3695\n",
      "Epoch 64/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 20.5010 - time_to_eruption_loss: 4.3266 - quantile_loss: 2.1189 - time_to_eruption_mae: 4.3266 - quantile_mae: 4.4103 - val_loss: 17.1849 - val_time_to_eruption_loss: 3.5887 - val_quantile_loss: 1.7544 - val_time_to_eruption_mae: 3.5887 - val_quantile_mae: 3.6618\n",
      "Epoch 65/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 19.5447 - time_to_eruption_loss: 4.1137 - quantile_loss: 2.0159 - time_to_eruption_mae: 4.1137 - quantile_mae: 4.1879 - val_loss: 18.7671 - val_time_to_eruption_loss: 3.9357 - val_quantile_loss: 1.9521 - val_time_to_eruption_mae: 3.9357 - val_quantile_mae: 4.0880\n",
      "Epoch 66/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 19.7191 - time_to_eruption_loss: 4.1530 - quantile_loss: 2.0349 - time_to_eruption_mae: 4.1530 - quantile_mae: 4.2255 - val_loss: 21.2838 - val_time_to_eruption_loss: 4.5014 - val_quantile_loss: 2.2067 - val_time_to_eruption_mae: 4.5014 - val_quantile_mae: 4.5736\n",
      "Epoch 67/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 19.3529 - time_to_eruption_loss: 4.0717 - quantile_loss: 1.9947 - time_to_eruption_mae: 4.0717 - quantile_mae: 4.1504 - val_loss: 15.8349 - val_time_to_eruption_loss: 3.2836 - val_quantile_loss: 1.6312 - val_time_to_eruption_mae: 3.2836 - val_quantile_mae: 3.4491\n",
      "Epoch 68/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 19.7644 - time_to_eruption_loss: 4.1636 - quantile_loss: 2.0420 - time_to_eruption_mae: 4.1636 - quantile_mae: 4.2548 - val_loss: 18.1875 - val_time_to_eruption_loss: 3.8109 - val_quantile_loss: 1.8731 - val_time_to_eruption_mae: 3.8109 - val_quantile_mae: 3.8783\n",
      "Epoch 69/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 19.4765 - time_to_eruption_loss: 4.0991 - quantile_loss: 2.0092 - time_to_eruption_mae: 4.0991 - quantile_mae: 4.1773 - val_loss: 23.5316 - val_time_to_eruption_loss: 5.0003 - val_quantile_loss: 2.4628 - val_time_to_eruption_mae: 5.0003 - val_quantile_mae: 5.0799\n",
      "Epoch 70/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 18.9567 - time_to_eruption_loss: 3.9850 - quantile_loss: 1.9502 - time_to_eruption_mae: 3.9850 - quantile_mae: 4.0627 - val_loss: 17.0993 - val_time_to_eruption_loss: 3.5705 - val_quantile_loss: 1.7523 - val_time_to_eruption_mae: 3.5705 - val_quantile_mae: 3.6718\n",
      "Epoch 71/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 19.2255 - time_to_eruption_loss: 4.0448 - quantile_loss: 1.9804 - time_to_eruption_mae: 4.0448 - quantile_mae: 4.1155 - val_loss: 21.6326 - val_time_to_eruption_loss: 4.5787 - val_quantile_loss: 2.2523 - val_time_to_eruption_mae: 4.5787 - val_quantile_mae: 4.6461\n",
      "Epoch 72/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 19.0378 - time_to_eruption_loss: 4.0025 - quantile_loss: 1.9624 - time_to_eruption_mae: 4.0025 - quantile_mae: 4.0834 - val_loss: 15.4030 - val_time_to_eruption_loss: 3.1886 - val_quantile_loss: 1.5857 - val_time_to_eruption_mae: 3.1886 - val_quantile_mae: 3.3432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 18.5368 - time_to_eruption_loss: 3.8921 - quantile_loss: 1.9071 - time_to_eruption_mae: 3.8921 - quantile_mae: 3.9678 - val_loss: 18.3305 - val_time_to_eruption_loss: 3.8455 - val_quantile_loss: 1.8886 - val_time_to_eruption_mae: 3.8455 - val_quantile_mae: 3.9006\n",
      "Epoch 74/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 18.0669 - time_to_eruption_loss: 3.7879 - quantile_loss: 1.8574 - time_to_eruption_mae: 3.7879 - quantile_mae: 3.8587 - val_loss: 19.3508 - val_time_to_eruption_loss: 4.0700 - val_quantile_loss: 2.0143 - val_time_to_eruption_mae: 4.0700 - val_quantile_mae: 4.1689\n",
      "Epoch 75/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 18.5030 - time_to_eruption_loss: 3.8862 - quantile_loss: 1.9017 - time_to_eruption_mae: 3.8862 - quantile_mae: 3.9596 - val_loss: 15.4514 - val_time_to_eruption_loss: 3.2013 - val_quantile_loss: 1.5882 - val_time_to_eruption_mae: 3.2013 - val_quantile_mae: 3.3703\n",
      "Epoch 76/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.9215 - time_to_eruption_loss: 3.7561 - quantile_loss: 1.8417 - time_to_eruption_mae: 3.7561 - quantile_mae: 3.8304 - val_loss: 17.3653 - val_time_to_eruption_loss: 3.6313 - val_quantile_loss: 1.7876 - val_time_to_eruption_mae: 3.6313 - val_quantile_mae: 3.7297\n",
      "Epoch 77/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 18.2196 - time_to_eruption_loss: 3.8224 - quantile_loss: 1.8740 - time_to_eruption_mae: 3.8224 - quantile_mae: 3.8900 - val_loss: 19.3806 - val_time_to_eruption_loss: 4.0801 - val_quantile_loss: 2.0038 - val_time_to_eruption_mae: 4.0801 - val_quantile_mae: 4.1385\n",
      "Epoch 78/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 18.4481 - time_to_eruption_loss: 3.8744 - quantile_loss: 1.8948 - time_to_eruption_mae: 3.8744 - quantile_mae: 3.9492 - val_loss: 18.5839 - val_time_to_eruption_loss: 3.8993 - val_quantile_loss: 1.9321 - val_time_to_eruption_mae: 3.8993 - val_quantile_mae: 4.0144\n",
      "Epoch 79/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.9467 - time_to_eruption_loss: 3.7628 - quantile_loss: 1.8413 - time_to_eruption_mae: 3.7628 - quantile_mae: 3.8347 - val_loss: 18.2849 - val_time_to_eruption_loss: 3.8350 - val_quantile_loss: 1.8915 - val_time_to_eruption_mae: 3.8350 - val_quantile_mae: 3.9042\n",
      "Epoch 80/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.8799 - time_to_eruption_loss: 3.7481 - quantile_loss: 1.8361 - time_to_eruption_mae: 3.7481 - quantile_mae: 3.8172 - val_loss: 16.3355 - val_time_to_eruption_loss: 3.4010 - val_quantile_loss: 1.6811 - val_time_to_eruption_mae: 3.4010 - val_quantile_mae: 3.5136\n",
      "Epoch 81/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.6836 - time_to_eruption_loss: 3.7040 - quantile_loss: 1.8144 - time_to_eruption_mae: 3.7040 - quantile_mae: 3.7771 - val_loss: 19.4936 - val_time_to_eruption_loss: 4.1032 - val_quantile_loss: 2.0215 - val_time_to_eruption_mae: 4.1032 - val_quantile_mae: 4.1735\n",
      "Epoch 82/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.3323 - time_to_eruption_loss: 3.6253 - quantile_loss: 1.7751 - time_to_eruption_mae: 3.6253 - quantile_mae: 3.6935 - val_loss: 15.6151 - val_time_to_eruption_loss: 3.2426 - val_quantile_loss: 1.5918 - val_time_to_eruption_mae: 3.2426 - val_quantile_mae: 3.3258\n",
      "Epoch 83/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.6624 - time_to_eruption_loss: 3.7007 - quantile_loss: 1.8079 - time_to_eruption_mae: 3.7007 - quantile_mae: 3.7659 - val_loss: 16.9180 - val_time_to_eruption_loss: 3.5362 - val_quantile_loss: 1.7220 - val_time_to_eruption_mae: 3.5362 - val_quantile_mae: 3.5894\n",
      "Epoch 84/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.3520 - time_to_eruption_loss: 3.6305 - quantile_loss: 1.7795 - time_to_eruption_mae: 3.6305 - quantile_mae: 3.6964 - val_loss: 21.7483 - val_time_to_eruption_loss: 4.6057 - val_quantile_loss: 2.2744 - val_time_to_eruption_mae: 4.6057 - val_quantile_mae: 4.6840\n",
      "Epoch 85/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.9769 - time_to_eruption_loss: 3.7704 - quantile_loss: 1.8457 - time_to_eruption_mae: 3.7704 - quantile_mae: 3.8424 - val_loss: 18.2053 - val_time_to_eruption_loss: 3.8184 - val_quantile_loss: 1.8826 - val_time_to_eruption_mae: 3.8184 - val_quantile_mae: 3.9341\n",
      "Epoch 86/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 17.4602 - time_to_eruption_loss: 3.6560 - quantile_loss: 1.7874 - time_to_eruption_mae: 3.6560 - quantile_mae: 3.7273 - val_loss: 14.6873 - val_time_to_eruption_loss: 3.0348 - val_quantile_loss: 1.5011 - val_time_to_eruption_mae: 3.0348 - val_quantile_mae: 3.1682\n",
      "Epoch 87/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.7415 - time_to_eruption_loss: 3.4954 - quantile_loss: 1.7134 - time_to_eruption_mae: 3.4954 - quantile_mae: 3.5662 - val_loss: 16.2023 - val_time_to_eruption_loss: 3.3759 - val_quantile_loss: 1.6516 - val_time_to_eruption_mae: 3.3759 - val_quantile_mae: 3.4319\n",
      "Epoch 88/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 17.0097 - time_to_eruption_loss: 3.5554 - quantile_loss: 1.7408 - time_to_eruption_mae: 3.5554 - quantile_mae: 3.6248 - val_loss: 12.4242 - val_time_to_eruption_loss: 2.5305 - val_quantile_loss: 1.2548 - val_time_to_eruption_mae: 2.5305 - val_quantile_mae: 2.6698\n",
      "Epoch 89/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.7572 - time_to_eruption_loss: 3.4989 - quantile_loss: 1.7143 - time_to_eruption_mae: 3.4989 - quantile_mae: 3.5678 - val_loss: 14.6498 - val_time_to_eruption_loss: 3.0320 - val_quantile_loss: 1.4754 - val_time_to_eruption_mae: 3.0320 - val_quantile_mae: 3.0749\n",
      "Epoch 90/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.9863 - time_to_eruption_loss: 3.5510 - quantile_loss: 1.7376 - time_to_eruption_mae: 3.5510 - quantile_mae: 3.6234 - val_loss: 13.7604 - val_time_to_eruption_loss: 2.8295 - val_quantile_loss: 1.3983 - val_time_to_eruption_mae: 2.8295 - val_quantile_mae: 2.9656\n",
      "Epoch 91/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.4895 - time_to_eruption_loss: 3.4406 - quantile_loss: 1.6833 - time_to_eruption_mae: 3.4406 - quantile_mae: 3.5095 - val_loss: 14.2160 - val_time_to_eruption_loss: 2.9300 - val_quantile_loss: 1.4523 - val_time_to_eruption_mae: 2.9300 - val_quantile_mae: 3.0704\n",
      "Epoch 92/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.2335 - time_to_eruption_loss: 3.3832 - quantile_loss: 1.6571 - time_to_eruption_mae: 3.3832 - quantile_mae: 3.4485 - val_loss: 13.8360 - val_time_to_eruption_loss: 2.8489 - val_quantile_loss: 1.3979 - val_time_to_eruption_mae: 2.8489 - val_quantile_mae: 2.9232\n",
      "Epoch 93/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.4645 - time_to_eruption_loss: 3.4353 - quantile_loss: 1.6818 - time_to_eruption_mae: 3.4353 - quantile_mae: 3.4991 - val_loss: 16.8037 - val_time_to_eruption_loss: 3.5090 - val_quantile_loss: 1.7253 - val_time_to_eruption_mae: 3.5090 - val_quantile_mae: 3.5919\n",
      "Epoch 94/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.5527 - time_to_eruption_loss: 3.4553 - quantile_loss: 1.6898 - time_to_eruption_mae: 3.4553 - quantile_mae: 3.5241 - val_loss: 17.3057 - val_time_to_eruption_loss: 3.6180 - val_quantile_loss: 1.7924 - val_time_to_eruption_mae: 3.6180 - val_quantile_mae: 3.7231\n",
      "Epoch 95/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.2682 - time_to_eruption_loss: 3.3919 - quantile_loss: 1.6608 - time_to_eruption_mae: 3.3919 - quantile_mae: 3.4573 - val_loss: 12.4947 - val_time_to_eruption_loss: 2.5493 - val_quantile_loss: 1.2593 - val_time_to_eruption_mae: 2.5493 - val_quantile_mae: 2.6713\n",
      "Epoch 96/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.3672 - time_to_eruption_loss: 3.4137 - quantile_loss: 1.6729 - time_to_eruption_mae: 3.4137 - quantile_mae: 3.4807 - val_loss: 13.0243 - val_time_to_eruption_loss: 2.6690 - val_quantile_loss: 1.3101 - val_time_to_eruption_mae: 2.6690 - val_quantile_mae: 2.7624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "444/444 [==============================] - 152s 341ms/step - loss: 16.2166 - time_to_eruption_loss: 3.3810 - quantile_loss: 1.6550 - time_to_eruption_mae: 3.3810 - quantile_mae: 3.4476 - val_loss: 12.0689 - val_time_to_eruption_loss: 2.4532 - val_quantile_loss: 1.2190 - val_time_to_eruption_mae: 2.4532 - val_quantile_mae: 2.5752\n",
      "Epoch 98/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 16.2591 - time_to_eruption_loss: 3.3906 - quantile_loss: 1.6607 - time_to_eruption_mae: 3.3906 - quantile_mae: 3.4583 - val_loss: 14.9906 - val_time_to_eruption_loss: 3.1081 - val_quantile_loss: 1.5228 - val_time_to_eruption_mae: 3.1081 - val_quantile_mae: 3.1841\n",
      "Epoch 99/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 15.8865 - time_to_eruption_loss: 3.3078 - quantile_loss: 1.6193 - time_to_eruption_mae: 3.3078 - quantile_mae: 3.3723 - val_loss: 13.1960 - val_time_to_eruption_loss: 2.7071 - val_quantile_loss: 1.3319 - val_time_to_eruption_mae: 2.7071 - val_quantile_mae: 2.7795\n",
      "Epoch 100/100\n",
      "444/444 [==============================] - 151s 341ms/step - loss: 15.9783 - time_to_eruption_loss: 3.3283 - quantile_loss: 1.6301 - time_to_eruption_mae: 3.3283 - quantile_mae: 3.3972 - val_loss: 23.6279 - val_time_to_eruption_loss: 5.0238 - val_quantile_loss: 2.4966 - val_time_to_eruption_mae: 5.0238 - val_quantile_mae: 5.1186\n",
      "Restoring model weights from the end of the best epoch.\n",
      " 2/28 [=>............................] - ETA: 3sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0200s vs `on_predict_batch_end` time: 0.2354s). Check your callbacks.\n",
      "28/28 [==============================] - 7s 252ms/step\n",
      "INFO:tensorflow:Assets written to: ./models/model_cnn2d_Big_3\\assets\n",
      "************************************************************\n",
      "Prediction MAE: 2453247.4159572506\n",
      "************************************************************\n",
      "Num Fold: 5\n",
      "Train segments: 3545 Val segments: 886\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_33 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_35 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_37 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_39 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           [(None, 128, 235, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_30 (ConvModel (None, 16, 14, 128)  576352      input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_31 (ConvModel (None, 16, 14, 128)  576352      input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_32 (ConvModel (None, 16, 14, 128)  576352      input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_33 (ConvModel (None, 16, 14, 128)  576352      input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_34 (ConvModel (None, 16, 14, 128)  576352      input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_35 (ConvModel (None, 16, 14, 128)  576352      input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_36 (ConvModel (None, 16, 14, 128)  576352      input_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_37 (ConvModel (None, 16, 14, 128)  576352      input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_38 (ConvModel (None, 16, 14, 128)  576352      input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_model_sensor_39 (ConvModel (None, 16, 14, 128)  576352      input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_30 (Gl (None, 128)          0           conv_model_sensor_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_31 (Gl (None, 128)          0           conv_model_sensor_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_32 (Gl (None, 128)          0           conv_model_sensor_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_33 (Gl (None, 128)          0           conv_model_sensor_33[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_34 (Gl (None, 128)          0           conv_model_sensor_34[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_35 (Gl (None, 128)          0           conv_model_sensor_35[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_36 (Gl (None, 128)          0           conv_model_sensor_36[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_37 (Gl (None, 128)          0           conv_model_sensor_37[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_38 (Gl (None, 128)          0           conv_model_sensor_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_39 (Gl (None, 128)          0           conv_model_sensor_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1280)         0           global_average_pooling2d_30[0][0]\n",
      "                                                                 global_average_pooling2d_31[0][0]\n",
      "                                                                 global_average_pooling2d_32[0][0]\n",
      "                                                                 global_average_pooling2d_33[0][0]\n",
      "                                                                 global_average_pooling2d_34[0][0]\n",
      "                                                                 global_average_pooling2d_35[0][0]\n",
      "                                                                 global_average_pooling2d_36[0][0]\n",
      "                                                                 global_average_pooling2d_37[0][0]\n",
      "                                                                 global_average_pooling2d_38[0][0]\n",
      "                                                                 global_average_pooling2d_39[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_169 (Dropout)           (None, 1280)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 300)          384300      dropout_169[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 300)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_170 (Dropout)           (None, 300)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           19264       dropout_170[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_171 (Dropout)           (None, 64)           0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_to_eruption (Dense)        (None, 1)            65          dropout_171[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "quantile (Dense)                (None, 3)            195         dropout_171[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,167,344\n",
      "Trainable params: 6,151,664\n",
      "Non-trainable params: 15,680\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 141s 317ms/step - loss: 57.8108 - time_to_eruption_loss: 12.6460 - quantile_loss: 6.1927 - time_to_eruption_mae: 12.6460 - quantile_mae: 12.9726 - val_loss: 51.0138 - val_time_to_eruption_loss: 11.1296 - val_quantile_loss: 5.4402 - val_time_to_eruption_mae: 11.1296 - val_quantile_mae: 11.6290\n",
      "Epoch 2/100\n",
      "444/444 [==============================] - 138s 311ms/step - loss: 53.0573 - time_to_eruption_loss: 11.5437 - quantile_loss: 5.8113 - time_to_eruption_mae: 11.5437 - quantile_mae: 12.1284 - val_loss: 54.9370 - val_time_to_eruption_loss: 12.0100 - val_quantile_loss: 5.8136 - val_time_to_eruption_mae: 12.0100 - val_quantile_mae: 11.8949\n",
      "Epoch 3/100\n",
      "444/444 [==============================] - 138s 312ms/step - loss: 51.5229 - time_to_eruption_loss: 11.2039 - quantile_loss: 5.6124 - time_to_eruption_mae: 11.2039 - quantile_mae: 11.7552 - val_loss: 55.0014 - val_time_to_eruption_loss: 12.0135 - val_quantile_loss: 5.8368 - val_time_to_eruption_mae: 12.0135 - val_quantile_mae: 12.0436\n",
      "Epoch 4/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 50.3943 - time_to_eruption_loss: 10.9530 - quantile_loss: 5.4660 - time_to_eruption_mae: 10.9530 - quantile_mae: 11.3836 - val_loss: 53.5528 - val_time_to_eruption_loss: 11.6886 - val_quantile_loss: 5.6782 - val_time_to_eruption_mae: 11.6886 - val_quantile_mae: 11.9123\n",
      "Epoch 5/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 48.8680 - time_to_eruption_loss: 10.6176 - quantile_loss: 5.2671 - time_to_eruption_mae: 10.6176 - quantile_mae: 11.0086 - val_loss: 49.4448 - val_time_to_eruption_loss: 10.7479 - val_quantile_loss: 5.3171 - val_time_to_eruption_mae: 10.7479 - val_quantile_mae: 11.1496\n",
      "Epoch 6/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 47.4172 - time_to_eruption_loss: 10.2930 - quantile_loss: 5.1045 - time_to_eruption_mae: 10.2930 - quantile_mae: 10.6352 - val_loss: 59.0722 - val_time_to_eruption_loss: 12.9007 - val_quantile_loss: 6.3204 - val_time_to_eruption_mae: 12.9007 - val_quantile_mae: 13.1736\n",
      "Epoch 7/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 46.0755 - time_to_eruption_loss: 9.9921 - quantile_loss: 4.9631 - time_to_eruption_mae: 9.9921 - quantile_mae: 10.3260 - val_loss: 54.1701 - val_time_to_eruption_loss: 11.8382 - val_quantile_loss: 5.6810 - val_time_to_eruption_mae: 11.8382 - val_quantile_mae: 11.8918\n",
      "Epoch 8/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 45.0621 - time_to_eruption_loss: 9.7770 - quantile_loss: 4.8222 - time_to_eruption_mae: 9.7770 - quantile_mae: 10.0335 - val_loss: 50.1172 - val_time_to_eruption_loss: 10.8710 - val_quantile_loss: 5.4983 - val_time_to_eruption_mae: 10.8710 - val_quantile_mae: 11.3468\n",
      "Epoch 9/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 42.2777 - time_to_eruption_loss: 9.1543 - quantile_loss: 4.5184 - time_to_eruption_mae: 9.1543 - quantile_mae: 9.3883 - val_loss: 41.5337 - val_time_to_eruption_loss: 8.9986 - val_quantile_loss: 4.3851 - val_time_to_eruption_mae: 8.9986 - val_quantile_mae: 9.0635\n",
      "Epoch 10/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 42.2817 - time_to_eruption_loss: 9.1532 - quantile_loss: 4.5110 - time_to_eruption_mae: 9.1532 - quantile_mae: 9.3759 - val_loss: 46.3950 - val_time_to_eruption_loss: 10.0873 - val_quantile_loss: 4.8883 - val_time_to_eruption_mae: 10.0873 - val_quantile_mae: 9.9843\n",
      "Epoch 11/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 40.3790 - time_to_eruption_loss: 8.7357 - quantile_loss: 4.2845 - time_to_eruption_mae: 8.7357 - quantile_mae: 8.9003 - val_loss: 54.7443 - val_time_to_eruption_loss: 11.9452 - val_quantile_loss: 5.8144 - val_time_to_eruption_mae: 11.9452 - val_quantile_mae: 11.8811\n",
      "Epoch 12/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 39.1515 - time_to_eruption_loss: 8.4630 - quantile_loss: 4.1571 - time_to_eruption_mae: 8.4630 - quantile_mae: 8.6461 - val_loss: 40.9507 - val_time_to_eruption_loss: 8.8609 - val_quantile_loss: 4.3716 - val_time_to_eruption_mae: 8.8609 - val_quantile_mae: 9.1959\n",
      "Epoch 13/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 37.5662 - time_to_eruption_loss: 8.1105 - quantile_loss: 3.9899 - time_to_eruption_mae: 8.1105 - quantile_mae: 8.2833 - val_loss: 36.3730 - val_time_to_eruption_loss: 7.8552 - val_quantile_loss: 3.8196 - val_time_to_eruption_mae: 7.8552 - val_quantile_mae: 7.9359\n",
      "Epoch 14/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 37.0287 - time_to_eruption_loss: 7.9959 - quantile_loss: 3.9179 - time_to_eruption_mae: 7.9959 - quantile_mae: 8.1530 - val_loss: 34.8049 - val_time_to_eruption_loss: 7.5053 - val_quantile_loss: 3.6589 - val_time_to_eruption_mae: 7.5053 - val_quantile_mae: 7.6288\n",
      "Epoch 15/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 36.2724 - time_to_eruption_loss: 7.8289 - quantile_loss: 3.8385 - time_to_eruption_mae: 7.8289 - quantile_mae: 7.9803 - val_loss: 28.2627 - val_time_to_eruption_loss: 6.0421 - val_quantile_loss: 2.9807 - val_time_to_eruption_mae: 6.0421 - val_quantile_mae: 6.2096\n",
      "Epoch 16/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 34.4178 - time_to_eruption_loss: 7.4148 - quantile_loss: 3.6445 - time_to_eruption_mae: 7.4148 - quantile_mae: 7.5527 - val_loss: 41.2874 - val_time_to_eruption_loss: 8.9675 - val_quantile_loss: 4.3024 - val_time_to_eruption_mae: 8.9675 - val_quantile_mae: 8.9216\n",
      "Epoch 17/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 34.9051 - time_to_eruption_loss: 7.5242 - quantile_loss: 3.6918 - time_to_eruption_mae: 7.5242 - quantile_mae: 7.6614 - val_loss: 33.5072 - val_time_to_eruption_loss: 7.2032 - val_quantile_loss: 3.5763 - val_time_to_eruption_mae: 7.2032 - val_quantile_mae: 7.4758\n",
      "Epoch 18/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 34.0184 - time_to_eruption_loss: 7.3262 - quantile_loss: 3.5968 - time_to_eruption_mae: 7.3262 - quantile_mae: 7.4708 - val_loss: 30.7083 - val_time_to_eruption_loss: 6.6042 - val_quantile_loss: 3.1737 - val_time_to_eruption_mae: 6.6042 - val_quantile_mae: 6.6102\n",
      "Epoch 19/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 33.5438 - time_to_eruption_loss: 7.2201 - quantile_loss: 3.5433 - time_to_eruption_mae: 7.2201 - quantile_mae: 7.3710 - val_loss: 26.2281 - val_time_to_eruption_loss: 5.5858 - val_quantile_loss: 2.7609 - val_time_to_eruption_mae: 5.5858 - val_quantile_mae: 5.8408\n",
      "Epoch 20/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 33.1020 - time_to_eruption_loss: 7.1209 - quantile_loss: 3.4973 - time_to_eruption_mae: 7.1209 - quantile_mae: 7.2662 - val_loss: 34.6391 - val_time_to_eruption_loss: 7.4510 - val_quantile_loss: 3.7153 - val_time_to_eruption_mae: 7.4510 - val_quantile_mae: 7.6441\n",
      "Epoch 21/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 31.6644 - time_to_eruption_loss: 6.8057 - quantile_loss: 3.3282 - time_to_eruption_mae: 6.8057 - quantile_mae: 6.9322 - val_loss: 25.2234 - val_time_to_eruption_loss: 5.3633 - val_quantile_loss: 2.6628 - val_time_to_eruption_mae: 5.3633 - val_quantile_mae: 5.5308\n",
      "Epoch 22/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 30.9652 - time_to_eruption_loss: 6.6505 - quantile_loss: 3.2625 - time_to_eruption_mae: 6.6505 - quantile_mae: 6.7824 - val_loss: 33.9908 - val_time_to_eruption_loss: 7.3117 - val_quantile_loss: 3.6487 - val_time_to_eruption_mae: 7.3117 - val_quantile_mae: 7.6558\n",
      "Epoch 23/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 31.2790 - time_to_eruption_loss: 6.7223 - quantile_loss: 3.2974 - time_to_eruption_mae: 6.7223 - quantile_mae: 6.8401 - val_loss: 27.1621 - val_time_to_eruption_loss: 5.8075 - val_quantile_loss: 2.8424 - val_time_to_eruption_mae: 5.8075 - val_quantile_mae: 5.9106\n",
      "Epoch 24/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 30.3312 - time_to_eruption_loss: 6.5122 - quantile_loss: 3.1949 - time_to_eruption_mae: 6.5122 - quantile_mae: 6.6232 - val_loss: 31.4494 - val_time_to_eruption_loss: 6.7664 - val_quantile_loss: 3.3059 - val_time_to_eruption_mae: 6.7664 - val_quantile_mae: 6.8800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 30.9029 - time_to_eruption_loss: 6.6419 - quantile_loss: 3.2590 - time_to_eruption_mae: 6.6419 - quantile_mae: 6.7678 - val_loss: 24.4779 - val_time_to_eruption_loss: 5.2103 - val_quantile_loss: 2.5656 - val_time_to_eruption_mae: 5.2103 - val_quantile_mae: 5.3556\n",
      "Epoch 26/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 29.7256 - time_to_eruption_loss: 6.3838 - quantile_loss: 3.1311 - time_to_eruption_mae: 6.3838 - quantile_mae: 6.5052 - val_loss: 21.7572 - val_time_to_eruption_loss: 4.6061 - val_quantile_loss: 2.2845 - val_time_to_eruption_mae: 4.6061 - val_quantile_mae: 4.8421\n",
      "Epoch 27/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 29.0085 - time_to_eruption_loss: 6.2276 - quantile_loss: 3.0581 - time_to_eruption_mae: 6.2276 - quantile_mae: 6.3501 - val_loss: 25.0998 - val_time_to_eruption_loss: 5.3623 - val_quantile_loss: 2.6208 - val_time_to_eruption_mae: 5.3623 - val_quantile_mae: 5.5034\n",
      "Epoch 28/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 29.4370 - time_to_eruption_loss: 6.3258 - quantile_loss: 3.1044 - time_to_eruption_mae: 6.3258 - quantile_mae: 6.4531 - val_loss: 23.8082 - val_time_to_eruption_loss: 5.0722 - val_quantile_loss: 2.4899 - val_time_to_eruption_mae: 5.0722 - val_quantile_mae: 5.1763\n",
      "Epoch 29/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 27.9846 - time_to_eruption_loss: 6.0028 - quantile_loss: 2.9477 - time_to_eruption_mae: 6.0028 - quantile_mae: 6.1102 - val_loss: 29.6780 - val_time_to_eruption_loss: 6.3805 - val_quantile_loss: 3.1300 - val_time_to_eruption_mae: 6.3805 - val_quantile_mae: 6.4667\n",
      "Epoch 30/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 27.7644 - time_to_eruption_loss: 5.9548 - quantile_loss: 2.9254 - time_to_eruption_mae: 5.9548 - quantile_mae: 6.0792 - val_loss: 27.4668 - val_time_to_eruption_loss: 5.8940 - val_quantile_loss: 2.8757 - val_time_to_eruption_mae: 5.8940 - val_quantile_mae: 5.9920\n",
      "Epoch 31/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 27.1083 - time_to_eruption_loss: 5.8127 - quantile_loss: 2.8491 - time_to_eruption_mae: 5.8127 - quantile_mae: 5.9305 - val_loss: 29.6839 - val_time_to_eruption_loss: 6.3923 - val_quantile_loss: 3.1099 - val_time_to_eruption_mae: 6.3923 - val_quantile_mae: 6.5112\n",
      "Epoch 32/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 26.2572 - time_to_eruption_loss: 5.6228 - quantile_loss: 2.7627 - time_to_eruption_mae: 5.6228 - quantile_mae: 5.7328 - val_loss: 21.0722 - val_time_to_eruption_loss: 4.4640 - val_quantile_loss: 2.2152 - val_time_to_eruption_mae: 4.4640 - val_quantile_mae: 4.6736\n",
      "Epoch 33/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 26.5235 - time_to_eruption_loss: 5.6854 - quantile_loss: 2.7840 - time_to_eruption_mae: 5.6854 - quantile_mae: 5.7867 - val_loss: 17.8884 - val_time_to_eruption_loss: 3.7522 - val_quantile_loss: 1.8863 - val_time_to_eruption_mae: 3.7522 - val_quantile_mae: 4.0517\n",
      "Epoch 34/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 26.2273 - time_to_eruption_loss: 5.6203 - quantile_loss: 2.7581 - time_to_eruption_mae: 5.6203 - quantile_mae: 5.7299 - val_loss: 22.0343 - val_time_to_eruption_loss: 4.6886 - val_quantile_loss: 2.2935 - val_time_to_eruption_mae: 4.6886 - val_quantile_mae: 4.7744\n",
      "Epoch 35/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 26.1695 - time_to_eruption_loss: 5.6040 - quantile_loss: 2.7478 - time_to_eruption_mae: 5.6040 - quantile_mae: 5.7038 - val_loss: 24.0920 - val_time_to_eruption_loss: 5.1406 - val_quantile_loss: 2.5135 - val_time_to_eruption_mae: 5.1406 - val_quantile_mae: 5.2419\n",
      "Epoch 36/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 25.5073 - time_to_eruption_loss: 5.4545 - quantile_loss: 2.6780 - time_to_eruption_mae: 5.4545 - quantile_mae: 5.5668 - val_loss: 22.0318 - val_time_to_eruption_loss: 4.6814 - val_quantile_loss: 2.2987 - val_time_to_eruption_mae: 4.6814 - val_quantile_mae: 4.8195\n",
      "Epoch 37/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 25.2424 - time_to_eruption_loss: 5.3980 - quantile_loss: 2.6464 - time_to_eruption_mae: 5.3980 - quantile_mae: 5.4976 - val_loss: 23.9305 - val_time_to_eruption_loss: 5.1083 - val_quantile_loss: 2.4943 - val_time_to_eruption_mae: 5.1083 - val_quantile_mae: 5.1668\n",
      "Epoch 38/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 24.7955 - time_to_eruption_loss: 5.2985 - quantile_loss: 2.6035 - time_to_eruption_mae: 5.2985 - quantile_mae: 5.4093 - val_loss: 21.2890 - val_time_to_eruption_loss: 4.5223 - val_quantile_loss: 2.2066 - val_time_to_eruption_mae: 4.5223 - val_quantile_mae: 4.5642\n",
      "Epoch 39/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 24.4759 - time_to_eruption_loss: 5.2295 - quantile_loss: 2.5652 - time_to_eruption_mae: 5.2295 - quantile_mae: 5.3291 - val_loss: 20.2115 - val_time_to_eruption_loss: 4.2799 - val_quantile_loss: 2.0996 - val_time_to_eruption_mae: 4.2799 - val_quantile_mae: 4.4017\n",
      "Epoch 40/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 24.7225 - time_to_eruption_loss: 5.2857 - quantile_loss: 2.5897 - time_to_eruption_mae: 5.2857 - quantile_mae: 5.3797 - val_loss: 18.4835 - val_time_to_eruption_loss: 3.8917 - val_quantile_loss: 1.9211 - val_time_to_eruption_mae: 3.8917 - val_quantile_mae: 4.0672\n",
      "Epoch 41/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 23.3114 - time_to_eruption_loss: 4.9698 - quantile_loss: 2.4387 - time_to_eruption_mae: 4.9698 - quantile_mae: 5.0623 - val_loss: 21.3793 - val_time_to_eruption_loss: 4.5378 - val_quantile_loss: 2.2358 - val_time_to_eruption_mae: 4.5378 - val_quantile_mae: 4.6198\n",
      "Epoch 42/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 23.5531 - time_to_eruption_loss: 5.0263 - quantile_loss: 2.4583 - time_to_eruption_mae: 5.0263 - quantile_mae: 5.1183 - val_loss: 19.9806 - val_time_to_eruption_loss: 4.2304 - val_quantile_loss: 2.0683 - val_time_to_eruption_mae: 4.2304 - val_quantile_mae: 4.3303\n",
      "Epoch 43/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 23.5433 - time_to_eruption_loss: 5.0225 - quantile_loss: 2.4597 - time_to_eruption_mae: 5.0225 - quantile_mae: 5.1139 - val_loss: 23.5055 - val_time_to_eruption_loss: 5.0053 - val_quantile_loss: 2.4904 - val_time_to_eruption_mae: 5.0053 - val_quantile_mae: 5.1551\n",
      "Epoch 44/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 22.8859 - time_to_eruption_loss: 4.8761 - quantile_loss: 2.3902 - time_to_eruption_mae: 4.8761 - quantile_mae: 4.9591 - val_loss: 16.8113 - val_time_to_eruption_loss: 3.5219 - val_quantile_loss: 1.7341 - val_time_to_eruption_mae: 3.5219 - val_quantile_mae: 3.6291\n",
      "Epoch 45/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 22.9034 - time_to_eruption_loss: 4.8813 - quantile_loss: 2.3891 - time_to_eruption_mae: 4.8813 - quantile_mae: 4.9656 - val_loss: 21.4979 - val_time_to_eruption_loss: 4.5688 - val_quantile_loss: 2.2317 - val_time_to_eruption_mae: 4.5688 - val_quantile_mae: 4.6566\n",
      "Epoch 46/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 23.6412 - time_to_eruption_loss: 5.0442 - quantile_loss: 2.4697 - time_to_eruption_mae: 5.0442 - quantile_mae: 5.1387 - val_loss: 19.4381 - val_time_to_eruption_loss: 4.1013 - val_quantile_loss: 2.0367 - val_time_to_eruption_mae: 4.1013 - val_quantile_mae: 4.2661\n",
      "Epoch 47/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 22.8952 - time_to_eruption_loss: 4.8780 - quantile_loss: 2.3885 - time_to_eruption_mae: 4.8780 - quantile_mae: 4.9733 - val_loss: 19.9911 - val_time_to_eruption_loss: 4.2300 - val_quantile_loss: 2.0770 - val_time_to_eruption_mae: 4.2300 - val_quantile_mae: 4.3388\n",
      "Epoch 48/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 22.4280 - time_to_eruption_loss: 4.7745 - quantile_loss: 2.3361 - time_to_eruption_mae: 4.7745 - quantile_mae: 4.8658 - val_loss: 21.2277 - val_time_to_eruption_loss: 4.5046 - val_quantile_loss: 2.2145 - val_time_to_eruption_mae: 4.5046 - val_quantile_mae: 4.5804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 22.3286 - time_to_eruption_loss: 4.7496 - quantile_loss: 2.3316 - time_to_eruption_mae: 4.7496 - quantile_mae: 4.8449 - val_loss: 18.6314 - val_time_to_eruption_loss: 3.9270 - val_quantile_loss: 1.9227 - val_time_to_eruption_mae: 3.9270 - val_quantile_mae: 4.0170\n",
      "Epoch 50/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 21.5363 - time_to_eruption_loss: 4.5727 - quantile_loss: 2.2432 - time_to_eruption_mae: 4.5727 - quantile_mae: 4.6595 - val_loss: 16.1785 - val_time_to_eruption_loss: 3.3779 - val_quantile_loss: 1.6620 - val_time_to_eruption_mae: 3.3779 - val_quantile_mae: 3.4911\n",
      "Epoch 51/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 21.9912 - time_to_eruption_loss: 4.6751 - quantile_loss: 2.2875 - time_to_eruption_mae: 4.6751 - quantile_mae: 4.7670 - val_loss: 18.0125 - val_time_to_eruption_loss: 3.7862 - val_quantile_loss: 1.8666 - val_time_to_eruption_mae: 3.7862 - val_quantile_mae: 3.9315\n",
      "Epoch 52/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 20.9891 - time_to_eruption_loss: 4.4515 - quantile_loss: 2.1839 - time_to_eruption_mae: 4.4515 - quantile_mae: 4.5415 - val_loss: 19.5658 - val_time_to_eruption_loss: 4.1455 - val_quantile_loss: 1.9847 - val_time_to_eruption_mae: 4.1455 - val_quantile_mae: 4.1693\n",
      "Epoch 53/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 21.4483 - time_to_eruption_loss: 4.5546 - quantile_loss: 2.2334 - time_to_eruption_mae: 4.5546 - quantile_mae: 4.6394 - val_loss: 20.6673 - val_time_to_eruption_loss: 4.3834 - val_quantile_loss: 2.1389 - val_time_to_eruption_mae: 4.3834 - val_quantile_mae: 4.4255\n",
      "Epoch 54/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 21.4772 - time_to_eruption_loss: 4.5604 - quantile_loss: 2.2387 - time_to_eruption_mae: 4.5604 - quantile_mae: 4.6431 - val_loss: 16.5335 - val_time_to_eruption_loss: 3.4575 - val_quantile_loss: 1.7041 - val_time_to_eruption_mae: 3.4575 - val_quantile_mae: 3.5497\n",
      "Epoch 55/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 20.6903 - time_to_eruption_loss: 4.3855 - quantile_loss: 2.1501 - time_to_eruption_mae: 4.3855 - quantile_mae: 4.4635 - val_loss: 16.6912 - val_time_to_eruption_loss: 3.4910 - val_quantile_loss: 1.7284 - val_time_to_eruption_mae: 3.4910 - val_quantile_mae: 3.5990\n",
      "Epoch 56/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 21.4053 - time_to_eruption_loss: 4.5445 - quantile_loss: 2.2252 - time_to_eruption_mae: 4.5445 - quantile_mae: 4.6307 - val_loss: 16.2256 - val_time_to_eruption_loss: 3.3885 - val_quantile_loss: 1.6682 - val_time_to_eruption_mae: 3.3885 - val_quantile_mae: 3.5115\n",
      "Epoch 57/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 20.5750 - time_to_eruption_loss: 4.3597 - quantile_loss: 2.1330 - time_to_eruption_mae: 4.3597 - quantile_mae: 4.4381 - val_loss: 16.1793 - val_time_to_eruption_loss: 3.3764 - val_quantile_loss: 1.6719 - val_time_to_eruption_mae: 3.3764 - val_quantile_mae: 3.5235\n",
      "Epoch 58/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 20.5611 - time_to_eruption_loss: 4.3558 - quantile_loss: 2.1344 - time_to_eruption_mae: 4.3558 - quantile_mae: 4.4443 - val_loss: 24.2962 - val_time_to_eruption_loss: 5.1724 - val_quantile_loss: 2.6043 - val_time_to_eruption_mae: 5.1724 - val_quantile_mae: 5.3542\n",
      "Epoch 59/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 20.0761 - time_to_eruption_loss: 4.2470 - quantile_loss: 2.0857 - time_to_eruption_mae: 4.2470 - quantile_mae: 4.3247 - val_loss: 24.5035 - val_time_to_eruption_loss: 5.2341 - val_quantile_loss: 2.5633 - val_time_to_eruption_mae: 5.2341 - val_quantile_mae: 5.2828\n",
      "Epoch 60/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 20.3968 - time_to_eruption_loss: 4.3183 - quantile_loss: 2.1212 - time_to_eruption_mae: 4.3183 - quantile_mae: 4.4010 - val_loss: 21.3412 - val_time_to_eruption_loss: 4.5294 - val_quantile_loss: 2.2218 - val_time_to_eruption_mae: 4.5294 - val_quantile_mae: 4.5644\n",
      "Epoch 61/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.5010 - time_to_eruption_loss: 4.1205 - quantile_loss: 2.0191 - time_to_eruption_mae: 4.1205 - quantile_mae: 4.1947 - val_loss: 17.1548 - val_time_to_eruption_loss: 3.6065 - val_quantile_loss: 1.7307 - val_time_to_eruption_mae: 3.6065 - val_quantile_mae: 3.6431\n",
      "Epoch 62/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.5906 - time_to_eruption_loss: 4.1390 - quantile_loss: 2.0373 - time_to_eruption_mae: 4.1390 - quantile_mae: 4.2258 - val_loss: 15.9012 - val_time_to_eruption_loss: 3.3142 - val_quantile_loss: 1.6471 - val_time_to_eruption_mae: 3.3142 - val_quantile_mae: 3.4602\n",
      "Epoch 63/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.9568 - time_to_eruption_loss: 4.2203 - quantile_loss: 2.0778 - time_to_eruption_mae: 4.2203 - quantile_mae: 4.3202 - val_loss: 18.2991 - val_time_to_eruption_loss: 3.8574 - val_quantile_loss: 1.8726 - val_time_to_eruption_mae: 3.8574 - val_quantile_mae: 3.8836\n",
      "Epoch 64/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.3681 - time_to_eruption_loss: 4.0912 - quantile_loss: 2.0063 - time_to_eruption_mae: 4.0912 - quantile_mae: 4.1743 - val_loss: 14.9640 - val_time_to_eruption_loss: 3.1130 - val_quantile_loss: 1.5166 - val_time_to_eruption_mae: 3.1130 - val_quantile_mae: 3.1525\n",
      "Epoch 65/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.3719 - time_to_eruption_loss: 4.0917 - quantile_loss: 2.0084 - time_to_eruption_mae: 4.0917 - quantile_mae: 4.1747 - val_loss: 15.9135 - val_time_to_eruption_loss: 3.3206 - val_quantile_loss: 1.6341 - val_time_to_eruption_mae: 3.3206 - val_quantile_mae: 3.4272\n",
      "Epoch 66/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.2501 - time_to_eruption_loss: 4.0646 - quantile_loss: 1.9959 - time_to_eruption_mae: 4.0646 - quantile_mae: 4.1430 - val_loss: 21.9684 - val_time_to_eruption_loss: 4.6864 - val_quantile_loss: 2.2269 - val_time_to_eruption_mae: 4.6864 - val_quantile_mae: 4.5766\n",
      "Epoch 67/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 18.9176 - time_to_eruption_loss: 3.9914 - quantile_loss: 1.9574 - time_to_eruption_mae: 3.9914 - quantile_mae: 4.0673 - val_loss: 16.3894 - val_time_to_eruption_loss: 3.4311 - val_quantile_loss: 1.6708 - val_time_to_eruption_mae: 3.4311 - val_quantile_mae: 3.4912\n",
      "Epoch 68/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 18.7079 - time_to_eruption_loss: 3.9456 - quantile_loss: 1.9327 - time_to_eruption_mae: 3.9456 - quantile_mae: 4.0161 - val_loss: 18.6719 - val_time_to_eruption_loss: 3.9867 - val_quantile_loss: 1.7321 - val_time_to_eruption_mae: 3.9867 - val_quantile_mae: 3.6625\n",
      "Epoch 69/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 18.9867 - time_to_eruption_loss: 4.0083 - quantile_loss: 1.9611 - time_to_eruption_mae: 4.0083 - quantile_mae: 4.0804 - val_loss: 14.4148 - val_time_to_eruption_loss: 2.9898 - val_quantile_loss: 1.4653 - val_time_to_eruption_mae: 2.9898 - val_quantile_mae: 3.0844\n",
      "Epoch 70/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 18.7624 - time_to_eruption_loss: 3.9573 - quantile_loss: 1.9427 - time_to_eruption_mae: 3.9573 - quantile_mae: 4.0325 - val_loss: 15.6896 - val_time_to_eruption_loss: 3.2756 - val_quantile_loss: 1.5923 - val_time_to_eruption_mae: 3.2756 - val_quantile_mae: 3.3261\n",
      "Epoch 71/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 18.5318 - time_to_eruption_loss: 3.9055 - quantile_loss: 1.9164 - time_to_eruption_mae: 3.9055 - quantile_mae: 3.9908 - val_loss: 14.3669 - val_time_to_eruption_loss: 2.9757 - val_quantile_loss: 1.4738 - val_time_to_eruption_mae: 2.9757 - val_quantile_mae: 3.1299\n",
      "Epoch 72/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 19.1355 - time_to_eruption_loss: 4.0403 - quantile_loss: 1.9842 - time_to_eruption_mae: 4.0403 - quantile_mae: 4.1322 - val_loss: 13.7079 - val_time_to_eruption_loss: 2.8357 - val_quantile_loss: 1.3769 - val_time_to_eruption_mae: 2.8357 - val_quantile_mae: 2.8982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 18.5851 - time_to_eruption_loss: 3.9196 - quantile_loss: 1.9186 - time_to_eruption_mae: 3.9196 - quantile_mae: 3.9971 - val_loss: 15.1930 - val_time_to_eruption_loss: 3.1696 - val_quantile_loss: 1.5252 - val_time_to_eruption_mae: 3.1696 - val_quantile_mae: 3.1926\n",
      "Epoch 74/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 18.3079 - time_to_eruption_loss: 3.8578 - quantile_loss: 1.8878 - time_to_eruption_mae: 3.8578 - quantile_mae: 3.9307 - val_loss: 14.3822 - val_time_to_eruption_loss: 2.9732 - val_quantile_loss: 1.5013 - val_time_to_eruption_mae: 2.9732 - val_quantile_mae: 3.2159\n",
      "Epoch 75/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 18.3288 - time_to_eruption_loss: 3.8624 - quantile_loss: 1.8924 - time_to_eruption_mae: 3.8624 - quantile_mae: 3.9429 - val_loss: 13.6212 - val_time_to_eruption_loss: 2.8126 - val_quantile_loss: 1.3855 - val_time_to_eruption_mae: 2.8126 - val_quantile_mae: 2.9178\n",
      "Epoch 76/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.8052 - time_to_eruption_loss: 3.7466 - quantile_loss: 1.8361 - time_to_eruption_mae: 3.7466 - quantile_mae: 3.8204 - val_loss: 15.0221 - val_time_to_eruption_loss: 3.1227 - val_quantile_loss: 1.5505 - val_time_to_eruption_mae: 3.1227 - val_quantile_mae: 3.2781\n",
      "Epoch 77/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.7556 - time_to_eruption_loss: 3.7368 - quantile_loss: 1.8292 - time_to_eruption_mae: 3.7368 - quantile_mae: 3.8084 - val_loss: 19.2641 - val_time_to_eruption_loss: 4.0723 - val_quantile_loss: 1.9965 - val_time_to_eruption_mae: 4.0723 - val_quantile_mae: 4.1255\n",
      "Epoch 78/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.9330 - time_to_eruption_loss: 3.7762 - quantile_loss: 1.8506 - time_to_eruption_mae: 3.7762 - quantile_mae: 3.8538 - val_loss: 17.8646 - val_time_to_eruption_loss: 3.7557 - val_quantile_loss: 1.8658 - val_time_to_eruption_mae: 3.7557 - val_quantile_mae: 3.8682\n",
      "Epoch 79/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.7328 - time_to_eruption_loss: 3.7321 - quantile_loss: 1.8293 - time_to_eruption_mae: 3.7321 - quantile_mae: 3.8086 - val_loss: 16.9825 - val_time_to_eruption_loss: 3.5653 - val_quantile_loss: 1.7467 - val_time_to_eruption_mae: 3.5653 - val_quantile_mae: 3.6257\n",
      "Epoch 80/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.9559 - time_to_eruption_loss: 3.7821 - quantile_loss: 1.8509 - time_to_eruption_mae: 3.7821 - quantile_mae: 3.8543 - val_loss: 13.0092 - val_time_to_eruption_loss: 2.6782 - val_quantile_loss: 1.3168 - val_time_to_eruption_mae: 2.6782 - val_quantile_mae: 2.7640\n",
      "Epoch 81/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.8522 - time_to_eruption_loss: 3.7578 - quantile_loss: 1.8432 - time_to_eruption_mae: 3.7578 - quantile_mae: 3.8291 - val_loss: 13.8622 - val_time_to_eruption_loss: 2.8682 - val_quantile_loss: 1.4136 - val_time_to_eruption_mae: 2.8682 - val_quantile_mae: 2.9733\n",
      "Epoch 82/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.2081 - time_to_eruption_loss: 3.6156 - quantile_loss: 1.7706 - time_to_eruption_mae: 3.6156 - quantile_mae: 3.6867 - val_loss: 13.9086 - val_time_to_eruption_loss: 2.8779 - val_quantile_loss: 1.4212 - val_time_to_eruption_mae: 2.8779 - val_quantile_mae: 2.9808\n",
      "Epoch 83/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.4769 - time_to_eruption_loss: 3.6742 - quantile_loss: 1.8021 - time_to_eruption_mae: 3.6742 - quantile_mae: 3.7451 - val_loss: 13.1207 - val_time_to_eruption_loss: 2.7016 - val_quantile_loss: 1.3368 - val_time_to_eruption_mae: 2.7016 - val_quantile_mae: 2.8199\n",
      "Epoch 84/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.9762 - time_to_eruption_loss: 3.7857 - quantile_loss: 1.8563 - time_to_eruption_mae: 3.7857 - quantile_mae: 3.8616 - val_loss: 12.0041 - val_time_to_eruption_loss: 2.4502 - val_quantile_loss: 1.2267 - val_time_to_eruption_mae: 2.4502 - val_quantile_mae: 2.6107\n",
      "Epoch 85/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.9778 - time_to_eruption_loss: 3.5635 - quantile_loss: 1.7477 - time_to_eruption_mae: 3.5635 - quantile_mae: 3.6318 - val_loss: 14.4971 - val_time_to_eruption_loss: 3.0034 - val_quantile_loss: 1.5086 - val_time_to_eruption_mae: 3.0034 - val_quantile_mae: 3.1512\n",
      "Epoch 86/100\n",
      "444/444 [==============================] - 139s 313ms/step - loss: 17.1690 - time_to_eruption_loss: 3.6067 - quantile_loss: 1.7682 - time_to_eruption_mae: 3.6067 - quantile_mae: 3.6762 - val_loss: 11.7420 - val_time_to_eruption_loss: 2.3969 - val_quantile_loss: 1.1807 - val_time_to_eruption_mae: 2.3969 - val_quantile_mae: 2.4926\n",
      "Epoch 87/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 17.0396 - time_to_eruption_loss: 3.5779 - quantile_loss: 1.7550 - time_to_eruption_mae: 3.5779 - quantile_mae: 3.6450 - val_loss: 12.8246 - val_time_to_eruption_loss: 2.6377 - val_quantile_loss: 1.3001 - val_time_to_eruption_mae: 2.6377 - val_quantile_mae: 2.7424\n",
      "Epoch 88/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.6717 - time_to_eruption_loss: 3.4957 - quantile_loss: 1.7161 - time_to_eruption_mae: 3.4957 - quantile_mae: 3.5670 - val_loss: 13.8973 - val_time_to_eruption_loss: 2.8846 - val_quantile_loss: 1.3864 - val_time_to_eruption_mae: 2.8846 - val_quantile_mae: 2.9097\n",
      "Epoch 89/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.6682 - time_to_eruption_loss: 3.4961 - quantile_loss: 1.7135 - time_to_eruption_mae: 3.4961 - quantile_mae: 3.5660 - val_loss: 21.1806 - val_time_to_eruption_loss: 4.4941 - val_quantile_loss: 2.2360 - val_time_to_eruption_mae: 4.4941 - val_quantile_mae: 4.5694\n",
      "Epoch 90/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.4374 - time_to_eruption_loss: 3.4442 - quantile_loss: 1.6927 - time_to_eruption_mae: 3.4442 - quantile_mae: 3.5205 - val_loss: 13.8225 - val_time_to_eruption_loss: 2.8671 - val_quantile_loss: 1.3858 - val_time_to_eruption_mae: 2.8671 - val_quantile_mae: 2.8942\n",
      "Epoch 91/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.9314 - time_to_eruption_loss: 3.5549 - quantile_loss: 1.7436 - time_to_eruption_mae: 3.5549 - quantile_mae: 3.6297 - val_loss: 13.6534 - val_time_to_eruption_loss: 2.8216 - val_quantile_loss: 1.3988 - val_time_to_eruption_mae: 2.8216 - val_quantile_mae: 2.9539\n",
      "Epoch 92/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.4293 - time_to_eruption_loss: 3.4438 - quantile_loss: 1.6868 - time_to_eruption_mae: 3.4438 - quantile_mae: 3.5178 - val_loss: 11.4954 - val_time_to_eruption_loss: 2.3451 - val_quantile_loss: 1.1476 - val_time_to_eruption_mae: 2.3451 - val_quantile_mae: 2.4352\n",
      "Epoch 93/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.1573 - time_to_eruption_loss: 3.3829 - quantile_loss: 1.6600 - time_to_eruption_mae: 3.3829 - quantile_mae: 3.4556 - val_loss: 13.7691 - val_time_to_eruption_loss: 2.8478 - val_quantile_loss: 1.4126 - val_time_to_eruption_mae: 2.8478 - val_quantile_mae: 2.9348\n",
      "Epoch 94/100\n",
      "444/444 [==============================] - 139s 312ms/step - loss: 16.7918 - time_to_eruption_loss: 3.5254 - quantile_loss: 1.7264 - time_to_eruption_mae: 3.5254 - quantile_mae: 3.6008 - val_loss: 13.7777 - val_time_to_eruption_loss: 2.8529 - val_quantile_loss: 1.4034 - val_time_to_eruption_mae: 2.8529 - val_quantile_mae: 2.9343\n",
      "Epoch 95/100\n",
      "444/444 [==============================] - 141s 318ms/step - loss: 16.4067 - time_to_eruption_loss: 3.4400 - quantile_loss: 1.6843 - time_to_eruption_mae: 3.4400 - quantile_mae: 3.5059 - val_loss: 13.5780 - val_time_to_eruption_loss: 2.8023 - val_quantile_loss: 1.4058 - val_time_to_eruption_mae: 2.8023 - val_quantile_mae: 2.9483\n",
      "Epoch 96/100\n",
      "444/444 [==============================] - 140s 315ms/step - loss: 16.1628 - time_to_eruption_loss: 3.3856 - quantile_loss: 1.6580 - time_to_eruption_mae: 3.3856 - quantile_mae: 3.4514 - val_loss: 23.7150 - val_time_to_eruption_loss: 5.0668 - val_quantile_loss: 2.4860 - val_time_to_eruption_mae: 5.0668 - val_quantile_mae: 5.0933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "444/444 [==============================] - 140s 315ms/step - loss: 16.2349 - time_to_eruption_loss: 3.4014 - quantile_loss: 1.6662 - time_to_eruption_mae: 3.4014 - quantile_mae: 3.4733 - val_loss: 13.5300 - val_time_to_eruption_loss: 2.7987 - val_quantile_loss: 1.3715 - val_time_to_eruption_mae: 2.7987 - val_quantile_mae: 2.8758\n",
      "Epoch 98/100\n",
      "444/444 [==============================] - 140s 315ms/step - loss: 16.1956 - time_to_eruption_loss: 3.3925 - quantile_loss: 1.6614 - time_to_eruption_mae: 3.3925 - quantile_mae: 3.4640 - val_loss: 11.9094 - val_time_to_eruption_loss: 2.4361 - val_quantile_loss: 1.2002 - val_time_to_eruption_mae: 2.4361 - val_quantile_mae: 2.5238\n",
      "Epoch 99/100\n",
      "444/444 [==============================] - 140s 315ms/step - loss: 16.0599 - time_to_eruption_loss: 3.3618 - quantile_loss: 1.6482 - time_to_eruption_mae: 3.3618 - quantile_mae: 3.4263 - val_loss: 10.4977 - val_time_to_eruption_loss: 2.1216 - val_quantile_loss: 1.0467 - val_time_to_eruption_mae: 2.1216 - val_quantile_mae: 2.2101\n",
      "Epoch 100/100\n",
      "444/444 [==============================] - 140s 316ms/step - loss: 16.2431 - time_to_eruption_loss: 3.4033 - quantile_loss: 1.6656 - time_to_eruption_mae: 3.4033 - quantile_mae: 3.4764 - val_loss: 10.8531 - val_time_to_eruption_loss: 2.1982 - val_quantile_loss: 1.0958 - val_time_to_eruption_mae: 2.1982 - val_quantile_mae: 2.3417\n",
      "Restoring model weights from the end of the best epoch.\n",
      " 2/28 [=>............................] - ETA: 1sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0209s vs `on_predict_batch_end` time: 0.2024s). Check your callbacks.\n",
      "28/28 [==============================] - 6s 216ms/step\n",
      "INFO:tensorflow:Assets written to: ./models/model_cnn2d_Big_4\\assets\n",
      "************************************************************\n",
      "Prediction MAE: 2121576.139721158\n",
      "************************************************************\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 7. Training\n",
    "\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "callback_early_stopping = ReturnBestEarlyStopping(monitor='val_time_to_eruption_loss', patience=15, \n",
    "                                                  verbose=1, restore_best_weights=True)\n",
    "callback_lrsched = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "list_callbacks = [callback_early_stopping, callback_lrsched]\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "list_history, list_models = [], []\n",
    "\n",
    "for num_fold, (train_index, val_index) in enumerate(kf.split(list_segments_train,\n",
    "                                                             np.zeros(len(list_segments_train)))):\n",
    "    \n",
    "    segments_train_fold = np.asarray(list_segments_train)[train_index]\n",
    "    segments_val_fold = np.asarray(list_segments_train)[val_index]\n",
    "\n",
    "    X_train_generator = VolcanoSequencesGenerator(segments_train_fold, dict_segments_spectograms_paths_train,\n",
    "                                                  batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                  transforms=getTrainTransforms(), training=True)\n",
    "\n",
    "    X_val_generator = VolcanoSequencesGenerator(segments_val_fold, dict_segments_spectograms_paths_train,\n",
    "                                                 batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                transforms=None, training=True)\n",
    "\n",
    "    print(f'Num Fold: {num_fold + 1}')\n",
    "    print(f'Train segments: {len(train_index)} Val segments: {len(val_index)}')\n",
    "\n",
    "    model = buildModel(size=118, summary=True)\n",
    "\n",
    "    history = model.fit(X_train_generator,\n",
    "                        validation_data=X_val_generator,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=list_callbacks,\n",
    "                        epochs=100,\n",
    "                        verbose=1)\n",
    "\n",
    "    list_history.append(history)\n",
    "    list_models.append(model)\n",
    "\n",
    "    X_val_batch, y_val_target = X_val_generator.generateOrderedSequences(list(segments_val_fold))\n",
    "    y_pred_val = model.predict(X_val_batch, verbose=1)\n",
    "    y_pred_val[0], y_pred_val[1] = y_pred_val[0]*(10**6), y_pred_val[1]*(10**6)\n",
    "    y_val_target = y_val_target*(10**6)\n",
    "    mae = np.abs(y_val_target - y_pred_val[0])\n",
    "\n",
    "    model.save(f'./models/model_cnn2d_Big_{num_fold}')\n",
    "\n",
    "    print('***'*20)\n",
    "    print(f'Prediction MAE: {mae.mean()}')\n",
    "    print('***'*20)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [03:35<00:00, 43.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "2121575.7414598744\n",
      "2122551.3861439275\n",
      "************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 8. Cross Val Score\n",
    "\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "batch_size = 4\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "df_val_all = pd.DataFrame()\n",
    "\n",
    "for num_fold, (train_index, val_index) in tqdm(enumerate(kf.split(list_segments_train,\n",
    "                                                             np.zeros(len(list_segments_train)))), \n",
    "                                               total=5, position=0):\n",
    "    \n",
    "    \n",
    "    segments_train_fold = np.asarray(list_segments_train)[train_index]\n",
    "    segments_val_fold = np.asarray(list_segments_train)[val_index]\n",
    "\n",
    "    X_val_generator = VolcanoSequencesGenerator(segments_val_fold, dict_segments_spectograms_paths_train,\n",
    "                                                batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                transforms=None, training=True)   \n",
    "\n",
    "    model = models.load_model(f'./models/model_cnn2d_Big_{num_fold}', compile=False)\n",
    "\n",
    "    X_val_sequences, y_val_target = X_val_generator.generateOrderedSequences(segments_val_fold)\n",
    "    y_pred_val = model.predict(X_val_sequences)\n",
    "\n",
    "    df_tmp = pd.DataFrame({\n",
    "            'pred' :  y_pred_val[0].squeeze()*(10**6),\n",
    "            'pred_q_30' : y_pred_val[1][:, 0]*(10**6),\n",
    "            'pred_q_50' : y_pred_val[1][:, 1]*(10**6),\n",
    "            'pred_q_70' : y_pred_val[1][:, 2]*(10**6),\n",
    "            'y_true' : y_val_target.flatten()*(10**6)\n",
    "    })\n",
    "\n",
    "    df_val_all = pd.concat([df_val_all, df_tmp], axis=0)\n",
    "\n",
    "print('***'*20)\n",
    "print(np.mean(np.abs(df_tmp['y_true'] - df_tmp['pred'])))\n",
    "print(np.mean(np.abs(df_tmp['y_true'] - df_tmp['pred_q_50'])))\n",
    "print('***'*20)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/565 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637BA6798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637B8B168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637B95B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637B9A948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637B9F828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637BA65E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637B9F948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637B9ACA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637B9A048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028637B953A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E27048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E2C558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E278B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E1BC18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6DF9E58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E06EE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E10CA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E15B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E15678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000286B6E109D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DA4F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DC7948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DCC828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DD13A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DCC708> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DC7A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DBA288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DA4A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DB2948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x0000028735DBA708> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE15B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE2B0D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE2F1F8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE36288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE36E58> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE36F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE36318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE2F678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE2B9D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000287BAE21D38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE6B0D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE715E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE6B948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE5ECA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE3DEE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE4AF78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE54DC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE57C18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE57708> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002883BE54A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 565/565 [06:59<00:00,  1.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>time_to_eruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.520000e+03</td>\n",
       "      <td>4.520000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.066993e+09</td>\n",
       "      <td>2.577856e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.162904e+08</td>\n",
       "      <td>1.201171e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.602880e+05</td>\n",
       "      <td>8.140305e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.458995e+08</td>\n",
       "      <td>1.658928e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.060695e+09</td>\n",
       "      <td>2.778482e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.599284e+09</td>\n",
       "      <td>3.532871e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147116e+09</td>\n",
       "      <td>5.317459e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         segment_id  time_to_eruption\n",
       "count  4.520000e+03      4.520000e+03\n",
       "mean   1.066993e+09      2.577856e+07\n",
       "std    6.162904e+08      1.201171e+07\n",
       "min    8.602880e+05      8.140305e+05\n",
       "25%    5.458995e+08      1.658928e+07\n",
       "50%    1.060695e+09      2.778482e+07\n",
       "75%    1.599284e+09      3.532871e+07\n",
       "max    2.147116e+09      5.317459e+07"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################\n",
    "# 9. Inference\n",
    "\n",
    "\n",
    "# del X_val_sequences, y_val_target, list_cv_pred, y_pred_cv, y_cv_target, df_cv, X_cv_sequences, y_cv_target\n",
    "gc.collect()\n",
    "\n",
    "list_models = [models.load_model(f'./models/model_cnn2d_Big_{i}', compile=False) for i in range(5)]\n",
    "\n",
    "X_test_generator = VolcanoSequencesGenerator(list(unique_segments_id_test), dict_segments_spectograms_paths_test,\n",
    "                                             batch_size=8, dict_labels=dict_labels, \n",
    "                                             transforms=None, training=False)\n",
    "\n",
    "batch_size_prediction = 8\n",
    "idx = 0\n",
    "num_test_steps = int(np.ceil(len(unique_segments_id_test) / batch_size_prediction))\n",
    "list_test_segments = list(unique_segments_id_test)\n",
    "array_predictions = np.zeros((len(list_test_segments)))\n",
    "\n",
    "for i in tqdm(range(num_test_steps), total=num_test_steps, position=0):\n",
    "    list_tmp_segments = list_test_segments[idx:(idx+batch_size_prediction)]\n",
    "    X_test_sequences = X_test_generator.generateOrderedSequences(list_tmp_segments)\n",
    "     \n",
    "    predictions = [model(X_test_sequences)[0].numpy().squeeze() for model in list_models]\n",
    "    predictions = np.mean(np.asarray(predictions), axis=0)*(10**6)\n",
    "    array_predictions[idx:(idx+batch_size_prediction)] = predictions\n",
    "    idx += batch_size_prediction   \n",
    "\n",
    "df_submission = pd.DataFrame({\n",
    "    'segment_id' : list_test_segments,\n",
    "    'time_to_eruption' : array_predictions\n",
    "})\n",
    "\n",
    "df_submission.to_csv('./FinalSubmissions/' + 'submission_big_cnn.csv', index=False)\n",
    "df_submission.describe()\n",
    "\n",
    "#############################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
