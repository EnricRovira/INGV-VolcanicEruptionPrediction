{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 1. Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, train_test_split\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers, metrics, losses, optimizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import librosa\n",
    "import scipy.signal\n",
    "import albumentations\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 2. Paths & Global Variables\n",
    "\n",
    "## 2.1 Paths\n",
    "\n",
    "path = '../../../01_Data/'\n",
    "path_sequences = path + '01_GeneratedSequences/'\n",
    "path_spectograms_tiny = path + '02_GeneratedSpectograms_MelFeatures/'\n",
    "path_spectograms_big = path + '03_GeneratedSpectograms_Scipy/'\n",
    "path_spectograms_stft = path + '04_GeneratedSpectogramsSTFT/'\n",
    "\n",
    "path_models_mha = [f'../models/model_mha_{num_fold}' for num_fold in range(5)]\n",
    "path_models_spectogram_tiny = [f'../models/model_cnn2d_Tiny_{num_fold}' for num_fold in range(5)]\n",
    "path_models_spectogram_big = [f'../models/model_cnn2d_Big_{num_fold}' for num_fold in range(5)]\n",
    "path_models_spectogram_stft = [f'../models/model_cnn2d_STFT_{num_fold}' for num_fold in range(5)]\n",
    "path_models_lgbm_stft = [f'../models/model_lgbm_STFT_{num_fold}' for num_fold in range(5)]\n",
    "\n",
    "df_train = pd.read_csv(path + 'train.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv') \n",
    "\n",
    "train_paths = glob.glob(path + 'train/*')\n",
    "test_paths = glob.glob(path + 'test/*')\n",
    "\n",
    "unique_segments_id_train = set(df_train['segment_id'])\n",
    "unique_segments_id_test = set(df_sample_submission['segment_id'])\n",
    "\n",
    "dict_unique_segments_id = { v : k for k, v in enumerate(unique_segments_id_train)}\n",
    "dict_unique_segments_id_inv = { k : v for k, v in enumerate(unique_segments_id_train)}\n",
    "\n",
    "## 2.2 Global Variables\n",
    "\n",
    "SEQ_LENGTH = 60_001\n",
    "\n",
    "IMG_SIZE_TINY = (40, 118)\n",
    "IMG_SIZE_BIG = (128, 235)\n",
    "IMG_SIZE_STFT = (128, 469)\n",
    "\n",
    "# SAMPLING_RATE = 100 \n",
    "# DURATION = 600\n",
    "# SAMPLES = SAMPLING_RATE * DURATION\n",
    "# N_MELS = 128\n",
    "# HOP_LENGTH = 256 # 256\n",
    "# FMIN = 0\n",
    "# FMAX = SAMPLING_RATE // 2\n",
    "# N_FFT = N_MELS * 20 #2048\n",
    "\n",
    "# N_MFCC = 40\n",
    "# MAX_PAD_LENGTH = 118\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 3. Global Functions\n",
    "\n",
    "def buildSequences(df, dict_segment_paths, training=True, mask_value=-1.0):\n",
    "    x = np.zeros((len(dict_segment_paths), SEQ_LENGTH, 10))\n",
    "    if training:\n",
    "        y = np.zeros(len(dict_segment_paths))\n",
    "    for i, segment in enumerate(tqdm(dict_segment_paths, total=len(dict_segment_paths), position=0)):\n",
    "        segment_path = dict_segment_paths[segment]\n",
    "        df_tmp = pd.read_csv(segment_path)\n",
    "        df_tmp = df_tmp.fillna(mask_value)\n",
    "        x[i] = df_tmp.values[-SEQ_LENGTH:]\n",
    "        if training:\n",
    "            y[i] = df['time_to_eruption'][df['segment_id']==segment].values[0]\n",
    "    if training:\n",
    "        return x, y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def scale(x, mean_, std_):\n",
    "    return (x - mean_) / std_\n",
    "\n",
    "\n",
    "def unscale(x, mean_, std_):\n",
    "    return (x * std_) + mean_\n",
    "\n",
    "\n",
    "def monoToColor(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "#     X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Scale to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "\n",
    "# # 5.2 Augmentations\n",
    "\n",
    "# def noiseInjection(batch_sequences, noise_factor=0.075):\n",
    "#     noise = np.random.randn(batch_sequences.shape[0], batch_sequences.shape[1], batch_sequences.shape[2])\n",
    "#     augmented_data = batch_sequences + noise_factor * noise\n",
    "#     return augmented_data\n",
    "\n",
    "\n",
    "# def timeShifting(batch_sequences, shift_max):\n",
    "#     shift = np.random.randint(shift_max)\n",
    "#     for sensor in range(10):\n",
    "#         batch_sequences[:, :, sensor] = np.roll(batch_sequences[:, :, sensor], shift)\n",
    "#     return batch_sequences\n",
    "\n",
    "# def makeAugmentations(list_segments, dict_path_data, seq_length):\n",
    "    \n",
    "#     batch_sequences = np.asarray([np.load(dict_path_data[segment]) for segment in list_segments])\n",
    "    \n",
    "#     list_augmentations = [0, 1, 2, 3]\n",
    "#     current_augmentations = list(np.random.choice(list_augmentations, size=np.random.randint(1, 4) ,replace=False))\n",
    "      \n",
    "#     # Add random noise\n",
    "#     if 0 in current_augmentations:\n",
    "#         batch_sequences = noiseInjection(batch_sequences, noise_factor=0.05)   \n",
    "\n",
    "#     # Time shifting\n",
    "#     if 1 in current_augmentations:\n",
    "#         batch_sequences = timeShifting(batch_sequences, shift_max=6_000) \n",
    "\n",
    "#     # Random batch sequence sensors slices to null\n",
    "#     if 2 in current_augmentations:\n",
    "#         num_random_sensors = np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "#         random_sensors = list(set(np.random.randint(0, 9, size=num_random_sensors)))\n",
    "#         random_ini_position = np.random.randint(0, seq_length, size=num_random_sensors)\n",
    "#         random_length = np.random.randint(random_ini_position, random_ini_position+6_000, size=num_random_sensors)\n",
    "#         random_length -= random_ini_position\n",
    "\n",
    "#         if num_random_sensors!=0:\n",
    "#             for i, sensor in enumerate(random_sensors):\n",
    "#                 batch_sequences[:, random_ini_position[i]:random_ini_position[i]+random_length[i], sensor] = 0.0\n",
    "\n",
    "#     # Shut-down sensor\n",
    "#     if 3 in current_augmentations:\n",
    "#         sensor = np.random.randint(0, 9)\n",
    "#         batch_sequences[:, :, sensor] = 0.0\n",
    "        \n",
    "#     return batch_sequences\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 4. Preprocess\n",
    "\n",
    "# Mha\n",
    "dict_segments_sequences_paths_train = {\n",
    "    segment : path_sequences + 'train/' + str(segment) + '.npy' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_sequences_paths_test = {\n",
    "    segment : path_sequences + 'test/' + str(segment) + '.npy' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "# Spectogram - Tiny\n",
    "dict_segments_spectograms_tiny_paths_train = {\n",
    "    segment : path_spectograms_tiny + 'train/' + str(segment) + '/' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_spectograms_tiny_paths_test = {\n",
    "    segment : path_spectograms_tiny + 'test/' + str(segment) + '/' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "# Spectogram - Big\n",
    "\n",
    "dict_segments_spectograms_big_paths_train = {\n",
    "    segment : path_spectograms_big + 'train/' + str(segment) + '/' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_spectograms_big_paths_test = {\n",
    "    segment : path_spectograms_big + 'test/' + str(segment) + '/' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "# Spectogram - STFT\n",
    "\n",
    "dict_segments_spectograms_stft_paths_train = {\n",
    "    segment : path_spectograms_stft + 'train/' + str(segment) + '/' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_spectograms_stft_paths_test = {\n",
    "    segment : path_spectograms_stft + 'test/' + str(segment) + '/' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "######\n",
    "\n",
    "dict_positions_segments = {k : i for i, k in enumerate(dict_segments_sequences_paths_train.keys())}\n",
    "\n",
    "df_train['time_to_eruption'] = df_train['time_to_eruption']/(10**6)\n",
    "\n",
    "dict_labels = {\n",
    "    segment : df_train['time_to_eruption'][df_train['segment_id']==segment].values.flatten()\n",
    "    for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "# dict_nans_train = getdDictsSpectoGramsNulls(dict_segment_paths_train)\n",
    "# dict_nans_test = getdDictsSpectoGramsNulls(dict_segment_paths_test)\n",
    "\n",
    "# np.save(path + 'dict_nans_train.npy', dict_nans_train)\n",
    "# np.save(path + 'dict_nans_test.npy', dict_nans_test)\n",
    "\n",
    "dict_nans_train = np.load(path + 'dict_nans_train.npy', allow_pickle=True).flatten()[0]\n",
    "dict_nans_test = np.load(path + 'dict_nans_test.npy', allow_pickle=True).flatten()[0]\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 5. Global Functions\n",
    "\n",
    "def getTinyTransforms():\n",
    "    return albumentations.Compose([\n",
    "            albumentations.OneOf([\n",
    "                albumentations.GaussNoise(p=0.2),\n",
    "                albumentations.Cutout(num_holes=4, max_h_size=6, max_w_size=6, fill_value=0, p=0.2),\n",
    "            ], p=0.3),\n",
    "    ])\n",
    "\n",
    "def getBigTransforms():\n",
    "    return albumentations.Compose([\n",
    "            albumentations.OneOf([\n",
    "                albumentations.GaussNoise(p=0.2),\n",
    "                albumentations.Cutout(num_holes=8, max_h_size=12, max_w_size=12, fill_value=0, p=0.2),\n",
    "            ], p=0.3),\n",
    "        albumentations.OpticalDistortion(p=0.3),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.05, rotate_limit=1, p=0.5),\n",
    "        albumentations.RandomCrop(IMG_SIZE_BIG[0]-10, IMG_SIZE_BIG[1]-10, p=0.5),\n",
    "        albumentations.PadIfNeeded(min_height=IMG_SIZE_BIG[0], min_width=IMG_SIZE_BIG[1], value=0, p=1.0)\n",
    "    ])\n",
    "\n",
    "def getStftTransforms():\n",
    "    return albumentations.Compose([\n",
    "            albumentations.OneOf([\n",
    "                albumentations.GaussNoise(p=0.2),\n",
    "                albumentations.Cutout(num_holes=8, max_h_size=12, max_w_size=12, fill_value=0, p=0.2),\n",
    "            ], p=0.3),\n",
    "        albumentations.OpticalDistortion(p=0.3),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.05, rotate_limit=1, p=0.5),\n",
    "        albumentations.RandomCrop(IMG_SIZE_STFT[0]-10, IMG_SIZE_STFT[1]-10, p=0.5),\n",
    "        albumentations.PadIfNeeded(min_height=IMG_SIZE_STFT[0], min_width=IMG_SIZE_STFT[1], value=0, p=1.0)\n",
    "    ])\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 6. Generator\n",
    "\n",
    "# 5.1 MHA Model Data Generator\n",
    "class MHAVolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_sequences, batch_size, dict_labels, augmentations, shuffle=False, training=True):\n",
    "        super(MHAVolcanoSequencesGenerator, self).__init__()\n",
    "        \n",
    "        self.dict_means = {0: 0.09421943291597953, 1: 0.9208114415834104, 2: -0.026617075839858038, \n",
    "                           3: 0.09724443370400684, 4: 1.704695380910225, 5: -0.1180321202370159, 6: 0.7667902421713446, \n",
    "                           7: 0.7804286101804458, 8: -0.2075797991904395, 9: 0.014516944212624944} \n",
    "        \n",
    "        self.dict_stds =  {0: 1820.6211174856987, 1: 1931.0901612736805, 2: 1738.1671740163413, \n",
    "                           3: 1669.8837574619292, 4: 568.5221048211192, 5: 1848.4917466767877, 6: 1623.353060255481, \n",
    "                           7: 1618.2714709240895, 8: 1590.9403316558762, 9: 1906.41447528788}\n",
    "        \n",
    "        self.segments = segments\n",
    "        self.path_sequences = path_sequences\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.augmentations = augmentations\n",
    "        self.shuffle = shuffle\n",
    "        self.training = training\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        \n",
    "        array_sequences = np.asarray([np.load(self.path_sequences[segment], allow_pickle=True)[-SEQ_LENGTH:, :]\n",
    "                                     for segment in list_batch_segments])\n",
    "        \n",
    "        if self.augmentations:\n",
    "            array_sequences = self.augmentBatch(array_sequences)\n",
    "        \n",
    "        array_sequences[:, :, 0] = scale(array_sequences[:, :, 0], self.dict_means[0], self.dict_stds[0])\n",
    "        array_sequences[:, :, 1] = scale(array_sequences[:, :, 1], self.dict_means[1], self.dict_stds[1])\n",
    "        array_sequences[:, :, 2] = scale(array_sequences[:, :, 2], self.dict_means[2], self.dict_stds[2])\n",
    "        array_sequences[:, :, 3] = scale(array_sequences[:, :, 3], self.dict_means[3], self.dict_stds[3])\n",
    "        array_sequences[:, :, 4] = scale(array_sequences[:, :, 4], self.dict_means[4], self.dict_stds[4])\n",
    "        array_sequences[:, :, 5] = scale(array_sequences[:, :, 5], self.dict_means[5], self.dict_stds[5])\n",
    "        array_sequences[:, :, 6] = scale(array_sequences[:, :, 6], self.dict_means[6], self.dict_stds[6])\n",
    "        array_sequences[:, :, 7] = scale(array_sequences[:, :, 7], self.dict_means[7], self.dict_stds[7])\n",
    "        array_sequences[:, :, 8] = scale(array_sequences[:, :, 8], self.dict_means[8], self.dict_stds[8])\n",
    "        array_sequences[:, :, 9] = scale(array_sequences[:, :, 9], self.dict_means[9], self.dict_stds[9])\n",
    "        \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "            return array_sequences, array_labels\n",
    "        else:\n",
    "            return array_sequences\n",
    "        \n",
    "        \n",
    "    def noiseInjection(self, batch_sequences, noise_factor=0.075):\n",
    "        noise = np.random.randn(batch_sequences.shape[0], batch_sequences.shape[1], batch_sequences.shape[2])\n",
    "        augmented_data = batch_sequences + noise_factor * noise\n",
    "        return augmented_data\n",
    "    \n",
    "    \n",
    "    def timeShifting(self, batch_sequences, shift_max):\n",
    "        shift = np.random.randint(shift_max)\n",
    "        for sensor in range(10):\n",
    "            batch_sequences[:, :, sensor] = np.roll(batch_sequences[:, :, sensor], shift)\n",
    "        return batch_sequences\n",
    "       \n",
    "    \n",
    "    def augmentBatch(self, batch_sequences):\n",
    "        \n",
    "        # Add random noise\n",
    "        if np.random.random() > 0.5:\n",
    "            batch_sequences = self.noiseInjection(batch_sequences, noise_factor=0.005)   \n",
    "            \n",
    "        # Time shifting\n",
    "        if np.random.random() > 0.5:\n",
    "            batch_sequences = self.timeShifting(batch_sequences, shift_max=600) \n",
    "                           \n",
    "        # Shut-down sensor\n",
    "        if np.random.random() > 0.5:\n",
    "            sensor = np.random.randint(0, 9)\n",
    "            batch_sequences[:, :, sensor] = 0.0\n",
    "                \n",
    "        return batch_sequences\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "        \n",
    "\n",
    "# 5.2 Spectogram Model Data Generator\n",
    "class SpectoGramVolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_spectograms, batch_size, dict_labels, transforms, shuffle=False, training=True):\n",
    "        super(SpectoGramVolcanoSequencesGenerator, self).__init__()\n",
    "        self.segments = segments\n",
    "        self.path_spectograms = path_spectograms\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.shuffle = shuffle\n",
    "        self.training = training\n",
    "        self.transforms = transforms\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        array_spectograms_s0 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_0.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s1 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_1.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s2 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_2.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s3 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_3.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s4 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_4.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s5 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_5.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s6 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_6.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s7 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_7.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s8 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_8.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s9 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_9.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        \n",
    "        if self.transforms:\n",
    "            data_s0, data_s1 = {'image':array_spectograms_s0}, {'image':array_spectograms_s1}\n",
    "            data_s2, data_s3 = {'image':array_spectograms_s2}, {'image':array_spectograms_s3}\n",
    "            data_s4, data_s5 = {'image':array_spectograms_s4}, {'image':array_spectograms_s5}\n",
    "            data_s6, data_s7 = {'image':array_spectograms_s6}, {'image':array_spectograms_s7}\n",
    "            data_s8, data_s9 = {'image':array_spectograms_s8}, {'image':array_spectograms_s9}\n",
    "            \n",
    "            array_spectograms_s0 = np.stack([self.transforms(image=x)['image'] for x in data_s0['image']], axis=0)\n",
    "            array_spectograms_s1 = np.stack([self.transforms(image=x)['image'] for x in data_s1['image']], axis=0)\n",
    "            array_spectograms_s2 = np.stack([self.transforms(image=x)['image'] for x in data_s2['image']], axis=0)\n",
    "            array_spectograms_s3 = np.stack([self.transforms(image=x)['image'] for x in data_s3['image']], axis=0)\n",
    "            array_spectograms_s4 = np.stack([self.transforms(image=x)['image'] for x in data_s4['image']], axis=0)\n",
    "            array_spectograms_s5 = np.stack([self.transforms(image=x)['image'] for x in data_s5['image']], axis=0)\n",
    "            array_spectograms_s6 = np.stack([self.transforms(image=x)['image'] for x in data_s6['image']], axis=0)\n",
    "            array_spectograms_s7 = np.stack([self.transforms(image=x)['image'] for x in data_s7['image']], axis=0)\n",
    "            array_spectograms_s8 = np.stack([self.transforms(image=x)['image'] for x in data_s8['image']], axis=0)\n",
    "            array_spectograms_s9 = np.stack([self.transforms(image=x)['image'] for x in data_s9['image']], axis=0)\n",
    "                 \n",
    "        batch = (array_spectograms_s0/255, array_spectograms_s1/255, array_spectograms_s2/255, array_spectograms_s3/255, \n",
    "                 array_spectograms_s4/255, array_spectograms_s5/255, array_spectograms_s6/255, array_spectograms_s7/255, \n",
    "                 array_spectograms_s8/255, array_spectograms_s9/255)    \n",
    "            \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "        if self.training:\n",
    "            return batch, array_labels\n",
    "        else:\n",
    "            return batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "# 5.3 STFT Model Data Generator\n",
    "class STFTVolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_spectograms, batch_size, dict_labels, transforms, shuffle=False, training=True):\n",
    "        super(STFTVolcanoSequencesGenerator, self).__init__()\n",
    "        \n",
    "        self.segments = segments\n",
    "        self.path_spectograms = path_spectograms\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.transforms = transforms\n",
    "        self.shuffle = shuffle\n",
    "        self.training = training\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        array_spectograms_s0 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_0.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s1 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_1.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s2 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_2.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s3 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_3.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s4 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_4.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s5 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_5.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s6 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_6.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s7 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_7.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s8 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_8.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s9 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_9.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)  \n",
    "        \n",
    "        if self.transforms:\n",
    "            data_s0, data_s1 = {'image':array_spectograms_s0}, {'image':array_spectograms_s1}\n",
    "            data_s2, data_s3 = {'image':array_spectograms_s2}, {'image':array_spectograms_s3}\n",
    "            data_s4, data_s5 = {'image':array_spectograms_s4}, {'image':array_spectograms_s5}\n",
    "            data_s6, data_s7 = {'image':array_spectograms_s6}, {'image':array_spectograms_s7}\n",
    "            data_s8, data_s9 = {'image':array_spectograms_s8}, {'image':array_spectograms_s9}\n",
    "            \n",
    "            array_spectograms_s0 = np.stack([self.transforms(image=x)['image'] for x in data_s0['image']], axis=0)\n",
    "            array_spectograms_s1 = np.stack([self.transforms(image=x)['image'] for x in data_s1['image']], axis=0)\n",
    "            array_spectograms_s2 = np.stack([self.transforms(image=x)['image'] for x in data_s2['image']], axis=0)\n",
    "            array_spectograms_s3 = np.stack([self.transforms(image=x)['image'] for x in data_s3['image']], axis=0)\n",
    "            array_spectograms_s4 = np.stack([self.transforms(image=x)['image'] for x in data_s4['image']], axis=0)\n",
    "            array_spectograms_s5 = np.stack([self.transforms(image=x)['image'] for x in data_s5['image']], axis=0)\n",
    "            array_spectograms_s6 = np.stack([self.transforms(image=x)['image'] for x in data_s6['image']], axis=0)\n",
    "            array_spectograms_s7 = np.stack([self.transforms(image=x)['image'] for x in data_s7['image']], axis=0)\n",
    "            array_spectograms_s8 = np.stack([self.transforms(image=x)['image'] for x in data_s8['image']], axis=0)\n",
    "            array_spectograms_s9 = np.stack([self.transforms(image=x)['image'] for x in data_s9['image']], axis=0)\n",
    "                 \n",
    "        batch = (array_spectograms_s0/255, array_spectograms_s1/255, array_spectograms_s2/255, array_spectograms_s3/255, \n",
    "                 array_spectograms_s4/255, array_spectograms_s5/255, array_spectograms_s6/255, array_spectograms_s7/255, \n",
    "                 array_spectograms_s8/255, array_spectograms_s9/255)       \n",
    "            \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "            return batch, array_labels\n",
    "        else:\n",
    "            return batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "\n",
    "# 5.4\n",
    "def buildDataset(dict_paths):\n",
    "            \n",
    "    fs = 100                # sampling frequency \n",
    "    n = 256                 # FFT segment size\n",
    "    max_f = 20              # ～20Hz\n",
    "\n",
    "    delta_f = fs / n        # 0.39Hz\n",
    "    delta_t = n / fs / 2    # 1.28s\n",
    "\n",
    "    feature_set = []\n",
    "    for segment_id in tqdm(dict_paths, total=len(dict_paths), position=0):\n",
    "        data = np.load(dict_paths[segment_id])\n",
    "        segment = [segment_id]\n",
    "        for sensor in range(10):\n",
    "            x = data[:, sensor]\n",
    "            f, t, Z = scipy.signal.stft(x, fs = fs, window = 'hann', nperseg = n)\n",
    "            f = f[:round(max_f/delta_f)+1]\n",
    "            Z = np.abs(Z[:round(max_f/delta_f)+1]).T\n",
    "\n",
    "            th = Z.mean() * 1 \n",
    "            Z_pow = Z.copy()\n",
    "            Z_pow[Z < th] = 0\n",
    "            Z_num = Z_pow.copy()\n",
    "            Z_num[Z >= th] = 1\n",
    "\n",
    "            Z_pow_sum = Z_pow.sum(axis = 0)\n",
    "            Z_num_sum = Z_num.sum(axis = 0)\n",
    "\n",
    "            A_pow = Z_pow_sum[round(10/delta_f):].sum()\n",
    "            A_num = Z_num_sum[round(10/delta_f):].sum()\n",
    "            BH_pow = Z_pow_sum[round(5/delta_f):round(8/delta_f)].sum()\n",
    "            BH_num = Z_num_sum[round(5/delta_f):round(8/delta_f)].sum()\n",
    "            BL_pow = Z_pow_sum[round(1.5/delta_f):round(2.5/delta_f)].sum()\n",
    "            BL_num = Z_num_sum[round(1.5/delta_f):round(2.5/delta_f)].sum()\n",
    "            C_pow = Z_pow_sum[round(0.6/delta_f):round(1.2/delta_f)].sum()\n",
    "            C_num = Z_num_sum[round(0.6/delta_f):round(1.2/delta_f)].sum()\n",
    "            D_pow = Z_pow_sum[round(2/delta_f):round(4/delta_f)].sum()\n",
    "            D_num = Z_num_sum[round(2/delta_f):round(4/delta_f)].sum()\n",
    "            segment += [A_pow, A_num, BH_pow, BH_num, BL_pow, BL_num, C_pow, C_num, D_pow, D_num]\n",
    "\n",
    "        feature_set.append(segment)\n",
    "\n",
    "    cols = ['segment_id']\n",
    "    for i in range(10):\n",
    "        for j in ['A_pow', 'A_num','BH_pow', 'BH_num','BL_pow', 'BL_num','C_pow', 'C_num','D_pow', 'D_num']:\n",
    "            cols += [f's{i+1}_{j}']\n",
    "    feature_df = pd.DataFrame(feature_set, columns = cols)\n",
    "    feature_df['segment_id'] = feature_df['segment_id'].astype('int')\n",
    "    \n",
    "    return feature_df\n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2378be0b96f4b08bbaffa6d37fbdc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mha\n",
      "cnn2d_stft\n",
      "lgbm_stft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e497bb9b48b14d16af1410f7dabc0f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=887.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tiny\n",
      "big\n",
      "mha\n",
      "cnn2d_stft\n",
      "lgbm_stft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8c8233e1bd4072a53c156a615b34d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=886.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tiny\n",
      "big\n",
      "mha\n",
      "cnn2d_stft\n",
      "lgbm_stft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca758e1487ca43a5aff178f6bce7e2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=886.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tiny\n",
      "big\n",
      "mha\n",
      "cnn2d_stft\n",
      "lgbm_stft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febbaf12d4b24c7a8d85c9f866db7761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=886.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tiny\n",
      "big\n",
      "mha\n",
      "cnn2d_stft\n",
      "lgbm_stft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455dc8a97fa44ebf9e861193918970ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=886.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tiny\n",
      "big\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-be8adbb20ad2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'big'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tiny'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mkey_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mdict_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     \u001b[1;32melif\u001b[0m  \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'stft'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mkey_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pred'"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 6. Build Train Set\n",
    "\n",
    "batch_size = 8\n",
    "qt_augmentations = 10\n",
    "\n",
    "pbar = tqdm(total=(len(path_models_mha)*5), position=0)\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "\n",
    "dict_predictions = {'tiny' : {}, 'big': {}, 'mha': {}, 'lgbm_stft': {}, 'cnn2d_stft': {}, 'y_true': {}, 'segment_id': {}}        \n",
    "for num_fold, (train_index, val_index) in enumerate(kf.split(list_segments_train,\n",
    "                                                     np.zeros(len(list_segments_train)))):\n",
    "\n",
    "    paths = (path_models_mha[num_fold], path_models_spectogram_stft[num_fold],  \n",
    "             path_models_lgbm_stft[num_fold], path_models_spectogram_tiny[num_fold], \n",
    "             path_models_spectogram_big[num_fold])\n",
    "    \n",
    "    for path in paths: \n",
    "        type_model = '_'.join(path.split('_')[-3:-1]).lower() if path.split('_')[-2].lower() == 'stft' else path.split('_')[-2].lower()\n",
    "        print(type_model)\n",
    "        if type_model in {'tiny', 'big', 'mha', 'cnn2d_stft'}:\n",
    "            model = models.load_model(path, compile=False) \n",
    "        \n",
    "        segments_train_fold = np.asarray(list_segments_train)[train_index]\n",
    "        segments_val_fold = np.asarray(list_segments_train)[val_index]\n",
    "        \n",
    "        if type_model in {'tiny'}:\n",
    "            X_val_generator = SpectoGramVolcanoSequencesGenerator(segments_val_fold, \n",
    "                                                                  dict_segments_spectograms_tiny_paths_train,\n",
    "                                                                  batch_size=batch_size, dict_labels=dict_labels,\n",
    "                                                                  transforms=getTinyTransforms(),\n",
    "                                                                  training=True, shuffle=False)     \n",
    "        elif type_model in {'big'}:\n",
    "            X_val_generator = SpectoGramVolcanoSequencesGenerator(segments_val_fold, \n",
    "                                                                  dict_segments_spectograms_big_paths_train,\n",
    "                                                                  batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                                  transforms=getBigTransforms(),\n",
    "                                                                  training=True, shuffle=False)\n",
    "            \n",
    "        elif type_model in {'mha'}:\n",
    "            X_val_generator = MHAVolcanoSequencesGenerator(segments_val_fold, \n",
    "                                                          dict_segments_sequences_paths_train,\n",
    "                                                          batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                          augmentations=True, training=True, shuffle=False)   \n",
    "        elif type_model in {'cnn2d_stft'}:\n",
    "            X_val_generator = STFTVolcanoSequencesGenerator(segments_val_fold, \n",
    "                                                            dict_segments_spectograms_stft_paths_train,\n",
    "                                                            batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                            transforms=getStftTransforms(), training=True, shuffle=False)\n",
    "        elif type_model in {'lgbm_stft'}:\n",
    "             pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Incorrect Type model, {type_model}')\n",
    "              \n",
    "        y_val_target = np.asarray([dict_labels[segment] for segment in segments_val_fold])\n",
    "\n",
    "        list_segments = list(segments_val_fold)\n",
    "        array_predictions = np.zeros((len(list_segments)), dtype=np.float32)\n",
    "        array_confidence = np.zeros((len(list_segments)), dtype=np.float32)\n",
    "        \n",
    "        if type_model not in {'lgbm_stft'}:\n",
    "            array_predictions_std = np.zeros((len(list_segments)), dtype=np.float32)\n",
    "            array_confidence_std = np.zeros((len(list_segments)), dtype=np.float32)\n",
    "            list_predictions, list_conf = [], []\n",
    "            for i in range(qt_augmentations):\n",
    "                y_pred_val = model.predict(X_val_generator)\n",
    "                list_predictions.append(y_pred_val[0])\n",
    "                list_conf.append(y_pred_val[1][:, 2] - y_pred_val[1][:, 0])\n",
    "\n",
    "            array_predictions = np.asarray(list_predictions).mean(axis=0).squeeze().astype(np.float32)\n",
    "            array_confidence = np.asarray(list_conf).mean(axis=0).squeeze().astype(np.float32)\n",
    "            array_predictions_std = np.asarray(list_predictions).std(axis=0).squeeze().astype(np.float32)\n",
    "            array_confidence_std = np.asarray(list_conf).std(axis=0).squeeze().astype(np.float32)\n",
    "\n",
    "            dict_predictions[type_model][num_fold] = {\n",
    "                'pred_mean': array_predictions, 'pred_std': array_predictions_std,\n",
    "                'conf_mean' : array_confidence, 'conf_std' : array_confidence_std\n",
    "            }\n",
    "            \n",
    "        else: \n",
    "            df_X_val = buildDataset({k: dict_segments_sequences_paths_train[k] for k in segments_val_fold})\n",
    "            df_X_val = df_X_val.merge(df_train[['segment_id', 'time_to_eruption']], how='inner')\n",
    "            features = [col for col in df_X_val.columns.tolist() if col not in ['segment_id', 'time_to_eruption']]\n",
    "            X_val = df_X_val[features]\n",
    "            y_val = df_X_val['time_to_eruption']\n",
    "            model = pickle.load(open(f'{path}.pickle', 'rb'))\n",
    "            array_predictions = model.predict(X_val)\n",
    "            array_confidence[:] = 1.0\n",
    "            dict_predictions[type_model][num_fold] = {'pred' : array_predictions, 'conf' : array_confidence}\n",
    "            \n",
    "        \n",
    "        pbar.update(1)\n",
    "            \n",
    "        dict_predictions['y_true'][num_fold] = y_val_target\n",
    "        dict_predictions['segment_id'][num_fold] = segments_val_fold\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        \n",
    "pbar.close()\n",
    "\n",
    "np.save(f'./dict_predictions.npy', dict_predictions) \n",
    "dict_predictions = np.load(f'./dict_predictions.npy', allow_pickle=True).flatten()[0]\n",
    "\n",
    "# del list_predictions\n",
    "# gc.collect()\n",
    "\n",
    "columns_names = []\n",
    "for key in dict_predictions.keys():\n",
    "    if key in ['mha', 'big', 'tiny', 'cnn2d_stft', 'lgbm_stft']:\n",
    "        for fold in list(dict_predictions[key].keys()):\n",
    "            if isinstance(dict_predictions[key][fold], dict):\n",
    "                for var in dict_predictions[key][fold]:\n",
    "                    columns_names.append(f'{key}_{fold}_{var}')  \n",
    "            else:\n",
    "                columns_names.append(f'{key}_{fold}')\n",
    "                \n",
    "columns_names = columns_names + ['y_true', 'segment_id']\n",
    "dict_df = {}\n",
    "\n",
    "for col in columns_names:\n",
    "    if col.split('_')[0] in ['mha']:\n",
    "        key_1, key_2, key_3 = col.split('_')[0], col.split('_')[1], '_'.join([col.split('_')[2], col.split('_')[3]])\n",
    "        dict_df[col] = dict_predictions[key_1][int(key_2)][key_3]\n",
    "    elif col.split('_')[0] in ['big', 'tiny']:\n",
    "        key_1, key_2, key_3 = col.split('_')[0], col.split('_')[1], col.split('_')[2]\n",
    "        dict_df[col] = dict_predictions[key_1][int(key_2)][key_3]\n",
    "    elif  col.split('_')[1] in ['stft']:\n",
    "        key_1, key_2, key_3, key_4 = col.split('_')[0], col.split('_')[1], col.split('_')[2], col.split('_')[3]\n",
    "        dict_df[col] = dict_predictions[f'{key_1}_{key_2}'][int(key_3)][key_4]\n",
    "    else:\n",
    "        dict_df[col] = np.concatenate(list(dict_predictions[col].values()))\n",
    "# print('='*50)       \n",
    "# for key in dict_df:\n",
    "#     print(f'{key}: {len(dict_df[key])}')\n",
    "\n",
    "dict_build_df = {\n",
    "    'segment_id' : dict_df['segment_id'].squeeze(),\n",
    "    'mha_pred_mean' : np.concatenate([dict_df['mha_0_pred_mean'], dict_df['mha_1_pred_mean'], \n",
    "                                 dict_df['mha_2_pred_mean'], dict_df['mha_3_pred_mean'], dict_df['mha_4_pred_mean']]).squeeze(),\n",
    "    'mha_pred_std' : np.concatenate([dict_df['mha_0_pred_std'], dict_df['mha_1_pred_std'], \n",
    "                                 dict_df['mha_2_pred_std'], dict_df['mha_3_pred_std'], dict_df['mha_4_pred_std']]).squeeze(),\n",
    "    'mha_conf_mean' : np.concatenate([dict_df['mha_0_conf_mean'], dict_df['mha_1_conf_mean'], \n",
    "                                 dict_df['mha_2_conf_mean'], dict_df['mha_3_conf_mean'], dict_df['mha_4_conf_mean']]).squeeze(),\n",
    "    'mha_conf_std' : np.concatenate([dict_df['mha_0_conf_std'], dict_df['mha_1_conf_std'], \n",
    "                                 dict_df['mha_2_conf_std'], dict_df['mha_3_conf_std'], dict_df['mha_4_conf_std']]).squeeze(),\n",
    "    'cnn_tiny_pred' : np.concatenate([dict_df['tiny_0_pred'], dict_df['tiny_1_pred'], \n",
    "                                      dict_df['tiny_2_pred'], dict_df['tiny_3_pred'], dict_df['tiny_4_pred']]).squeeze(),\n",
    "    'cnn_tiny_conf' : np.concatenate([dict_df['tiny_0_conf'], dict_df['tiny_1_conf'], \n",
    "                                      dict_df['tiny_2_conf'], dict_df['tiny_3_conf'], dict_df['tiny_4_conf']]).squeeze(),\n",
    "    'cnn_big_pred' : np.concatenate([dict_df['big_0_pred'], dict_df['big_1_pred'], \n",
    "                                     dict_df['big_2_pred'], dict_df['big_3_pred'], dict_df['big_4_pred']]).squeeze(),\n",
    "    'cnn_big_conf' : np.concatenate([dict_df['big_0_conf'], dict_df['big_1_conf'], \n",
    "                                     dict_df['big_2_conf'], dict_df['big_3_conf'], dict_df['big_4_conf']]).squeeze(),\n",
    "    'cnn_stft_pred' : np.concatenate([dict_df['cnn2d_stft_0_pred'], dict_df['cnn2d_stft_1_pred'], \n",
    "                                     dict_df['cnn2d_stft_2_pred'], dict_df['cnn2d_stft_3_pred'], dict_df['cnn2d_stft_4_pred']]).squeeze(),\n",
    "    'cnn_stft_conf' : np.concatenate([dict_df['cnn2d_stft_0_conf'], dict_df['cnn2d_stft_1_conf'], \n",
    "                                     dict_df['cnn2d_stft_2_conf'], dict_df['cnn2d_stft_3_conf'], dict_df['cnn2d_stft_4_conf']]).squeeze(),\n",
    "    'lgbm_stft_pred' : np.concatenate([dict_df['lgbm_stft_0_pred'], dict_df['lgbm_stft_1_pred'], \n",
    "                                     dict_df['lgbm_stft_2_pred'], dict_df['lgbm_stft_3_pred'], dict_df['lgbm_stft_4_pred']]).squeeze(),\n",
    "    'lgbm_stft_conf' : np.concatenate([dict_df['lgbm_stft_0_conf'], dict_df['lgbm_stft_1_conf'], \n",
    "                                     dict_df['lgbm_stft_2_conf'], dict_df['lgbm_stft_3_conf'], dict_df['lgbm_stft_4_conf']]).squeeze(),\n",
    "    'y_true' : dict_df['y_true'].squeeze() \n",
    "}\n",
    "\n",
    "df_train_l2 = pd.DataFrame(dict_build_df)\n",
    "df_train_l2.to_csv('./df_train_l2.csv', index=False)\n",
    "\n",
    "# del dict_predictions, dict_build_df\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4431 4431 4431\n",
      "4431 4431 4431\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 7. Add other models predictions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train_l2 = pd.read_csv('./df_train_l2.csv')\n",
    "df_tabular_time = pd.read_csv('./df_time_tabular_validations.csv')\n",
    "df_tabular_stft2 = pd.read_csv('./df_stft_tabular_validations.csv')\n",
    "\n",
    "df_tabular_time['time_to_eruption'] = df_tabular_time['time_to_eruption'] /(10**6)\n",
    "df_tabular_stft2['time_to_eruption'] = df_tabular_stft2['time_to_eruption'] /(10**6)\n",
    "\n",
    "df_tabular_time.columns = ['segment_id', 'lgbm_time_pred_mean']\n",
    "df_tabular_stft2.columns = ['segment_id', 'lgbm_stft_pred_mean']\n",
    "\n",
    "df_added = pd.merge(df_tabular_time, df_tabular_stft2, how='inner', on=['segment_id'])\n",
    "print(df_added.shape[0], df_tabular_time.shape[0], df_tabular_stft2.shape[0])\n",
    "df_all = pd.merge(df_train_l2, df_added, how='inner', on=['segment_id'])\n",
    "print(df_all.shape[0], df_added.shape[0], df_train_l2.shape[0])\n",
    "df_all.to_csv('./df_all_train_l2.csv', index=False)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_names = []\n",
    "# for key in dict_predictions.keys():\n",
    "#     if key in ['mha', 'big', 'tiny', 'cnn2d_stft', 'lgbm_stft']:\n",
    "#         for fold in list(dict_predictions[key].keys()):\n",
    "#             if isinstance(dict_predictions[key][fold], dict):\n",
    "#                 for var in dict_predictions[key][fold]:\n",
    "#                     columns_names.append(f'{key}_{fold}_{var}')  \n",
    "#             else:\n",
    "#                 columns_names.append(f'{key}_{fold}')\n",
    "                \n",
    "# columns_names = columns_names + ['y_true', 'segment_id']\n",
    "# dict_df = {}\n",
    "# for col in columns_names:\n",
    "#     if col.split('_')[0] in ['mha', 'big', 'tiny']:\n",
    "#         key_1, key_2, key_3 = col.split('_')[0], col.split('_')[1], '_'.join([col.split('_')[2], col.split('_')[3]])\n",
    "#         dict_df[col] = dict_predictions[key_1][int(key_2)][key_3]\n",
    "#     elif col.split('_')[1] in ['stft'] and col.split('_')[0] != 'lgbm':\n",
    "#         key_1, key_2, key_3, key_4 = col.split('_')[0], col.split('_')[1], col.split('_')[2], '_'.join(col.split('_')[-2:])\n",
    "#         dict_df[col] = dict_predictions[f'{key_1}_{key_2}'][int(key_3)][key_4]\n",
    "#     elif col.split('_')[1] in ['stft'] and col.split('_')[0] == 'lgbm':\n",
    "#         key_1, key_2, key_3, key_4 = col.split('_')[0], col.split('_')[1], col.split('_')[2], col.split('_')[3]\n",
    "#         dict_df[col] = dict_predictions[f'{key_1}_{key_2}'][int(key_3)][key_4]\n",
    "#     else:\n",
    "#         dict_df[col] = np.concatenate(list(dict_predictions[col].values()))\n",
    "# # print('='*50)       \n",
    "# # for key in dict_df:\n",
    "# #     print(f'{key}: {len(dict_df[key])}')\n",
    "\n",
    "# dict_build_df = {\n",
    "#     'segment_id' : dict_df['segment_id'].squeeze(),\n",
    "#     'mha_pred_mean' : np.concatenate([dict_df['mha_0_pred_mean'], dict_df['mha_1_pred_mean'], \n",
    "#                                  dict_df['mha_2_pred_mean'], dict_df['mha_3_pred_mean'], dict_df['mha_4_pred_mean']]).squeeze(),\n",
    "#     'mha_pred_std' : np.concatenate([dict_df['mha_0_pred_std'], dict_df['mha_1_pred_std'], \n",
    "#                                  dict_df['mha_2_pred_std'], dict_df['mha_3_pred_std'], dict_df['mha_4_pred_std']]).squeeze(),\n",
    "#     'mha_conf_mean' : np.concatenate([dict_df['mha_0_conf_mean'], dict_df['mha_1_conf_mean'], \n",
    "#                                  dict_df['mha_2_conf_mean'], dict_df['mha_3_conf_mean'], dict_df['mha_4_conf_mean']]).squeeze(),\n",
    "#     'mha_conf_std' : np.concatenate([dict_df['mha_0_conf_std'], dict_df['mha_1_conf_std'], \n",
    "#                                  dict_df['mha_2_conf_std'], dict_df['mha_3_conf_std'], dict_df['mha_4_conf_std']]).squeeze(),\n",
    "#     'cnn_tiny_pred_mean' : np.concatenate([dict_df['tiny_0_pred_mean'], dict_df['tiny_1_pred_mean'], \n",
    "#                                           dict_df['tiny_2_pred_mean'], dict_df['tiny_3_pred_mean'], dict_df['tiny_4_pred_mean']]).squeeze(),\n",
    "#     'cnn_tiny_pred_std' : np.concatenate([dict_df['tiny_0_pred_std'], dict_df['tiny_1_pred_std'], \n",
    "#                                           dict_df['tiny_2_pred_std'], dict_df['tiny_3_pred_std'], dict_df['tiny_4_pred_std']]).squeeze(),\n",
    "#     'cnn_tiny_conf_mean' : np.concatenate([dict_df['tiny_0_conf_mean'], dict_df['tiny_1_conf_mean'], \n",
    "#                                       dict_df['tiny_2_conf_mean'], dict_df['tiny_3_conf_mean'], dict_df['tiny_4_conf_mean']]).squeeze(),\n",
    "#     'cnn_tiny_conf_std' : np.concatenate([dict_df['tiny_0_conf_std'], dict_df['tiny_1_conf_std'], \n",
    "#                                       dict_df['tiny_2_conf_std'], dict_df['tiny_3_conf_std'], dict_df['tiny_4_conf_std']]).squeeze(),\n",
    "#     'cnn_big_pred_mean' : np.concatenate([dict_df['big_0_pred_mean'], dict_df['big_1_pred_mean'], \n",
    "#                                      dict_df['big_2_pred_mean'], dict_df['big_3_pred_mean'], dict_df['big_4_pred_mean']]).squeeze(),\n",
    "#     'cnn_big_pred_std' : np.concatenate([dict_df['big_0_pred_std'], dict_df['big_1_pred_std'], \n",
    "#                                      dict_df['big_2_pred_std'], dict_df['big_3_pred_std'], dict_df['big_4_pred_std']]).squeeze(),\n",
    "#     'cnn_big_conf_mean' : np.concatenate([dict_df['big_0_conf_mean'], dict_df['big_1_conf_mean'], \n",
    "#                                      dict_df['big_2_conf_mean'], dict_df['big_3_conf_mean'], dict_df['big_4_conf_mean']]).squeeze(),\n",
    "#     'cnn_big_conf_std' : np.concatenate([dict_df['big_0_conf_std'], dict_df['big_1_conf_std'], \n",
    "#                                      dict_df['big_2_conf_std'], dict_df['big_3_conf_std'], dict_df['big_4_conf_std']]).squeeze(),\n",
    "#     'cnn_stft_pred_mean' : np.concatenate([dict_df['cnn2d_stft_0_pred_mean'], dict_df['cnn2d_stft_1_pred_mean'], \n",
    "#                                      dict_df['cnn2d_stft_2_pred_mean'], dict_df['cnn2d_stft_3_pred_mean'], dict_df['cnn2d_stft_4_pred_mean']]).squeeze(),\n",
    "#     'cnn_stft_pred_std' : np.concatenate([dict_df['cnn2d_stft_0_pred_std'], dict_df['cnn2d_stft_1_pred_std'], \n",
    "#                                      dict_df['cnn2d_stft_2_pred_std'], dict_df['cnn2d_stft_3_pred_std'], dict_df['cnn2d_stft_4_pred_std']]).squeeze(),\n",
    "#     'cnn_stft_conf_mean' : np.concatenate([dict_df['cnn2d_stft_0_conf_mean'], dict_df['cnn2d_stft_1_conf_mean'], \n",
    "#                                      dict_df['cnn2d_stft_2_conf_mean'], dict_df['cnn2d_stft_3_conf_mean'], dict_df['cnn2d_stft_4_conf_mean']]).squeeze(),\n",
    "#     'cnn_stft_conf_std' : np.concatenate([dict_df['cnn2d_stft_0_conf_std'], dict_df['cnn2d_stft_1_conf_std'], \n",
    "#                                      dict_df['cnn2d_stft_2_conf_std'], dict_df['cnn2d_stft_3_conf_std'], dict_df['cnn2d_stft_4_conf_std']]).squeeze(),\n",
    "#     'lgbm_stft_pred' : np.concatenate([dict_df['lgbm_stft_0_pred'], dict_df['lgbm_stft_1_pred'], \n",
    "#                                      dict_df['lgbm_stft_2_pred'], dict_df['lgbm_stft_3_pred'], dict_df['lgbm_stft_4_pred']]).squeeze(),\n",
    "#     'lgbm_stft_conf' : np.concatenate([dict_df['lgbm_stft_0_conf'], dict_df['lgbm_stft_1_conf'], \n",
    "#                                      dict_df['lgbm_stft_2_conf'], dict_df['lgbm_stft_3_conf'], dict_df['lgbm_stft_4_conf']]).squeeze(),\n",
    "#     'y_true' : dict_df['y_true'].squeeze() \n",
    "# }\n",
    "\n",
    "# df_train_l2 = pd.DataFrame(dict_build_df)\n",
    "# df_train_l2.to_csv('./df_train_l2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
