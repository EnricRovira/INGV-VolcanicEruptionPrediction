{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 1. Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, train_test_split\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers, metrics, losses, optimizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import scipy.signal\n",
    "import torch\n",
    "import albumentations\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 2. Paths & Global Variables\n",
    "\n",
    "## 2.1 Paths\n",
    "\n",
    "path = '../../../01_Data/'\n",
    "path_sequences = path + '01_GeneratedSequences/'\n",
    "path_spectograms_tiny = path + '02_GeneratedSpectograms_MelFeatures/'\n",
    "path_spectograms_big = path + '03_GeneratedSpectograms_Scipy/'\n",
    "path_spectograms_stft = path + '04_GeneratedSpectogramsSTFT/'\n",
    "\n",
    "path_models_mha = [f'../models/model_mha_{num_fold}' for num_fold in range(5)]\n",
    "path_models_spectogram_tiny = [f'../models/model_cnn2d_Tiny_{num_fold}' for num_fold in range(5)]\n",
    "path_models_spectogram_big = [f'../models/model_cnn2d_Big_{num_fold}' for num_fold in range(5)]\n",
    "path_models_spectogram_stft = [f'../models/model_cnn2d_STFT_{num_fold}' for num_fold in range(5)]\n",
    "path_models_lgbm_stft = [f'../models/model_lgbm_STFT_{num_fold}' for num_fold in range(5)]\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(path + 'train.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv') \n",
    "\n",
    "train_paths = glob.glob(path + 'train/*')\n",
    "test_paths = glob.glob(path + 'test/*')\n",
    "\n",
    "unique_segments_id_train = set(df_train['segment_id'])\n",
    "unique_segments_id_test = set(df_sample_submission['segment_id'])\n",
    "\n",
    "dict_unique_segments_id = { v : k for k, v in enumerate(unique_segments_id_train)}\n",
    "dict_unique_segments_id_inv = { k : v for k, v in enumerate(unique_segments_id_train)}\n",
    "\n",
    "## 2.2 Global Variables\n",
    "\n",
    "SEQ_LENGTH = 60_001\n",
    "\n",
    "IMG_SIZE_TINY = (40, 118)\n",
    "IMG_SIZE_BIG = (128, 235)\n",
    "IMG_SIZE_STFT = (128, 469)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 3. Global Functions\n",
    "\n",
    "def buildSequences(df, dict_segment_paths, training=True, mask_value=-1.0):\n",
    "    x = np.zeros((len(dict_segment_paths), SEQ_LENGTH, 10))\n",
    "    if training:\n",
    "        y = np.zeros(len(dict_segment_paths))\n",
    "    for i, segment in enumerate(tqdm(dict_segment_paths, total=len(dict_segment_paths), position=0)):\n",
    "        segment_path = dict_segment_paths[segment]\n",
    "        df_tmp = pd.read_csv(segment_path)\n",
    "        df_tmp = df_tmp.fillna(mask_value)\n",
    "        x[i] = df_tmp.values[-SEQ_LENGTH:]\n",
    "        if training:\n",
    "            y[i] = df['time_to_eruption'][df['segment_id']==segment].values[0]\n",
    "    if training:\n",
    "        return x, y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def scale(x, mean_, std_):\n",
    "    return (x - mean_) / std_\n",
    "\n",
    "\n",
    "def unscale(x, mean_, std_):\n",
    "    return (x * std_) + mean_\n",
    "\n",
    "\n",
    "def monoToColor(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "#     X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Scale to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "\n",
    "# 5.2 Augmentations\n",
    "\n",
    "# def noiseInjection(batch_sequences, noise_factor=0.075):\n",
    "#     noise = np.random.randn(batch_sequences.shape[0], batch_sequences.shape[1], batch_sequences.shape[2])\n",
    "#     augmented_data = batch_sequences + noise_factor * noise\n",
    "#     return augmented_data\n",
    "\n",
    "\n",
    "# def timeShifting(batch_sequences, shift_max):\n",
    "#     shift = np.random.randint(shift_max)\n",
    "#     for sensor in range(10):\n",
    "#         batch_sequences[:, :, sensor] = np.roll(batch_sequences[:, :, sensor], shift)\n",
    "#     return batch_sequences\n",
    "\n",
    "# def makeAugmentations(list_segments, dict_path_data, seq_length):\n",
    "    \n",
    "#     batch_sequences = np.asarray([np.load(dict_path_data[segment]) for segment in list_segments], dtype=np.float32)\n",
    "    \n",
    "#     list_augmentations = [0, 1, 2, 3]\n",
    "#     current_augmentations = list(np.random.choice(list_augmentations, size=np.random.randint(1, 4) ,replace=False))\n",
    "      \n",
    "#     # Add random noise\n",
    "#     if 0 in current_augmentations:\n",
    "#         batch_sequences = noiseInjection(batch_sequences, noise_factor=0.05)   \n",
    "\n",
    "#     # Time shifting\n",
    "#     if 1 in current_augmentations:\n",
    "#         batch_sequences = timeShifting(batch_sequences, shift_max=6_000) \n",
    "\n",
    "#     # Random batch sequence sensors slices to null\n",
    "#     if 2 in current_augmentations:\n",
    "#         num_random_sensors = np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "#         random_sensors = list(set(np.random.randint(0, 9, size=num_random_sensors)))\n",
    "#         random_ini_position = np.random.randint(0, seq_length, size=num_random_sensors)\n",
    "#         random_length = np.random.randint(random_ini_position, random_ini_position+6_000, size=num_random_sensors)\n",
    "#         random_length -= random_ini_position\n",
    "\n",
    "#         if num_random_sensors!=0:\n",
    "#             for i, sensor in enumerate(random_sensors):\n",
    "#                 batch_sequences[:, random_ini_position[i]:random_ini_position[i]+random_length[i], sensor] = 0.0\n",
    "\n",
    "#     # Shut-down sensor\n",
    "#     if 3 in current_augmentations:\n",
    "#         sensor = np.random.randint(0, 9)\n",
    "#         batch_sequences[:, :, sensor] = 0.0\n",
    "        \n",
    "#     return batch_sequences.astype(np.float32)\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 4. Preprocess\n",
    "\n",
    "# Mha\n",
    "dict_segments_sequences_paths_train = {\n",
    "    segment : path_sequences + 'train/' + str(segment) + '.npy' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_sequences_paths_test = {\n",
    "    segment : path_sequences + 'test/' + str(segment) + '.npy' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "# Spectogram - Tiny\n",
    "dict_segments_spectograms_tiny_paths_train = {\n",
    "    segment : path_spectograms_tiny + 'train/' + str(segment) + '/' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_spectograms_tiny_paths_test = {\n",
    "    segment : path_spectograms_tiny + 'test/' + str(segment) + '/' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "# Spectogram - Big\n",
    "\n",
    "dict_segments_spectograms_big_paths_train = {\n",
    "    segment : path_spectograms_big + 'train/' + str(segment) + '/' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_spectograms_big_paths_test = {\n",
    "    segment : path_spectograms_big + 'test/' + str(segment) + '/' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "# Spectogram - STFT\n",
    "\n",
    "dict_segments_spectograms_stft_paths_train = {\n",
    "    segment : path_spectograms_stft + 'train/' + str(segment) + '/' for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "dict_segments_spectograms_stft_paths_test = {\n",
    "    segment : path_spectograms_stft + 'test/' + str(segment) + '/' for segment in unique_segments_id_test\n",
    "}\n",
    "\n",
    "######\n",
    "\n",
    "dict_positions_segments = {k : i for i, k in enumerate(dict_segments_sequences_paths_train.keys())}\n",
    "\n",
    "df_train['time_to_eruption'] = df_train['time_to_eruption']/(10**6)\n",
    "\n",
    "dict_labels = {\n",
    "    segment : df_train['time_to_eruption'][df_train['segment_id']==segment].values.flatten()\n",
    "    for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "###\n",
    "\n",
    "# dict_nans_train = getdDictsSpectoGramsNulls(dict_segment_paths_train)\n",
    "# dict_nans_test = getdDictsSpectoGramsNulls(dict_segment_paths_test)\n",
    "\n",
    "# np.save(path + 'dict_nans_train.npy', dict_nans_train)\n",
    "# np.save(path + 'dict_nans_test.npy', dict_nans_test)\n",
    "\n",
    "# dict_nans_train = np.load(path + 'dict_nans_train.npy', allow_pickle=True).flatten()[0]\n",
    "# dict_nans_test = np.load(path + 'dict_nans_test.npy', allow_pickle=True).flatten()[0]\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 5. Global Functions\n",
    "\n",
    "def getTinyTransforms():\n",
    "    return albumentations.Compose([\n",
    "            albumentations.OneOf([\n",
    "                albumentations.GaussNoise(p=0.2),\n",
    "                albumentations.Cutout(num_holes=4, max_h_size=6, max_w_size=6, fill_value=0, p=0.2),\n",
    "            ], p=0.3),\n",
    "    ])\n",
    "\n",
    "def getBigTransforms():\n",
    "    return albumentations.Compose([\n",
    "            albumentations.OneOf([\n",
    "                albumentations.GaussNoise(p=0.2),\n",
    "                albumentations.Cutout(num_holes=8, max_h_size=12, max_w_size=12, fill_value=0, p=0.2),\n",
    "            ], p=0.3),\n",
    "        albumentations.OpticalDistortion(p=0.3),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.05, rotate_limit=1, p=0.5),\n",
    "        albumentations.RandomCrop(IMG_SIZE_BIG[0]-10, IMG_SIZE_BIG[1]-10, p=0.5),\n",
    "        albumentations.PadIfNeeded(min_height=IMG_SIZE_BIG[0], min_width=IMG_SIZE_BIG[1], value=0, p=1.0)\n",
    "    ])\n",
    "\n",
    "def getStftTransforms():\n",
    "    return albumentations.Compose([\n",
    "            albumentations.OneOf([\n",
    "                albumentations.GaussNoise(p=0.2),\n",
    "                albumentations.Cutout(num_holes=8, max_h_size=12, max_w_size=12, fill_value=0, p=0.2),\n",
    "            ], p=0.3),\n",
    "        albumentations.OpticalDistortion(p=0.3),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.05, rotate_limit=1, p=0.5),\n",
    "        albumentations.RandomCrop(IMG_SIZE_STFT[0]-10, IMG_SIZE_STFT[1]-10, p=0.5),\n",
    "        albumentations.PadIfNeeded(min_height=IMG_SIZE_STFT[0], min_width=IMG_SIZE_STFT[1], value=0, p=1.0)\n",
    "    ])\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 6. Generator\n",
    "\n",
    "# 5.1 MHA Model Data Generator\n",
    "class MHAVolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_sequences, batch_size, dict_labels, augmentations, shuffle=False, training=True):\n",
    "        super(MHAVolcanoSequencesGenerator, self).__init__()\n",
    "        \n",
    "        self.dict_means = {0: 0.09421943291597953, 1: 0.9208114415834104, 2: -0.026617075839858038, \n",
    "                           3: 0.09724443370400684, 4: 1.704695380910225, 5: -0.1180321202370159, 6: 0.7667902421713446, \n",
    "                           7: 0.7804286101804458, 8: -0.2075797991904395, 9: 0.014516944212624944} \n",
    "        \n",
    "        self.dict_stds =  {0: 1820.6211174856987, 1: 1931.0901612736805, 2: 1738.1671740163413, \n",
    "                           3: 1669.8837574619292, 4: 568.5221048211192, 5: 1848.4917466767877, 6: 1623.353060255481, \n",
    "                           7: 1618.2714709240895, 8: 1590.9403316558762, 9: 1906.41447528788}\n",
    "        \n",
    "        self.segments = segments\n",
    "        self.path_sequences = path_sequences\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.augmentations = augmentations\n",
    "        self.shuffle = shuffle\n",
    "        self.training = training\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        \n",
    "        array_sequences = np.asarray([np.load(self.path_sequences[segment], allow_pickle=True)[-SEQ_LENGTH:, :]\n",
    "                                     for segment in list_batch_segments])\n",
    "        \n",
    "        if self.augmentations:\n",
    "            array_sequences = self.augmentBatch(array_sequences)\n",
    "        \n",
    "        array_sequences[:, :, 0] = scale(array_sequences[:, :, 0], self.dict_means[0], self.dict_stds[0])\n",
    "        array_sequences[:, :, 1] = scale(array_sequences[:, :, 1], self.dict_means[1], self.dict_stds[1])\n",
    "        array_sequences[:, :, 2] = scale(array_sequences[:, :, 2], self.dict_means[2], self.dict_stds[2])\n",
    "        array_sequences[:, :, 3] = scale(array_sequences[:, :, 3], self.dict_means[3], self.dict_stds[3])\n",
    "        array_sequences[:, :, 4] = scale(array_sequences[:, :, 4], self.dict_means[4], self.dict_stds[4])\n",
    "        array_sequences[:, :, 5] = scale(array_sequences[:, :, 5], self.dict_means[5], self.dict_stds[5])\n",
    "        array_sequences[:, :, 6] = scale(array_sequences[:, :, 6], self.dict_means[6], self.dict_stds[6])\n",
    "        array_sequences[:, :, 7] = scale(array_sequences[:, :, 7], self.dict_means[7], self.dict_stds[7])\n",
    "        array_sequences[:, :, 8] = scale(array_sequences[:, :, 8], self.dict_means[8], self.dict_stds[8])\n",
    "        array_sequences[:, :, 9] = scale(array_sequences[:, :, 9], self.dict_means[9], self.dict_stds[9])\n",
    "        \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "            return array_sequences, array_labels\n",
    "        else:\n",
    "            return array_sequences\n",
    "        \n",
    "        \n",
    "    def noiseInjection(self, batch_sequences, noise_factor=0.075):\n",
    "        noise = np.random.randn(batch_sequences.shape[0], batch_sequences.shape[1], batch_sequences.shape[2])\n",
    "        augmented_data = batch_sequences + noise_factor * noise\n",
    "        return augmented_data\n",
    "    \n",
    "    \n",
    "    def timeShifting(self, batch_sequences, shift_max):\n",
    "        shift = np.random.randint(shift_max)\n",
    "        for sensor in range(10):\n",
    "            batch_sequences[:, :, sensor] = np.roll(batch_sequences[:, :, sensor], shift)\n",
    "        return batch_sequences\n",
    "       \n",
    "    \n",
    "    def augmentBatch(self, batch_sequences):\n",
    "        \n",
    "        # Add random noise\n",
    "        if np.random.random() > 0.5:\n",
    "            batch_sequences = self.noiseInjection(batch_sequences, noise_factor=0.005)   \n",
    "            \n",
    "        # Time shifting\n",
    "        if np.random.random() > 0.5:\n",
    "            batch_sequences = self.timeShifting(batch_sequences, shift_max=600) \n",
    "                           \n",
    "        # Shut-down sensor\n",
    "        if np.random.random() > 0.5:\n",
    "            sensor = np.random.randint(0, 9)\n",
    "            batch_sequences[:, :, sensor] = 0.0\n",
    "                \n",
    "        return batch_sequences\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "        \n",
    "\n",
    "# 5.2 Spectogram Model Data Generator\n",
    "class SpectoGramVolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_spectograms, batch_size, dict_labels, transforms, shuffle=False, training=True):\n",
    "        super(SpectoGramVolcanoSequencesGenerator, self).__init__()\n",
    "        self.segments = segments\n",
    "        self.path_spectograms = path_spectograms\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.shuffle = shuffle\n",
    "        self.training = training\n",
    "        self.transforms = transforms\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        array_spectograms_s0 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_0.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s1 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_1.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s2 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_2.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s3 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_3.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s4 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_4.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s5 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_5.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s6 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_6.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s7 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_7.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s8 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_8.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)\n",
    "        array_spectograms_s9 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_9.npy', \n",
    "                                                            allow_pickle=True)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        \n",
    "        if self.transforms:\n",
    "            data_s0, data_s1 = {'image':array_spectograms_s0}, {'image':array_spectograms_s1}\n",
    "            data_s2, data_s3 = {'image':array_spectograms_s2}, {'image':array_spectograms_s3}\n",
    "            data_s4, data_s5 = {'image':array_spectograms_s4}, {'image':array_spectograms_s5}\n",
    "            data_s6, data_s7 = {'image':array_spectograms_s6}, {'image':array_spectograms_s7}\n",
    "            data_s8, data_s9 = {'image':array_spectograms_s8}, {'image':array_spectograms_s9}\n",
    "            \n",
    "            array_spectograms_s0 = np.stack([self.transforms(image=x)['image'] for x in data_s0['image']], axis=0)\n",
    "            array_spectograms_s1 = np.stack([self.transforms(image=x)['image'] for x in data_s1['image']], axis=0)\n",
    "            array_spectograms_s2 = np.stack([self.transforms(image=x)['image'] for x in data_s2['image']], axis=0)\n",
    "            array_spectograms_s3 = np.stack([self.transforms(image=x)['image'] for x in data_s3['image']], axis=0)\n",
    "            array_spectograms_s4 = np.stack([self.transforms(image=x)['image'] for x in data_s4['image']], axis=0)\n",
    "            array_spectograms_s5 = np.stack([self.transforms(image=x)['image'] for x in data_s5['image']], axis=0)\n",
    "            array_spectograms_s6 = np.stack([self.transforms(image=x)['image'] for x in data_s6['image']], axis=0)\n",
    "            array_spectograms_s7 = np.stack([self.transforms(image=x)['image'] for x in data_s7['image']], axis=0)\n",
    "            array_spectograms_s8 = np.stack([self.transforms(image=x)['image'] for x in data_s8['image']], axis=0)\n",
    "            array_spectograms_s9 = np.stack([self.transforms(image=x)['image'] for x in data_s9['image']], axis=0)\n",
    "                 \n",
    "        batch = (array_spectograms_s0/255, array_spectograms_s1/255, array_spectograms_s2/255, array_spectograms_s3/255, \n",
    "                 array_spectograms_s4/255, array_spectograms_s5/255, array_spectograms_s6/255, array_spectograms_s7/255, \n",
    "                 array_spectograms_s8/255, array_spectograms_s9/255)    \n",
    "            \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "            return batch, array_labels\n",
    "        else:\n",
    "            return batch, None\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "# 5.3 STFT Model Data Generator\n",
    "class STFTVolcanoSequencesGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, segments, path_spectograms, batch_size, dict_labels, transforms, shuffle=False, training=True):\n",
    "        super(STFTVolcanoSequencesGenerator, self).__init__()\n",
    "        \n",
    "        self.segments = segments\n",
    "        self.path_spectograms = path_spectograms\n",
    "        self.batch_size = batch_size\n",
    "        self.dict_labels = dict_labels\n",
    "        self.transforms = transforms\n",
    "        self.shuffle = shuffle\n",
    "        self.training = training\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.num_steps = int(np.ceil(len(self.segments) / self.batch_size))\n",
    "        return self.num_steps\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        list_batch_segments = [self.segments[k] for k in indexes]\n",
    "        \n",
    "        array_spectograms_s0 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_0.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s1 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_1.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s2 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_2.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s3 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_3.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s4 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_4.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s5 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_5.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s6 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_6.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s7 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_7.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0)\n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s8 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_8.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8) \n",
    "        array_spectograms_s9 = np.asarray([np.load(f'{self.path_spectograms[segment]}{segment}_9.npy', \n",
    "                                                            allow_pickle=True).transpose(1, 0) \n",
    "                                     for segment in list_batch_segments]).astype(np.uint8)  \n",
    "        \n",
    "        if self.transforms:\n",
    "            data_s0, data_s1 = {'image':array_spectograms_s0}, {'image':array_spectograms_s1}\n",
    "            data_s2, data_s3 = {'image':array_spectograms_s2}, {'image':array_spectograms_s3}\n",
    "            data_s4, data_s5 = {'image':array_spectograms_s4}, {'image':array_spectograms_s5}\n",
    "            data_s6, data_s7 = {'image':array_spectograms_s6}, {'image':array_spectograms_s7}\n",
    "            data_s8, data_s9 = {'image':array_spectograms_s8}, {'image':array_spectograms_s9}\n",
    "            \n",
    "            array_spectograms_s0 = np.stack([self.transforms(image=x)['image'] for x in data_s0['image']], axis=0)\n",
    "            array_spectograms_s1 = np.stack([self.transforms(image=x)['image'] for x in data_s1['image']], axis=0)\n",
    "            array_spectograms_s2 = np.stack([self.transforms(image=x)['image'] for x in data_s2['image']], axis=0)\n",
    "            array_spectograms_s3 = np.stack([self.transforms(image=x)['image'] for x in data_s3['image']], axis=0)\n",
    "            array_spectograms_s4 = np.stack([self.transforms(image=x)['image'] for x in data_s4['image']], axis=0)\n",
    "            array_spectograms_s5 = np.stack([self.transforms(image=x)['image'] for x in data_s5['image']], axis=0)\n",
    "            array_spectograms_s6 = np.stack([self.transforms(image=x)['image'] for x in data_s6['image']], axis=0)\n",
    "            array_spectograms_s7 = np.stack([self.transforms(image=x)['image'] for x in data_s7['image']], axis=0)\n",
    "            array_spectograms_s8 = np.stack([self.transforms(image=x)['image'] for x in data_s8['image']], axis=0)\n",
    "            array_spectograms_s9 = np.stack([self.transforms(image=x)['image'] for x in data_s9['image']], axis=0)\n",
    "                 \n",
    "        batch = (array_spectograms_s0/255, array_spectograms_s1/255, array_spectograms_s2/255, array_spectograms_s3/255, \n",
    "                 array_spectograms_s4/255, array_spectograms_s5/255, array_spectograms_s6/255, array_spectograms_s7/255, \n",
    "                 array_spectograms_s8/255, array_spectograms_s9/255)       \n",
    "            \n",
    "        if self.training:\n",
    "            array_labels = np.asarray([self.dict_labels[segment] for segment in list_batch_segments])\n",
    "            return batch, array_labels\n",
    "        else:\n",
    "            return batch, None\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.segments))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "\n",
    "# 5.4\n",
    "def buildDataset(dict_paths):\n",
    "            \n",
    "    fs = 100                # sampling frequency \n",
    "    n = 256                 # FFT segment size\n",
    "    max_f = 20              # ï½ž20Hz\n",
    "\n",
    "    delta_f = fs / n        # 0.39Hz\n",
    "    delta_t = n / fs / 2    # 1.28s\n",
    "\n",
    "    feature_set = []\n",
    "    for segment_id in tqdm(dict_paths, total=len(dict_paths), position=0):\n",
    "        data = np.load(dict_paths[segment_id])\n",
    "        segment = [segment_id]\n",
    "        for sensor in range(10):\n",
    "            x = data[:, sensor]\n",
    "            f, t, Z = scipy.signal.stft(x, fs = fs, window = 'hann', nperseg = n)\n",
    "            f = f[:round(max_f/delta_f)+1]\n",
    "            Z = np.abs(Z[:round(max_f/delta_f)+1]).T\n",
    "\n",
    "            th = Z.mean() * 1 \n",
    "            Z_pow = Z.copy()\n",
    "            Z_pow[Z < th] = 0\n",
    "            Z_num = Z_pow.copy()\n",
    "            Z_num[Z >= th] = 1\n",
    "\n",
    "            Z_pow_sum = Z_pow.sum(axis = 0)\n",
    "            Z_num_sum = Z_num.sum(axis = 0)\n",
    "\n",
    "            A_pow = Z_pow_sum[round(10/delta_f):].sum()\n",
    "            A_num = Z_num_sum[round(10/delta_f):].sum()\n",
    "            BH_pow = Z_pow_sum[round(5/delta_f):round(8/delta_f)].sum()\n",
    "            BH_num = Z_num_sum[round(5/delta_f):round(8/delta_f)].sum()\n",
    "            BL_pow = Z_pow_sum[round(1.5/delta_f):round(2.5/delta_f)].sum()\n",
    "            BL_num = Z_num_sum[round(1.5/delta_f):round(2.5/delta_f)].sum()\n",
    "            C_pow = Z_pow_sum[round(0.6/delta_f):round(1.2/delta_f)].sum()\n",
    "            C_num = Z_num_sum[round(0.6/delta_f):round(1.2/delta_f)].sum()\n",
    "            D_pow = Z_pow_sum[round(2/delta_f):round(4/delta_f)].sum()\n",
    "            D_num = Z_num_sum[round(2/delta_f):round(4/delta_f)].sum()\n",
    "            segment += [A_pow, A_num, BH_pow, BH_num, BL_pow, BL_num, C_pow, C_num, D_pow, D_num]\n",
    "\n",
    "        feature_set.append(segment)\n",
    "\n",
    "    cols = ['segment_id']\n",
    "    for i in range(10):\n",
    "        for j in ['A_pow', 'A_num','BH_pow', 'BH_num','BL_pow', 'BL_num','C_pow', 'C_num','D_pow', 'D_num']:\n",
    "            cols += [f's{i+1}_{j}']\n",
    "    feature_df = pd.DataFrame(feature_set, columns = cols)\n",
    "    feature_df['segment_id'] = feature_df['segment_id'].astype('int')\n",
    "    \n",
    "    return feature_df\n",
    "    \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e9e63dda5849079d1e3000c4a4cf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "lgbm_stft\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97eb490e0060427697a5372c5bf522c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "cnn2d_stft\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "big\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "tiny\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "mha\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 8. Build Test Set\n",
    "\n",
    "qt_augmentations = 10\n",
    "batch_size = 8\n",
    "\n",
    "pbar = tqdm(total=(len(path_models_mha)*5), position=0)\n",
    "list_segments_test = list(unique_segments_id_test)\n",
    "\n",
    "paths = (path_models_lgbm_stft, path_models_spectogram_stft,\n",
    "         path_models_spectogram_big, path_models_spectogram_tiny, path_models_mha)\n",
    "\n",
    "for path in paths:\n",
    "    dict_predictions = {'segment_id': {}}\n",
    "    dict_predictions['segment_id'] = list(unique_segments_id_test)\n",
    "    type_model = '_'.join(path[0].split('_')[-3:-1]).lower() if path[0].split('_')[-2].lower() == 'stft' else path[0].split('_')[-2].lower()\n",
    "    if type_model in {'tiny', 'big', 'mha', 'cnn2d_stft'}:\n",
    "        list_models = [models.load_model(sub_path, compile=False) for sub_path in path]\n",
    "        \n",
    "    print('==='*30)\n",
    "    print(type_model)\n",
    "    print('==='*30)\n",
    "    \n",
    "    if type_model in {'tiny'}:\n",
    "        X_generator = SpectoGramVolcanoSequencesGenerator(list(dict_segments_spectograms_tiny_paths_test), \n",
    "                                                              dict_segments_spectograms_tiny_paths_test,\n",
    "                                                              batch_size=batch_size, dict_labels=dict_labels,\n",
    "                                                              transforms=getTinyTransforms(),\n",
    "                                                              training=False, shuffle=False)     \n",
    "    elif type_model in {'big'}:\n",
    "        X_generator = SpectoGramVolcanoSequencesGenerator(list(dict_segments_spectograms_big_paths_test), \n",
    "                                                              dict_segments_spectograms_big_paths_test,\n",
    "                                                              batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                              transforms=getBigTransforms(),\n",
    "                                                              training=False, shuffle=False)\n",
    "\n",
    "    elif type_model in {'mha'}:\n",
    "        X_generator = MHAVolcanoSequencesGenerator(list(dict_segments_sequences_paths_test), \n",
    "                                                      dict_segments_sequences_paths_test,\n",
    "                                                      batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                      augmentations=True, training=False, shuffle=False)   \n",
    "    elif type_model in {'cnn2d_stft'}:\n",
    "        X_generator = STFTVolcanoSequencesGenerator(list(dict_segments_spectograms_stft_paths_test), \n",
    "                                                    dict_segments_spectograms_stft_paths_test,\n",
    "                                                    batch_size=batch_size, dict_labels=dict_labels, \n",
    "                                                    transforms=getStftTransforms(), training=False, shuffle=False)\n",
    "    elif type_model in {'lgbm_stft'}:\n",
    "         pass\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Incorrect Type model, {type_model}')\n",
    "\n",
    "\n",
    "    list_test_segments = list(unique_segments_id_test)\n",
    "    array_predictions = np.zeros((len(list_test_segments)), dtype=np.float32)\n",
    "    array_confidence = np.zeros((len(list_test_segments)), dtype=np.float32)\n",
    "    \n",
    "    if type_model not in {'lgbm_stft'}:\n",
    "        array_predictions_std = np.zeros((len(list_test_segments)), dtype=np.float32)\n",
    "        array_confidence_std = np.zeros((len(list_test_segments)), dtype=np.float32)\n",
    "        list_predictions, list_conf = [], []\n",
    "        for i in range(qt_augmentations):\n",
    "            list_model_preds, list_model_confs = [], [] \n",
    "            for model in list_models:\n",
    "                preds = model.predict(X_generator)\n",
    "                list_model_preds.append(preds[0])\n",
    "                list_model_confs.append(preds[1])\n",
    "            preds = np.asarray(list_model_preds).mean(axis=0)\n",
    "            confs = np.asarray(list_model_confs).mean(axis=0)\n",
    "            list_predictions.append(preds)\n",
    "            list_conf.append(confs[:, 2] - confs[:, 0])\n",
    "\n",
    "        array_predictions = np.asarray(list_predictions).mean(axis=0).squeeze().astype(np.float32)\n",
    "        array_confidence = np.asarray(list_conf).mean(axis=0).squeeze().astype(np.float32)\n",
    "        array_predictions_std = np.asarray(list_predictions).std(axis=0).squeeze().astype(np.float32)\n",
    "        array_confidence_std = np.asarray(list_conf).std(axis=0).squeeze().astype(np.float32)\n",
    "\n",
    "        dict_predictions[type_model] = {\n",
    "            'pred_mean': array_predictions, 'pred_std': array_predictions_std,\n",
    "            'conf_mean' : array_confidence, 'conf_std' : array_confidence_std\n",
    "        }\n",
    "        \n",
    "        dict_build_test_df = {\n",
    "            'segment_id' : list_test_segments,\n",
    "            f'{type_model}_pred_mean' : dict_predictions[type_model]['pred_mean'].squeeze(),\n",
    "            f'{type_model}_pred_std' : dict_predictions[type_model]['pred_std'].squeeze(),\n",
    "            f'{type_model}_conf_mean' : dict_predictions[type_model]['conf_mean'].squeeze(),\n",
    "            f'{type_model}_conf_std' : dict_predictions[type_model]['conf_std'].squeeze()\n",
    "        }\n",
    "        \n",
    "            \n",
    "    else: \n",
    "        df_X_test = buildDataset(dict_segments_sequences_paths_test)\n",
    "        features = [col for col in df_X_test.columns.tolist() if col not in ['segment_id', 'time_to_eruption']]\n",
    "        X_test = df_X_test[features]\n",
    "        list_models = [pickle.load(open(f'{sub_path}.pickle', 'rb')) for sub_path in path]\n",
    "        array_predictions = np.mean([model.predict(X_test) for model in list_models], 0)\n",
    "        array_confidence[:] = 1.0\n",
    "\n",
    "        dict_predictions[type_model] = {'pred' : array_predictions, 'conf' : array_confidence}\n",
    "        dict_build_test_df = {\n",
    "            'segment_id' : list_test_segments,\n",
    "            f'{type_model}_pred' : dict_predictions[type_model]['pred'].squeeze(),\n",
    "            f'{type_model}_conf' : dict_predictions[type_model]['conf'].squeeze()\n",
    "        }\n",
    "        \n",
    "        del df_X_test, X_test\n",
    "        gc.collect()\n",
    "    \n",
    "    df_test_tmp = pd.DataFrame(dict_build_test_df)\n",
    "    df_test_tmp.to_csv(f'./df_test_l2_{type_model}.csv', index=False)\n",
    "    tf.keras.backend.clear_session()\n",
    "    del list_models, array_predictions\n",
    "    gc.collect()\n",
    "    \n",
    "    pbar.update(5)\n",
    "    \n",
    "        \n",
    "pbar.close()\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>time_to_eruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.520000e+03</td>\n",
       "      <td>4.520000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.066993e+09</td>\n",
       "      <td>2.395877e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.162904e+08</td>\n",
       "      <td>1.166302e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.602880e+05</td>\n",
       "      <td>4.135358e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.458995e+08</td>\n",
       "      <td>1.469955e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.060695e+09</td>\n",
       "      <td>2.448890e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.599284e+09</td>\n",
       "      <td>3.347515e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147116e+09</td>\n",
       "      <td>4.788298e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         segment_id  time_to_eruption\n",
       "count  4.520000e+03      4.520000e+03\n",
       "mean   1.066993e+09      2.395877e+07\n",
       "std    6.162904e+08      1.166302e+07\n",
       "min    8.602880e+05      4.135358e+05\n",
       "25%    5.458995e+08      1.469955e+07\n",
       "50%    1.060695e+09      2.448890e+07\n",
       "75%    1.599284e+09      3.347515e+07\n",
       "max    2.147116e+09      4.788298e+07"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_submission = pd.DataFrame({\n",
    "#     'segment_id' : df_test_tmp['segment_id'],\n",
    "#     'time_to_eruption' : df_test_tmp['mha_pred_mean']*(10**6)\n",
    "# })\n",
    "\n",
    "# df_submission.to_csv('./99_Submissions_tta/' + 'submission_tta_mha.csv', index=False)\n",
    "# df_submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4520 4520 4520\n",
      "4520 4520 4520\n",
      "[17:27:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:27:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:27:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:27:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:27:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 9. Make inference\n",
    "\n",
    "experiment_name = '1.2849_27-12-2020-17-15-23'\n",
    "type_models = ['mha', 'big', 'tiny', 'cnn2d_stft', 'lgbm_stft']\n",
    "list_dfs= [f'df_test_l2_{arch}.csv' for arch in type_models]\n",
    "\n",
    "for i, name_df_ in enumerate(list_dfs):\n",
    "    df_ = pd.read_csv(f'./{name_df_}', index_col=False)\n",
    "    if name_df_.split('_')[-1].split('.')[0] in {'tiny', 'big', 'stft'}:\n",
    "        cols_ = [df_.columns[0]] + [col for col in list(df_.columns[1:])]\n",
    "        df_.columns = cols_\n",
    "    if i==0:\n",
    "        df_test = df_.copy()\n",
    "    else:\n",
    "        df_test = pd.merge(df_test, df_, how='inner', on='segment_id')\n",
    "\n",
    "#####\n",
    "\n",
    "df_tabular_time = pd.read_csv('./df_time_tabular_predictions.csv')\n",
    "df_tabular_stft2 = pd.read_csv('./df_stft_tabular_predictions.csv')\n",
    "\n",
    "df_tabular_time.columns = ['segment_id', 'lgbm_time_pred_mean']\n",
    "df_tabular_stft2.columns = ['segment_id', 'lgbm_stft_pred_mean']\n",
    "\n",
    "df_tabular_time['lgbm_time_pred_mean'] = df_tabular_time['lgbm_time_pred_mean']/(10**6)\n",
    "df_tabular_stft2['lgbm_stft_pred_mean'] = df_tabular_stft2['lgbm_stft_pred_mean']/(10**6)\n",
    "\n",
    "df_added = pd.merge(df_tabular_time, df_tabular_stft2, how='inner', on=['segment_id'])\n",
    "print(df_added.shape[0], df_tabular_time.shape[0], df_tabular_stft2.shape[0])\n",
    "df_all_test = pd.merge(df_test, df_added, how='inner', on=['segment_id'])\n",
    "print(df_all_test.shape[0], df_added.shape[0], df_test.shape[0])\n",
    "\n",
    "#####\n",
    "\n",
    "list_test_columns = [col.replace('cnn2d', 'cnn') if col.split('_')[0]=='cnn2d' else col for col in list(df_all_test.columns)]\n",
    "list_test_columns = ['cnn_' + col if col.split('_')[0] in ['tiny', 'big'] else col for col in list_test_columns]\n",
    "df_all_test.columns =  list_test_columns       \n",
    "        \n",
    "list_train_columns = [col for col in list(pd.read_csv(f'./df_all_train_l2.csv', index_col=False).columns) if col not in ['y_true']]            \n",
    "df_all_test = df_all_test[list_train_columns]\n",
    "\n",
    "test_columns = [col for col in df_all_test.columns if col not in ['segment_id']]\n",
    "\n",
    "# XGBBOOST\n",
    "list_models_xgb = [pickle.load(open(f'./StackedModels/{experiment_name}/xgb_{fold}.pickle', 'rb')) for fold in range(5)]\n",
    "y_test_pred = np.asarray([model_xgb.predict(df_all_test[test_columns]) for model_xgb in list_models_xgb])\n",
    "df_all_test['xgb'] = y_test_pred.mean(axis=0)\n",
    "\n",
    "# CATBOOST\n",
    "list_models_cat = [pickle.load(open(f'./StackedModels/{experiment_name}/cat_{fold}.pickle', 'rb')) for fold in range(5)]\n",
    "y_test_pred = np.asarray([model_cat.predict(df_all_test[test_columns]) for model_cat in list_models_cat])\n",
    "df_all_test['cat'] = y_test_pred.mean(axis=0)\n",
    "\n",
    "# TABNET\n",
    "list_models_tab = [torch.load(f'./StackedModels/{experiment_name}/tabnet_{fold}') for fold in range(5)]\n",
    "y_test_pred = np.asarray([model_tab.predict(df_all_test[test_columns].values) for model_tab in list_models_tab])\n",
    "df_all_test['tab'] = y_test_pred.mean(axis=0)\n",
    "\n",
    "df_all_test.to_csv(f'./df_test_all.csv', index=False)\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>time_to_eruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.520000e+03</td>\n",
       "      <td>4.520000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.066993e+09</td>\n",
       "      <td>2.447001e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.162904e+08</td>\n",
       "      <td>1.138921e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.602880e+05</td>\n",
       "      <td>6.578686e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.458995e+08</td>\n",
       "      <td>1.736105e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.060695e+09</td>\n",
       "      <td>2.509899e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.599284e+09</td>\n",
       "      <td>3.288864e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147116e+09</td>\n",
       "      <td>4.759814e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         segment_id  time_to_eruption\n",
       "count  4.520000e+03      4.520000e+03\n",
       "mean   1.066993e+09      2.447001e+07\n",
       "std    6.162904e+08      1.138921e+07\n",
       "min    8.602880e+05      6.578686e+05\n",
       "25%    5.458995e+08      1.736105e+07\n",
       "50%    1.060695e+09      2.509899e+07\n",
       "75%    1.599284e+09      3.288864e+07\n",
       "max    2.147116e+09      4.759814e+07"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################\n",
    "# 10. Submission\n",
    "\n",
    "df_all_test = pd.read_csv('./df_test_all.csv')\n",
    "df_submission = pd.DataFrame({\n",
    "    'segment_id' : df_all_test['segment_id'],\n",
    "    # 'time_to_eruption' : (0.2*df_all_test['xgb'] + 0.2*df_all_test['cat'] + 0.6*df_all_test['tab'])*(10**6),\n",
    "    'time_to_eruption' : np.mean([df_all_test['xgb'], df_all_test['cat'], df_all_test['tab']], axis=0)*(10**6)\n",
    "})\n",
    "sub_path = '../FinalSubmissions/' + 'submission_l2_final_2.csv'\n",
    "df_submission.to_csv(sub_path, index=False)\n",
    "df_submission.describe()\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>time_to_eruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.520000e+03</td>\n",
       "      <td>4.520000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.066993e+09</td>\n",
       "      <td>2.344242e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.162904e+08</td>\n",
       "      <td>1.050946e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.602880e+05</td>\n",
       "      <td>1.036858e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.458995e+08</td>\n",
       "      <td>1.618550e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.060695e+09</td>\n",
       "      <td>2.653251e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.599284e+09</td>\n",
       "      <td>3.090897e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147116e+09</td>\n",
       "      <td>4.729145e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         segment_id  time_to_eruption\n",
       "count  4.520000e+03      4.520000e+03\n",
       "mean   1.066993e+09      2.344242e+07\n",
       "std    6.162904e+08      1.050946e+07\n",
       "min    8.602880e+05      1.036858e+06\n",
       "25%    5.458995e+08      1.618550e+07\n",
       "50%    1.060695e+09      2.653251e+07\n",
       "75%    1.599284e+09      3.090897e+07\n",
       "max    2.147116e+09      4.729145e+07"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check preds\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'./df_test_all.csv')\n",
    "\n",
    "df[['segment_id', 'cnn_tiny_pred_mean']]\n",
    "\n",
    "df_submission = pd.DataFrame({\n",
    "    'segment_id' : df['segment_id'],\n",
    "    'time_to_eruption' : df['cnn_tiny_pred_mean'] * (10**6)\n",
    "})\n",
    "\n",
    "sub_path = '../FinalSubmissions/' + 'check_cnn_tiny.csv'\n",
    "df_submission.to_csv(sub_path, index=False)\n",
    "df_submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c predict-volcanic-eruptions-ingv-oe -f {sub_path} -m experiment_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
