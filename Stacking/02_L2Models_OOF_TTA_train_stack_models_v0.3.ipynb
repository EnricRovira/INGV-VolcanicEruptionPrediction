{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 1. Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "import torch\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 2. Paths & Global Variables\n",
    "\n",
    "## 2.1 Paths\n",
    "\n",
    "path = '../../../01_Data/'\n",
    "path_sequences = path + '01_GeneratedSequences/'\n",
    "path_spectograms_tiny = path + '02_GeneratedSpectograms_MelFeatures/'\n",
    "path_spectograms_big = path + '03_GeneratedSpectograms_Scipy/'\n",
    "\n",
    "path_models_mha = [f'../models/model_mha_{num_fold}' for num_fold in range(5)]\n",
    "path_models_spectogram_tiny = [f'../models/model_cnn2d_Tiny_{num_fold}' for num_fold in range(5)]\n",
    "path_models_spectogram_big = [f'../models/model_cnn2d_Big_{num_fold}' for num_fold in range(5)]\n",
    "\n",
    "df_train_l2 = pd.read_csv('./df_all_train_l2.csv')\n",
    "df_train = pd.read_csv(path + 'train.csv')\n",
    "df_sample_submission = pd.read_csv(path + 'sample_submission.csv') \n",
    "\n",
    "\n",
    "unique_segments_id_train = set(df_train['segment_id'])\n",
    "unique_segments_id_test = set(df_sample_submission['segment_id'])\n",
    "\n",
    "dict_labels = {\n",
    "    segment : df_train['time_to_eruption'][df_train['segment_id']==segment].values.flatten()[0]\n",
    "    for segment in unique_segments_id_train\n",
    "}\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 3. Global Functions\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "        \n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing XGB Grid search.....\n",
      "Fitting 5 folds for each of 12150 candidates, totalling 60750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=18)]: Using backend LokyBackend with 18 concurrent workers.\n",
      "[Parallel(n_jobs=18)]: Done  14 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=18)]: Done 164 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=18)]: Done 414 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=18)]: Done 764 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=18)]: Done 1214 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=18)]: Done 1764 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=18)]: Done 2414 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=18)]: Done 3164 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=18)]: Done 4014 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=18)]: Done 4964 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=18)]: Done 6014 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=18)]: Done 7164 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=18)]: Done 8414 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=18)]: Done 9764 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=18)]: Done 11214 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=18)]: Done 12764 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=18)]: Done 14414 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=18)]: Done 16164 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=18)]: Done 18014 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=18)]: Done 19964 tasks      | elapsed: 29.5min\n",
      "[Parallel(n_jobs=18)]: Done 22014 tasks      | elapsed: 32.8min\n",
      "[Parallel(n_jobs=18)]: Done 24164 tasks      | elapsed: 36.3min\n",
      "[Parallel(n_jobs=18)]: Done 26414 tasks      | elapsed: 39.4min\n",
      "[Parallel(n_jobs=18)]: Done 28764 tasks      | elapsed: 42.7min\n",
      "[Parallel(n_jobs=18)]: Done 31214 tasks      | elapsed: 46.4min\n",
      "[Parallel(n_jobs=18)]: Done 33764 tasks      | elapsed: 50.5min\n",
      "[Parallel(n_jobs=18)]: Done 36414 tasks      | elapsed: 54.8min\n",
      "[Parallel(n_jobs=18)]: Done 39164 tasks      | elapsed: 58.6min\n",
      "[Parallel(n_jobs=18)]: Done 42014 tasks      | elapsed: 62.7min\n",
      "[Parallel(n_jobs=18)]: Done 44964 tasks      | elapsed: 67.2min\n",
      "[Parallel(n_jobs=18)]: Done 48014 tasks      | elapsed: 72.2min\n",
      "[Parallel(n_jobs=18)]: Done 51164 tasks      | elapsed: 76.7min\n",
      "[Parallel(n_jobs=18)]: Done 54414 tasks      | elapsed: 81.4min\n",
      "[Parallel(n_jobs=18)]: Done 57764 tasks      | elapsed: 86.6min\n",
      "[Parallel(n_jobs=18)]: Done 60750 out of 60750 | elapsed: 91.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:50:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "\n",
      " Score:\n",
      "-1.3555247960408694\n",
      "\n",
      " Best hyperparameters:\n",
      "{'alpha': 0.5, 'colsample_bytree': 0.9, 'gamma': 2.0, 'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 0.0, 'n_estimators': 200, 'subsample': 0.8}\n",
      "XGB: 1.026411279699399\n",
      "MHA: 2.4882792013633526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enric\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cnn_tiny_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cnn_tiny_pred'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e66c0fe8a164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"XGB: {np.mean(np.abs(X['y_true'] - X['xgb']))}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"MHA: {np.mean(np.abs(X['y_true'] - X['mha_pred_mean']))}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"CNN_TINY: {np.mean(np.abs(X['y_true'] - X['cnn_tiny_pred']))}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"CNN_BIG: {np.mean(np.abs(X['y_true'] - X['cnn_big_pred']))}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"MEAN MHA+TINY+BIG: {np.mean(np.abs(X['y_true'] - ((X['mha_pred_mean'] + X['cnn_tiny_pred'] + X['cnn_big_pred'])/3)))}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cnn_tiny_pred'"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 5. Grid Search\n",
    "\n",
    "grid_xgb = True\n",
    "grid_catb = False\n",
    "\n",
    "# 5.1 XGB 1h\n",
    "\n",
    "train_columns = [col for col in df_train_l2.columns if col not in ['y_true', 'segment_id']]\n",
    "X, y = df_train_l2[train_columns], df_train_l2['y_true']\n",
    "\n",
    "# 90mins\n",
    "if grid_xgb:\n",
    "    print('Executing XGB Grid search.....')\n",
    "    xgb_grid_params = {\n",
    "            'n_estimators' : [100, 150, 200],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'min_child_weight': [0.0, 0.2],\n",
    "            'alpha': [0.5, 1.0, 2.0, 2.5, 3.0],\n",
    "            'gamma': [0.5, 1.0, 2.0, 2.5, 3.0],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0], \n",
    "            'max_depth': [4, 5, 6]\n",
    "    }\n",
    "\n",
    "    xgb_model_grid = xgb.XGBRegressor(\n",
    "        objective= 'reg:linear',\n",
    "        nthread=1,\n",
    "        seed=12\n",
    "    )\n",
    "\n",
    "    scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model_grid,\n",
    "        param_grid=xgb_grid_params,\n",
    "        scoring = 'neg_mean_absolute_error',\n",
    "        n_jobs = 18,\n",
    "        cv = 5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print('\\n Score:')\n",
    "    print(grid_search.best_score_)\n",
    "    print('\\n Best hyperparameters:')\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    y_pred = grid_search.predict(X)\n",
    "\n",
    "    X['xgb'] = y_pred\n",
    "    X['y_true'] = y\n",
    "    X = X.reset_index(drop=True)\n",
    "\n",
    "    print(f\"XGB: {np.mean(np.abs(X['y_true'] - X['xgb']))}\")\n",
    "    print(f\"MHA: {np.mean(np.abs(X['y_true'] - X['mha_pred_mean']))}\")\n",
    "    print(f\"CNN_TINY: {np.mean(np.abs(X['y_true'] - X['cnn_tiny_pred_mean']))}\")\n",
    "    print(f\"CNN_BIG: {np.mean(np.abs(X['y_true'] - X['cnn_big_pred_mean']))}\")\n",
    "    print(f\"MEAN MHA+TINY+BIG: {np.mean(np.abs(X['y_true'] - ((X['mha_pred_mean'] + X['cnn_tiny_pred_mean'] + X['cnn_big_pred_mean'])/3)))}\")\n",
    "\n",
    "\n",
    "# 5.2 Catboost 1h\n",
    "# 200mins\n",
    "if grid_catb:\n",
    "    print('Executing CATBOOST Grid search.....')\n",
    "    cat_grid_params = {\n",
    "        'learning_rate' : [0.01, 0.025, 0.05, 0.1],\n",
    "        'iterations': [300, 400, 500, 600, 700, 800],\n",
    "        'depth': [3, 4, 5, 6],\n",
    "        'l2_leaf_reg': [10, 15, 20, 30, 40],\n",
    "        'bagging_temperature': [0.2, 0.3],\n",
    "        'colsample_bylevel' : [0.7, 0.8, 0.9]\n",
    "        # 'leaf_estimation_iterations': [10],\n",
    "    }\n",
    "    \n",
    "    cat_model_grid = catboost.CatBoostRegressor(\n",
    "        eval_metric='MAE',\n",
    "        random_seed=12,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=cat_model_grid,\n",
    "        param_grid=cat_grid_params,\n",
    "        scoring = 'neg_mean_absolute_error',\n",
    "        n_jobs = 16,\n",
    "        cv = 5,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print('\\n Score:')\n",
    "    print(grid_search.best_score_)\n",
    "    print('\\n Best hyperparameters:')\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    y_pred = grid_search.predict(X)\n",
    "    X['cat'] = y_pred\n",
    "    X = X.reset_index(drop=True)\n",
    "\n",
    "    print(f\"CAT: {np.mean(np.abs(X['y_true'] - X['cat']))}\")\n",
    "    print(f\"MHA: {np.mean(np.abs(X['y_true'] - X['mha_pred_mean']))}\")\n",
    "    print(f\"CNN_TINY: {np.mean(np.abs(X['y_true'] - X['cnn_tiny_pred_mean']))}\")\n",
    "    print(f\"CNN_BIG: {np.mean(np.abs(X['y_true'] - X['cnn_big_pred_mean']))}\")\n",
    "    print(f\"MEAN MHA+TINY+BIG: {np.mean(np.abs(X['y_true'] - ((X['mha_pred_mean'] + X['cnn_tiny_pred_mean'] + X['cnn_big_pred_mean'])/3)))}\")\n",
    "\n",
    "\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'alpha': 0.5, 'colsample_bytree': 0.8, 'gamma': 2.0, 'learning_rate': 0.05, 'max_depth': 4, \n",
    "#  'min_child_weight': 0.0, 'n_estimators': 150, 'subsample': 0.8}\n",
    "\n",
    "# {'alpha': 0.5, 'colsample_bytree': 0.9, 'gamma': 2.0, 'learning_rate': 0.05, 'max_depth': 4, \n",
    "#  'min_child_weight': 0.0, 'n_estimators': 200, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabnetMAE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"mae\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        mae = mean_absolute_error(y_true, y_score[:, 1])\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Num Fold: 1\n",
      "Train segments: 3544 Val segments: 887\n",
      "============================================================\n",
      "Device used : cuda\n",
      "Training xgboost\n",
      "[19:17:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:17:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Training catboost\n",
      "Training TabNet\n",
      "epoch 0  | loss: 23.59666| val_0_mae: 25.82856|  0:00:02s\n",
      "epoch 1  | loss: 22.58278| val_0_mae: 23.40626|  0:00:02s\n",
      "epoch 2  | loss: 21.94629| val_0_mae: 20.02263|  0:00:02s\n",
      "epoch 3  | loss: 21.25004| val_0_mae: 12.88641|  0:00:03s\n",
      "epoch 4  | loss: 20.31042| val_0_mae: 12.62921|  0:00:03s\n",
      "epoch 5  | loss: 19.13432| val_0_mae: 9.55822 |  0:00:03s\n",
      "epoch 6  | loss: 17.65253| val_0_mae: 8.84495 |  0:00:03s\n",
      "epoch 7  | loss: 15.90367| val_0_mae: 9.23727 |  0:00:03s\n",
      "epoch 8  | loss: 13.92524| val_0_mae: 9.3713  |  0:00:04s\n",
      "epoch 9  | loss: 11.56686| val_0_mae: 9.57456 |  0:00:04s\n",
      "epoch 10 | loss: 8.92784 | val_0_mae: 9.84163 |  0:00:04s\n",
      "epoch 11 | loss: 6.39669 | val_0_mae: 15.86592|  0:00:04s\n",
      "epoch 12 | loss: 4.15487 | val_0_mae: 17.36166|  0:00:04s\n",
      "epoch 13 | loss: 3.19851 | val_0_mae: 18.8098 |  0:00:05s\n",
      "epoch 14 | loss: 3.03084 | val_0_mae: 15.27743|  0:00:05s\n",
      "epoch 15 | loss: 2.78174 | val_0_mae: 13.26161|  0:00:05s\n",
      "epoch 16 | loss: 2.45241 | val_0_mae: 11.59498|  0:00:05s\n",
      "epoch 17 | loss: 2.42575 | val_0_mae: 10.90607|  0:00:05s\n",
      "epoch 18 | loss: 2.37566 | val_0_mae: 11.55752|  0:00:06s\n",
      "epoch 19 | loss: 2.41577 | val_0_mae: 11.13835|  0:00:06s\n",
      "epoch 20 | loss: 2.23663 | val_0_mae: 10.69807|  0:00:06s\n",
      "epoch 21 | loss: 2.27494 | val_0_mae: 10.15239|  0:00:06s\n",
      "epoch 22 | loss: 2.17063 | val_0_mae: 10.68627|  0:00:06s\n",
      "epoch 23 | loss: 2.18128 | val_0_mae: 8.32599 |  0:00:07s\n",
      "epoch 24 | loss: 2.14669 | val_0_mae: 9.48792 |  0:00:07s\n",
      "epoch 25 | loss: 2.06222 | val_0_mae: 8.55921 |  0:00:07s\n",
      "epoch 26 | loss: 2.14512 | val_0_mae: 8.73362 |  0:00:07s\n",
      "epoch 27 | loss: 2.03728 | val_0_mae: 7.93033 |  0:00:07s\n",
      "epoch 28 | loss: 2.01314 | val_0_mae: 8.67046 |  0:00:07s\n",
      "epoch 29 | loss: 2.09223 | val_0_mae: 8.66748 |  0:00:08s\n",
      "epoch 30 | loss: 2.05894 | val_0_mae: 7.18865 |  0:00:08s\n",
      "epoch 31 | loss: 2.05883 | val_0_mae: 6.87856 |  0:00:08s\n",
      "epoch 32 | loss: 2.03641 | val_0_mae: 6.94167 |  0:00:08s\n",
      "epoch 33 | loss: 2.05942 | val_0_mae: 6.95137 |  0:00:09s\n",
      "epoch 34 | loss: 1.94034 | val_0_mae: 6.49547 |  0:00:09s\n",
      "epoch 35 | loss: 1.86752 | val_0_mae: 6.55918 |  0:00:09s\n",
      "epoch 36 | loss: 1.8618  | val_0_mae: 6.80171 |  0:00:09s\n",
      "epoch 37 | loss: 1.94337 | val_0_mae: 5.47768 |  0:00:09s\n",
      "epoch 38 | loss: 1.89465 | val_0_mae: 5.565   |  0:00:09s\n",
      "epoch 39 | loss: 1.90323 | val_0_mae: 5.84999 |  0:00:10s\n",
      "epoch 40 | loss: 1.95883 | val_0_mae: 5.76133 |  0:00:10s\n",
      "epoch 41 | loss: 1.94805 | val_0_mae: 6.00318 |  0:00:10s\n",
      "epoch 42 | loss: 1.80968 | val_0_mae: 4.77011 |  0:00:10s\n",
      "epoch 43 | loss: 2.00088 | val_0_mae: 3.76917 |  0:00:10s\n",
      "epoch 44 | loss: 1.91277 | val_0_mae: 4.57311 |  0:00:11s\n",
      "epoch 45 | loss: 1.82907 | val_0_mae: 5.49854 |  0:00:11s\n",
      "epoch 46 | loss: 1.84791 | val_0_mae: 4.62534 |  0:00:11s\n",
      "epoch 47 | loss: 1.73265 | val_0_mae: 5.01886 |  0:00:11s\n",
      "epoch 48 | loss: 1.80034 | val_0_mae: 4.46747 |  0:00:11s\n",
      "epoch 49 | loss: 1.8824  | val_0_mae: 4.01181 |  0:00:12s\n",
      "epoch 50 | loss: 1.80545 | val_0_mae: 4.00878 |  0:00:12s\n",
      "epoch 51 | loss: 1.81518 | val_0_mae: 4.14286 |  0:00:12s\n",
      "epoch 52 | loss: 1.67775 | val_0_mae: 4.22438 |  0:00:12s\n",
      "epoch 53 | loss: 1.78626 | val_0_mae: 3.95452 |  0:00:12s\n",
      "epoch 54 | loss: 1.70321 | val_0_mae: 3.56621 |  0:00:13s\n",
      "epoch 55 | loss: 1.67106 | val_0_mae: 3.75045 |  0:00:13s\n",
      "epoch 56 | loss: 1.67007 | val_0_mae: 3.26713 |  0:00:13s\n",
      "epoch 57 | loss: 1.69031 | val_0_mae: 3.12487 |  0:00:13s\n",
      "epoch 58 | loss: 1.78019 | val_0_mae: 3.50995 |  0:00:13s\n",
      "epoch 59 | loss: 1.71801 | val_0_mae: 2.90714 |  0:00:14s\n",
      "epoch 60 | loss: 1.72082 | val_0_mae: 2.93941 |  0:00:14s\n",
      "epoch 61 | loss: 1.76428 | val_0_mae: 2.8251  |  0:00:14s\n",
      "epoch 62 | loss: 1.74804 | val_0_mae: 2.55736 |  0:00:14s\n",
      "epoch 63 | loss: 1.76069 | val_0_mae: 3.13673 |  0:00:14s\n",
      "epoch 64 | loss: 1.71396 | val_0_mae: 2.20835 |  0:00:15s\n",
      "epoch 65 | loss: 1.74484 | val_0_mae: 2.04449 |  0:00:15s\n",
      "epoch 66 | loss: 1.57505 | val_0_mae: 2.0915  |  0:00:15s\n",
      "epoch 67 | loss: 1.62809 | val_0_mae: 2.58245 |  0:00:15s\n",
      "epoch 68 | loss: 1.68779 | val_0_mae: 2.15737 |  0:00:15s\n",
      "epoch 69 | loss: 1.72807 | val_0_mae: 1.95895 |  0:00:16s\n",
      "epoch 70 | loss: 1.64003 | val_0_mae: 2.01166 |  0:00:16s\n",
      "epoch 71 | loss: 1.60881 | val_0_mae: 2.11179 |  0:00:16s\n",
      "epoch 72 | loss: 1.60466 | val_0_mae: 1.88187 |  0:00:16s\n",
      "epoch 73 | loss: 1.73879 | val_0_mae: 1.79227 |  0:00:16s\n",
      "epoch 74 | loss: 1.63991 | val_0_mae: 2.06149 |  0:00:17s\n",
      "epoch 75 | loss: 1.55965 | val_0_mae: 1.53407 |  0:00:17s\n",
      "epoch 76 | loss: 1.55763 | val_0_mae: 1.75709 |  0:00:17s\n",
      "epoch 77 | loss: 1.57158 | val_0_mae: 1.61024 |  0:00:17s\n",
      "epoch 78 | loss: 1.54196 | val_0_mae: 1.9228  |  0:00:17s\n",
      "epoch 79 | loss: 1.73427 | val_0_mae: 1.66003 |  0:00:18s\n",
      "epoch 80 | loss: 1.5622  | val_0_mae: 1.57684 |  0:00:18s\n",
      "epoch 81 | loss: 1.7112  | val_0_mae: 1.72383 |  0:00:18s\n",
      "epoch 82 | loss: 1.62081 | val_0_mae: 1.46006 |  0:00:18s\n",
      "epoch 83 | loss: 1.59136 | val_0_mae: 1.77097 |  0:00:18s\n",
      "epoch 84 | loss: 1.63524 | val_0_mae: 1.57273 |  0:00:19s\n",
      "epoch 85 | loss: 1.59937 | val_0_mae: 1.72244 |  0:00:19s\n",
      "epoch 86 | loss: 1.57058 | val_0_mae: 1.99901 |  0:00:19s\n",
      "epoch 87 | loss: 1.50824 | val_0_mae: 1.64613 |  0:00:19s\n",
      "epoch 88 | loss: 1.57103 | val_0_mae: 1.95283 |  0:00:19s\n",
      "epoch 89 | loss: 1.57768 | val_0_mae: 1.62153 |  0:00:20s\n",
      "epoch 90 | loss: 1.70092 | val_0_mae: 1.4588  |  0:00:20s\n",
      "epoch 91 | loss: 1.52834 | val_0_mae: 1.65572 |  0:00:20s\n",
      "epoch 92 | loss: 1.57126 | val_0_mae: 1.81993 |  0:00:20s\n",
      "epoch 93 | loss: 1.6027  | val_0_mae: 1.92872 |  0:00:20s\n",
      "epoch 94 | loss: 1.54079 | val_0_mae: 1.53015 |  0:00:21s\n",
      "epoch 95 | loss: 1.51803 | val_0_mae: 1.61437 |  0:00:21s\n",
      "epoch 96 | loss: 1.62018 | val_0_mae: 1.98601 |  0:00:21s\n",
      "epoch 97 | loss: 1.61914 | val_0_mae: 1.53222 |  0:00:21s\n",
      "epoch 98 | loss: 1.55649 | val_0_mae: 1.51758 |  0:00:21s\n",
      "epoch 99 | loss: 1.55778 | val_0_mae: 1.4465  |  0:00:22s\n",
      "epoch 100| loss: 1.53873 | val_0_mae: 1.47735 |  0:00:22s\n",
      "epoch 101| loss: 1.54399 | val_0_mae: 1.51332 |  0:00:22s\n",
      "epoch 102| loss: 1.55425 | val_0_mae: 1.55215 |  0:00:22s\n",
      "epoch 103| loss: 1.54149 | val_0_mae: 1.44075 |  0:00:22s\n",
      "epoch 104| loss: 1.78163 | val_0_mae: 1.65189 |  0:00:23s\n",
      "epoch 105| loss: 1.56188 | val_0_mae: 1.61979 |  0:00:23s\n",
      "epoch 106| loss: 1.5742  | val_0_mae: 1.55841 |  0:00:23s\n",
      "epoch 107| loss: 1.52216 | val_0_mae: 1.55273 |  0:00:23s\n",
      "epoch 108| loss: 1.55411 | val_0_mae: 1.50478 |  0:00:23s\n",
      "epoch 109| loss: 1.48528 | val_0_mae: 1.59315 |  0:00:23s\n",
      "epoch 110| loss: 1.53889 | val_0_mae: 1.47856 |  0:00:24s\n",
      "epoch 111| loss: 1.63589 | val_0_mae: 1.76501 |  0:00:24s\n",
      "epoch 112| loss: 1.97592 | val_0_mae: 1.90625 |  0:00:24s\n",
      "epoch 113| loss: 1.58249 | val_0_mae: 1.71369 |  0:00:24s\n",
      "epoch 114| loss: 1.63356 | val_0_mae: 1.57914 |  0:00:24s\n",
      "epoch 115| loss: 1.78817 | val_0_mae: 1.56387 |  0:00:25s\n",
      "epoch 116| loss: 1.5991  | val_0_mae: 1.6079  |  0:00:25s\n",
      "epoch 117| loss: 1.70529 | val_0_mae: 1.45112 |  0:00:25s\n",
      "epoch 118| loss: 1.49037 | val_0_mae: 1.48501 |  0:00:25s\n",
      "epoch 119| loss: 1.43191 | val_0_mae: 1.58848 |  0:00:25s\n",
      "epoch 120| loss: 1.52178 | val_0_mae: 1.48001 |  0:00:26s\n",
      "epoch 121| loss: 1.45617 | val_0_mae: 1.45294 |  0:00:26s\n",
      "epoch 122| loss: 1.4565  | val_0_mae: 1.43315 |  0:00:26s\n",
      "epoch 123| loss: 1.45782 | val_0_mae: 1.50336 |  0:00:26s\n",
      "epoch 124| loss: 1.42767 | val_0_mae: 1.56222 |  0:00:26s\n",
      "epoch 125| loss: 1.4696  | val_0_mae: 1.48757 |  0:00:27s\n",
      "epoch 126| loss: 1.41863 | val_0_mae: 1.47005 |  0:00:27s\n",
      "epoch 127| loss: 1.42185 | val_0_mae: 1.48403 |  0:00:27s\n",
      "epoch 128| loss: 1.46607 | val_0_mae: 1.5931  |  0:00:27s\n",
      "epoch 129| loss: 1.40869 | val_0_mae: 1.63725 |  0:00:27s\n",
      "epoch 130| loss: 1.40388 | val_0_mae: 1.61833 |  0:00:28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 131| loss: 1.5621  | val_0_mae: 1.61103 |  0:00:28s\n",
      "epoch 132| loss: 1.48142 | val_0_mae: 1.53228 |  0:00:28s\n",
      "epoch 133| loss: 1.49032 | val_0_mae: 1.60905 |  0:00:28s\n",
      "epoch 134| loss: 1.49219 | val_0_mae: 1.42555 |  0:00:28s\n",
      "epoch 135| loss: 1.5227  | val_0_mae: 1.49288 |  0:00:28s\n",
      "epoch 136| loss: 1.54507 | val_0_mae: 1.54579 |  0:00:29s\n",
      "epoch 137| loss: 1.36915 | val_0_mae: 1.37485 |  0:00:29s\n",
      "epoch 138| loss: 1.46379 | val_0_mae: 1.38748 |  0:00:29s\n",
      "epoch 139| loss: 1.4183  | val_0_mae: 1.67885 |  0:00:29s\n",
      "epoch 140| loss: 1.49419 | val_0_mae: 1.55611 |  0:00:29s\n",
      "epoch 141| loss: 1.59483 | val_0_mae: 1.40605 |  0:00:30s\n",
      "epoch 142| loss: 1.52692 | val_0_mae: 1.44879 |  0:00:30s\n",
      "epoch 143| loss: 1.49259 | val_0_mae: 1.44275 |  0:00:30s\n",
      "epoch 144| loss: 1.4878  | val_0_mae: 1.46143 |  0:00:30s\n",
      "epoch 145| loss: 1.42909 | val_0_mae: 1.43272 |  0:00:30s\n",
      "epoch 146| loss: 1.4193  | val_0_mae: 1.37177 |  0:00:31s\n",
      "epoch 147| loss: 1.47371 | val_0_mae: 1.40955 |  0:00:31s\n",
      "epoch 148| loss: 1.3822  | val_0_mae: 1.39274 |  0:00:31s\n",
      "epoch 149| loss: 1.56355 | val_0_mae: 1.41124 |  0:00:31s\n",
      "epoch 150| loss: 1.73203 | val_0_mae: 1.61724 |  0:00:31s\n",
      "epoch 151| loss: 1.58809 | val_0_mae: 1.64729 |  0:00:32s\n",
      "epoch 152| loss: 1.51731 | val_0_mae: 1.41153 |  0:00:32s\n",
      "epoch 153| loss: 1.36676 | val_0_mae: 1.40527 |  0:00:32s\n",
      "epoch 154| loss: 1.47807 | val_0_mae: 1.54577 |  0:00:32s\n",
      "epoch 155| loss: 1.39866 | val_0_mae: 1.51898 |  0:00:32s\n",
      "epoch 156| loss: 1.42633 | val_0_mae: 1.49795 |  0:00:32s\n",
      "epoch 157| loss: 1.37278 | val_0_mae: 1.61818 |  0:00:33s\n",
      "epoch 158| loss: 1.54845 | val_0_mae: 1.47133 |  0:00:33s\n",
      "epoch 159| loss: 1.39607 | val_0_mae: 1.45831 |  0:00:33s\n",
      "epoch 160| loss: 1.42443 | val_0_mae: 1.43691 |  0:00:33s\n",
      "epoch 161| loss: 1.51536 | val_0_mae: 1.46601 |  0:00:33s\n",
      "epoch 162| loss: 1.50667 | val_0_mae: 1.54926 |  0:00:34s\n",
      "epoch 163| loss: 1.473   | val_0_mae: 1.48785 |  0:00:34s\n",
      "epoch 164| loss: 1.52733 | val_0_mae: 1.55596 |  0:00:34s\n",
      "epoch 165| loss: 1.47556 | val_0_mae: 1.53572 |  0:00:34s\n",
      "epoch 166| loss: 1.43201 | val_0_mae: 1.51845 |  0:00:34s\n",
      "epoch 167| loss: 1.37226 | val_0_mae: 1.44491 |  0:00:35s\n",
      "epoch 168| loss: 1.41735 | val_0_mae: 1.35795 |  0:00:35s\n",
      "epoch 169| loss: 1.4071  | val_0_mae: 1.465   |  0:00:35s\n",
      "epoch 170| loss: 1.48116 | val_0_mae: 1.35983 |  0:00:35s\n",
      "epoch 171| loss: 1.41725 | val_0_mae: 1.47413 |  0:00:35s\n",
      "epoch 172| loss: 1.36533 | val_0_mae: 1.44322 |  0:00:36s\n",
      "epoch 173| loss: 1.35538 | val_0_mae: 1.37434 |  0:00:36s\n",
      "epoch 174| loss: 1.39355 | val_0_mae: 1.4054  |  0:00:36s\n",
      "epoch 175| loss: 1.38429 | val_0_mae: 1.38896 |  0:00:36s\n",
      "epoch 176| loss: 1.38369 | val_0_mae: 1.41785 |  0:00:36s\n",
      "epoch 177| loss: 1.36583 | val_0_mae: 1.39047 |  0:00:36s\n",
      "epoch 178| loss: 1.32107 | val_0_mae: 1.42003 |  0:00:37s\n",
      "epoch 179| loss: 1.35807 | val_0_mae: 1.43342 |  0:00:37s\n",
      "epoch 180| loss: 1.45484 | val_0_mae: 1.41362 |  0:00:37s\n",
      "epoch 181| loss: 1.3502  | val_0_mae: 1.56175 |  0:00:37s\n",
      "epoch 182| loss: 1.49258 | val_0_mae: 1.51693 |  0:00:37s\n",
      "epoch 183| loss: 1.3583  | val_0_mae: 1.4182  |  0:00:38s\n",
      "epoch 184| loss: 1.49114 | val_0_mae: 1.47985 |  0:00:38s\n",
      "epoch 185| loss: 1.38033 | val_0_mae: 1.42238 |  0:00:38s\n",
      "epoch 186| loss: 1.43157 | val_0_mae: 1.3937  |  0:00:38s\n",
      "epoch 187| loss: 1.40722 | val_0_mae: 1.4009  |  0:00:38s\n",
      "epoch 188| loss: 1.33183 | val_0_mae: 1.37166 |  0:00:39s\n",
      "epoch 189| loss: 1.30658 | val_0_mae: 1.37454 |  0:00:39s\n",
      "epoch 190| loss: 1.38743 | val_0_mae: 1.35008 |  0:00:39s\n",
      "epoch 191| loss: 1.3242  | val_0_mae: 1.48593 |  0:00:39s\n",
      "epoch 192| loss: 1.3795  | val_0_mae: 1.39826 |  0:00:39s\n",
      "epoch 193| loss: 1.34736 | val_0_mae: 1.55713 |  0:00:40s\n",
      "epoch 194| loss: 1.47788 | val_0_mae: 1.56224 |  0:00:40s\n",
      "epoch 195| loss: 1.36262 | val_0_mae: 1.38048 |  0:00:40s\n",
      "epoch 196| loss: 1.43795 | val_0_mae: 1.6521  |  0:00:40s\n",
      "epoch 197| loss: 1.39887 | val_0_mae: 1.41689 |  0:00:40s\n",
      "epoch 198| loss: 1.37437 | val_0_mae: 1.51023 |  0:00:41s\n",
      "epoch 199| loss: 1.4005  | val_0_mae: 1.47867 |  0:00:41s\n",
      "epoch 200| loss: 1.4007  | val_0_mae: 1.39183 |  0:00:41s\n",
      "epoch 201| loss: 1.41097 | val_0_mae: 1.37364 |  0:00:41s\n",
      "epoch 202| loss: 1.36006 | val_0_mae: 1.38941 |  0:00:41s\n",
      "epoch 203| loss: 1.36452 | val_0_mae: 1.44789 |  0:00:41s\n",
      "epoch 204| loss: 1.31756 | val_0_mae: 1.42937 |  0:00:42s\n",
      "epoch 205| loss: 1.42319 | val_0_mae: 1.41228 |  0:00:42s\n",
      "epoch 206| loss: 1.42525 | val_0_mae: 1.4281  |  0:00:42s\n",
      "epoch 207| loss: 1.39841 | val_0_mae: 1.39498 |  0:00:42s\n",
      "epoch 208| loss: 1.42148 | val_0_mae: 1.52941 |  0:00:42s\n",
      "epoch 209| loss: 1.44384 | val_0_mae: 1.61339 |  0:00:43s\n",
      "epoch 210| loss: 1.39905 | val_0_mae: 1.44776 |  0:00:43s\n",
      "epoch 211| loss: 1.37769 | val_0_mae: 1.45643 |  0:00:43s\n",
      "epoch 212| loss: 1.40914 | val_0_mae: 1.38563 |  0:00:43s\n",
      "epoch 213| loss: 1.3725  | val_0_mae: 1.36529 |  0:00:43s\n",
      "epoch 214| loss: 1.33471 | val_0_mae: 1.45099 |  0:00:44s\n",
      "epoch 215| loss: 1.32786 | val_0_mae: 1.45221 |  0:00:44s\n",
      "epoch 216| loss: 1.33602 | val_0_mae: 1.47752 |  0:00:44s\n",
      "epoch 217| loss: 1.30745 | val_0_mae: 1.42374 |  0:00:44s\n",
      "epoch 218| loss: 1.53892 | val_0_mae: 1.42844 |  0:00:44s\n",
      "epoch 219| loss: 1.57541 | val_0_mae: 1.59716 |  0:00:45s\n",
      "epoch 220| loss: 1.44667 | val_0_mae: 1.42295 |  0:00:45s\n",
      "epoch 221| loss: 1.35886 | val_0_mae: 1.38119 |  0:00:45s\n",
      "epoch 222| loss: 1.41016 | val_0_mae: 1.44697 |  0:00:45s\n",
      "epoch 223| loss: 1.33827 | val_0_mae: 1.42775 |  0:00:45s\n",
      "epoch 224| loss: 1.62509 | val_0_mae: 1.38494 |  0:00:45s\n",
      "epoch 225| loss: 1.36513 | val_0_mae: 1.34545 |  0:00:46s\n",
      "epoch 226| loss: 1.44904 | val_0_mae: 1.41676 |  0:00:46s\n",
      "epoch 227| loss: 1.32444 | val_0_mae: 1.40938 |  0:00:46s\n",
      "epoch 228| loss: 1.36091 | val_0_mae: 1.38741 |  0:00:46s\n",
      "epoch 229| loss: 1.30781 | val_0_mae: 1.46391 |  0:00:46s\n",
      "epoch 230| loss: 1.41791 | val_0_mae: 1.46585 |  0:00:47s\n",
      "epoch 231| loss: 1.3455  | val_0_mae: 1.35737 |  0:00:47s\n",
      "epoch 232| loss: 1.37479 | val_0_mae: 1.51295 |  0:00:47s\n",
      "epoch 233| loss: 1.40066 | val_0_mae: 1.65143 |  0:00:47s\n",
      "epoch 234| loss: 1.45248 | val_0_mae: 1.57349 |  0:00:47s\n",
      "epoch 235| loss: 1.33075 | val_0_mae: 1.40337 |  0:00:48s\n",
      "epoch 236| loss: 1.35229 | val_0_mae: 1.38768 |  0:00:48s\n",
      "epoch 237| loss: 1.33094 | val_0_mae: 1.37385 |  0:00:48s\n",
      "epoch 238| loss: 1.3563  | val_0_mae: 1.51076 |  0:00:48s\n",
      "epoch 239| loss: 1.43334 | val_0_mae: 1.44144 |  0:00:48s\n",
      "epoch 240| loss: 1.33922 | val_0_mae: 1.38832 |  0:00:49s\n",
      "epoch 241| loss: 1.32948 | val_0_mae: 1.37095 |  0:00:49s\n",
      "epoch 242| loss: 1.32915 | val_0_mae: 1.35659 |  0:00:49s\n",
      "epoch 243| loss: 1.31339 | val_0_mae: 1.3242  |  0:00:49s\n",
      "epoch 244| loss: 1.33701 | val_0_mae: 1.30624 |  0:00:49s\n",
      "epoch 245| loss: 1.32917 | val_0_mae: 1.38154 |  0:00:50s\n",
      "epoch 246| loss: 1.31586 | val_0_mae: 1.36187 |  0:00:50s\n",
      "epoch 247| loss: 1.34321 | val_0_mae: 1.43446 |  0:00:50s\n",
      "epoch 248| loss: 1.34761 | val_0_mae: 1.40181 |  0:00:50s\n",
      "epoch 249| loss: 1.49017 | val_0_mae: 1.3395  |  0:00:50s\n",
      "Stop training because you reached max_epochs = 250 with best_epoch = 244 and best_val_0_mae = 1.30624\n",
      "Best weights from best epoch are automatically used!\n",
      "XGB: 1.3621444231593636\n",
      "CAT: 1.3643214728135353\n",
      "TAB: 1.3062404886242729\n",
      "MHA: 2.5463061741149944\n",
      "CNN_TINY: 3.3042877605231142\n",
      "CNN_BIG: 2.4044881737767754\n",
      "MEAN MHA+TINY+BIG: 2.046623605755734\n",
      "MEAN XGB+CAT+TAB: 1.2903139934250427\n",
      "****************************************************************************************************\n",
      "============================================================\n",
      "Num Fold: 2\n",
      "Train segments: 3545 Val segments: 886\n",
      "============================================================\n",
      "Device used : cuda\n",
      "Training xgboost\n",
      "[19:18:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:18:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Training catboost\n",
      "Training TabNet\n",
      "epoch 0  | loss: 23.68894| val_0_mae: 28.05957|  0:00:00s\n",
      "epoch 1  | loss: 22.6652 | val_0_mae: 25.12211|  0:00:00s\n",
      "epoch 2  | loss: 22.06267| val_0_mae: 19.28047|  0:00:00s\n",
      "epoch 3  | loss: 21.4394 | val_0_mae: 19.09821|  0:00:00s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 20.58188| val_0_mae: 17.02153|  0:00:00s\n",
      "epoch 5  | loss: 19.46118| val_0_mae: 14.42634|  0:00:01s\n",
      "epoch 6  | loss: 18.02976| val_0_mae: 10.84923|  0:00:01s\n",
      "epoch 7  | loss: 16.2714 | val_0_mae: 7.94944 |  0:00:01s\n",
      "epoch 8  | loss: 14.22001| val_0_mae: 8.08145 |  0:00:01s\n",
      "epoch 9  | loss: 11.81805| val_0_mae: 7.76848 |  0:00:01s\n",
      "epoch 10 | loss: 9.10733 | val_0_mae: 10.47688|  0:00:02s\n",
      "epoch 11 | loss: 6.37891 | val_0_mae: 16.40004|  0:00:02s\n",
      "epoch 12 | loss: 3.9409  | val_0_mae: 24.95805|  0:00:02s\n",
      "epoch 13 | loss: 3.27744 | val_0_mae: 25.24245|  0:00:02s\n",
      "epoch 14 | loss: 3.17753 | val_0_mae: 17.56455|  0:00:02s\n",
      "epoch 15 | loss: 2.59362 | val_0_mae: 13.22283|  0:00:03s\n",
      "epoch 16 | loss: 2.41929 | val_0_mae: 13.59539|  0:00:03s\n",
      "epoch 17 | loss: 2.18275 | val_0_mae: 15.37704|  0:00:03s\n",
      "epoch 18 | loss: 2.26018 | val_0_mae: 14.21726|  0:00:03s\n",
      "epoch 19 | loss: 2.05895 | val_0_mae: 13.5312 |  0:00:03s\n",
      "epoch 20 | loss: 2.15603 | val_0_mae: 13.74895|  0:00:04s\n",
      "epoch 21 | loss: 2.08971 | val_0_mae: 13.12545|  0:00:04s\n",
      "epoch 22 | loss: 2.06891 | val_0_mae: 13.06601|  0:00:04s\n",
      "epoch 23 | loss: 2.01571 | val_0_mae: 12.36457|  0:00:04s\n",
      "epoch 24 | loss: 2.07604 | val_0_mae: 11.72217|  0:00:04s\n",
      "epoch 25 | loss: 1.9553  | val_0_mae: 10.86063|  0:00:05s\n",
      "epoch 26 | loss: 1.9043  | val_0_mae: 11.22334|  0:00:05s\n",
      "epoch 27 | loss: 1.8266  | val_0_mae: 10.16721|  0:00:05s\n",
      "epoch 28 | loss: 1.80097 | val_0_mae: 9.4179  |  0:00:05s\n",
      "epoch 29 | loss: 1.91686 | val_0_mae: 8.42846 |  0:00:05s\n",
      "epoch 30 | loss: 1.96022 | val_0_mae: 8.56717 |  0:00:05s\n",
      "epoch 31 | loss: 1.91749 | val_0_mae: 6.39614 |  0:00:06s\n",
      "epoch 32 | loss: 1.92876 | val_0_mae: 7.49745 |  0:00:06s\n",
      "epoch 33 | loss: 1.79619 | val_0_mae: 7.21    |  0:00:06s\n",
      "epoch 34 | loss: 1.84996 | val_0_mae: 7.42745 |  0:00:06s\n",
      "epoch 35 | loss: 1.8341  | val_0_mae: 5.60167 |  0:00:06s\n",
      "epoch 36 | loss: 1.79046 | val_0_mae: 7.25798 |  0:00:07s\n",
      "epoch 37 | loss: 1.84938 | val_0_mae: 5.66535 |  0:00:07s\n",
      "epoch 38 | loss: 1.79611 | val_0_mae: 5.80096 |  0:00:07s\n",
      "epoch 39 | loss: 1.87122 | val_0_mae: 5.7396  |  0:00:07s\n",
      "epoch 40 | loss: 1.77248 | val_0_mae: 4.96183 |  0:00:07s\n",
      "epoch 41 | loss: 1.74474 | val_0_mae: 4.58151 |  0:00:08s\n",
      "epoch 42 | loss: 1.72973 | val_0_mae: 5.22031 |  0:00:08s\n",
      "epoch 43 | loss: 1.76672 | val_0_mae: 3.86132 |  0:00:08s\n",
      "epoch 44 | loss: 1.86213 | val_0_mae: 4.4232  |  0:00:08s\n",
      "epoch 45 | loss: 1.75039 | val_0_mae: 4.98293 |  0:00:08s\n",
      "epoch 46 | loss: 1.67319 | val_0_mae: 4.57096 |  0:00:09s\n",
      "epoch 47 | loss: 1.74261 | val_0_mae: 4.00815 |  0:00:09s\n",
      "epoch 48 | loss: 1.749   | val_0_mae: 3.55104 |  0:00:09s\n",
      "epoch 49 | loss: 1.79686 | val_0_mae: 3.69007 |  0:00:09s\n",
      "epoch 50 | loss: 1.73506 | val_0_mae: 3.55393 |  0:00:09s\n",
      "epoch 51 | loss: 1.76047 | val_0_mae: 3.68524 |  0:00:10s\n",
      "epoch 52 | loss: 1.7439  | val_0_mae: 3.20185 |  0:00:10s\n",
      "epoch 53 | loss: 1.57103 | val_0_mae: 2.93343 |  0:00:10s\n",
      "epoch 54 | loss: 1.78809 | val_0_mae: 2.97286 |  0:00:10s\n",
      "epoch 55 | loss: 1.73096 | val_0_mae: 2.33917 |  0:00:10s\n",
      "epoch 56 | loss: 1.8662  | val_0_mae: 2.48198 |  0:00:10s\n",
      "epoch 57 | loss: 1.71502 | val_0_mae: 4.33359 |  0:00:11s\n",
      "epoch 58 | loss: 1.98895 | val_0_mae: 2.9655  |  0:00:11s\n",
      "epoch 59 | loss: 1.61658 | val_0_mae: 2.28102 |  0:00:11s\n",
      "epoch 60 | loss: 1.60621 | val_0_mae: 2.63418 |  0:00:11s\n",
      "epoch 61 | loss: 1.60135 | val_0_mae: 2.55792 |  0:00:11s\n",
      "epoch 62 | loss: 1.56301 | val_0_mae: 2.9614  |  0:00:12s\n",
      "epoch 63 | loss: 1.67166 | val_0_mae: 2.05565 |  0:00:12s\n",
      "epoch 64 | loss: 1.78329 | val_0_mae: 2.33544 |  0:00:12s\n",
      "epoch 65 | loss: 1.85262 | val_0_mae: 3.48361 |  0:00:12s\n",
      "epoch 66 | loss: 1.65217 | val_0_mae: 1.85749 |  0:00:12s\n",
      "epoch 67 | loss: 1.85947 | val_0_mae: 1.70865 |  0:00:13s\n",
      "epoch 68 | loss: 1.81685 | val_0_mae: 2.38061 |  0:00:13s\n",
      "epoch 69 | loss: 1.5565  | val_0_mae: 2.02052 |  0:00:13s\n",
      "epoch 70 | loss: 1.58224 | val_0_mae: 2.17542 |  0:00:13s\n",
      "epoch 71 | loss: 1.54573 | val_0_mae: 2.1244  |  0:00:13s\n",
      "epoch 72 | loss: 1.5467  | val_0_mae: 2.17276 |  0:00:14s\n",
      "epoch 73 | loss: 1.53536 | val_0_mae: 1.84395 |  0:00:14s\n",
      "epoch 74 | loss: 1.56349 | val_0_mae: 1.74352 |  0:00:14s\n",
      "epoch 75 | loss: 1.5254  | val_0_mae: 1.59019 |  0:00:14s\n",
      "epoch 76 | loss: 1.64683 | val_0_mae: 2.03308 |  0:00:14s\n",
      "epoch 77 | loss: 1.52003 | val_0_mae: 1.63823 |  0:00:15s\n",
      "epoch 78 | loss: 1.52291 | val_0_mae: 1.94601 |  0:00:15s\n",
      "epoch 79 | loss: 1.53189 | val_0_mae: 1.84223 |  0:00:15s\n",
      "epoch 80 | loss: 1.50147 | val_0_mae: 1.75563 |  0:00:15s\n",
      "epoch 81 | loss: 1.57352 | val_0_mae: 2.14415 |  0:00:15s\n",
      "epoch 82 | loss: 1.54153 | val_0_mae: 1.47515 |  0:00:16s\n",
      "epoch 83 | loss: 1.47922 | val_0_mae: 1.78933 |  0:00:16s\n",
      "epoch 84 | loss: 1.49272 | val_0_mae: 1.55319 |  0:00:16s\n",
      "epoch 85 | loss: 1.55794 | val_0_mae: 1.65273 |  0:00:16s\n",
      "epoch 86 | loss: 1.56347 | val_0_mae: 1.88291 |  0:00:16s\n",
      "epoch 87 | loss: 1.49414 | val_0_mae: 1.50796 |  0:00:16s\n",
      "epoch 88 | loss: 1.55461 | val_0_mae: 1.85273 |  0:00:17s\n",
      "epoch 89 | loss: 1.61504 | val_0_mae: 1.46089 |  0:00:17s\n",
      "epoch 90 | loss: 1.50945 | val_0_mae: 1.69528 |  0:00:17s\n",
      "epoch 91 | loss: 1.64968 | val_0_mae: 1.63835 |  0:00:17s\n",
      "epoch 92 | loss: 1.4219  | val_0_mae: 1.68594 |  0:00:17s\n",
      "epoch 93 | loss: 1.50045 | val_0_mae: 1.60777 |  0:00:18s\n",
      "epoch 94 | loss: 1.5265  | val_0_mae: 1.54608 |  0:00:18s\n",
      "epoch 95 | loss: 1.44155 | val_0_mae: 1.64671 |  0:00:18s\n",
      "epoch 96 | loss: 1.54095 | val_0_mae: 1.47932 |  0:00:18s\n",
      "epoch 97 | loss: 1.66732 | val_0_mae: 1.63383 |  0:00:18s\n",
      "epoch 98 | loss: 1.54932 | val_0_mae: 1.44075 |  0:00:19s\n",
      "epoch 99 | loss: 1.57482 | val_0_mae: 1.90453 |  0:00:19s\n",
      "epoch 100| loss: 1.53793 | val_0_mae: 1.41417 |  0:00:19s\n",
      "epoch 101| loss: 1.69595 | val_0_mae: 1.57545 |  0:00:19s\n",
      "epoch 102| loss: 1.49762 | val_0_mae: 1.72927 |  0:00:19s\n",
      "epoch 103| loss: 1.76068 | val_0_mae: 1.69914 |  0:00:20s\n",
      "epoch 104| loss: 1.51359 | val_0_mae: 1.40379 |  0:00:20s\n",
      "epoch 105| loss: 1.41701 | val_0_mae: 1.55734 |  0:00:20s\n",
      "epoch 106| loss: 1.44797 | val_0_mae: 1.42638 |  0:00:20s\n",
      "epoch 107| loss: 1.4353  | val_0_mae: 1.49835 |  0:00:20s\n",
      "epoch 108| loss: 1.46994 | val_0_mae: 1.46229 |  0:00:21s\n",
      "epoch 109| loss: 1.50447 | val_0_mae: 1.44216 |  0:00:21s\n",
      "epoch 110| loss: 1.3972  | val_0_mae: 1.43683 |  0:00:21s\n",
      "epoch 111| loss: 1.47527 | val_0_mae: 1.57168 |  0:00:21s\n",
      "epoch 112| loss: 1.49008 | val_0_mae: 1.44712 |  0:00:21s\n",
      "epoch 113| loss: 1.75428 | val_0_mae: 1.47225 |  0:00:22s\n",
      "epoch 114| loss: 1.55373 | val_0_mae: 1.6344  |  0:00:22s\n",
      "epoch 115| loss: 1.65504 | val_0_mae: 1.39427 |  0:00:22s\n",
      "epoch 116| loss: 1.41562 | val_0_mae: 1.51792 |  0:00:22s\n",
      "epoch 117| loss: 1.42951 | val_0_mae: 1.4747  |  0:00:22s\n",
      "epoch 118| loss: 1.5079  | val_0_mae: 1.5921  |  0:00:23s\n",
      "epoch 119| loss: 1.46632 | val_0_mae: 1.45727 |  0:00:23s\n",
      "epoch 120| loss: 1.49874 | val_0_mae: 1.62435 |  0:00:23s\n",
      "epoch 121| loss: 1.48282 | val_0_mae: 1.45978 |  0:00:23s\n",
      "epoch 122| loss: 1.48776 | val_0_mae: 1.46548 |  0:00:23s\n",
      "epoch 123| loss: 1.47478 | val_0_mae: 1.44205 |  0:00:23s\n",
      "epoch 124| loss: 1.39136 | val_0_mae: 1.43612 |  0:00:24s\n",
      "epoch 125| loss: 1.41287 | val_0_mae: 1.55014 |  0:00:24s\n",
      "epoch 126| loss: 1.45806 | val_0_mae: 1.36568 |  0:00:24s\n",
      "epoch 127| loss: 1.38052 | val_0_mae: 1.3847  |  0:00:24s\n",
      "epoch 128| loss: 1.43872 | val_0_mae: 1.43845 |  0:00:24s\n",
      "epoch 129| loss: 1.43472 | val_0_mae: 1.43595 |  0:00:25s\n",
      "epoch 130| loss: 1.50972 | val_0_mae: 1.51625 |  0:00:25s\n",
      "epoch 131| loss: 1.44134 | val_0_mae: 1.36656 |  0:00:25s\n",
      "epoch 132| loss: 1.40401 | val_0_mae: 1.44235 |  0:00:25s\n",
      "epoch 133| loss: 1.51499 | val_0_mae: 1.44168 |  0:00:25s\n",
      "epoch 134| loss: 1.49494 | val_0_mae: 1.43212 |  0:00:26s\n",
      "epoch 135| loss: 1.41807 | val_0_mae: 1.49746 |  0:00:26s\n",
      "epoch 136| loss: 1.57214 | val_0_mae: 1.4095  |  0:00:26s\n",
      "epoch 137| loss: 1.52146 | val_0_mae: 1.39598 |  0:00:26s\n",
      "epoch 138| loss: 1.47325 | val_0_mae: 1.62949 |  0:00:26s\n",
      "epoch 139| loss: 1.57876 | val_0_mae: 1.39605 |  0:00:27s\n",
      "epoch 140| loss: 1.52733 | val_0_mae: 1.39604 |  0:00:27s\n",
      "epoch 141| loss: 1.472   | val_0_mae: 1.35465 |  0:00:27s\n",
      "epoch 142| loss: 1.61726 | val_0_mae: 1.49194 |  0:00:27s\n",
      "epoch 143| loss: 1.48642 | val_0_mae: 1.43607 |  0:00:27s\n",
      "epoch 144| loss: 1.52222 | val_0_mae: 1.44905 |  0:00:27s\n",
      "epoch 145| loss: 1.40705 | val_0_mae: 1.51733 |  0:00:28s\n",
      "epoch 146| loss: 1.58378 | val_0_mae: 1.42065 |  0:00:28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 147| loss: 1.4405  | val_0_mae: 1.39349 |  0:00:28s\n",
      "epoch 148| loss: 1.38993 | val_0_mae: 1.34328 |  0:00:28s\n",
      "epoch 149| loss: 1.4466  | val_0_mae: 1.39517 |  0:00:28s\n",
      "epoch 150| loss: 1.39404 | val_0_mae: 1.51016 |  0:00:29s\n",
      "epoch 151| loss: 1.5756  | val_0_mae: 1.40897 |  0:00:29s\n",
      "epoch 152| loss: 1.43    | val_0_mae: 1.4816  |  0:00:29s\n",
      "epoch 153| loss: 1.37897 | val_0_mae: 1.451   |  0:00:29s\n",
      "epoch 154| loss: 1.44372 | val_0_mae: 1.49126 |  0:00:29s\n",
      "epoch 155| loss: 1.43164 | val_0_mae: 1.4026  |  0:00:30s\n",
      "epoch 156| loss: 1.49747 | val_0_mae: 1.50221 |  0:00:30s\n",
      "epoch 157| loss: 1.53596 | val_0_mae: 1.46493 |  0:00:30s\n",
      "epoch 158| loss: 1.43903 | val_0_mae: 1.54469 |  0:00:30s\n",
      "epoch 159| loss: 1.77119 | val_0_mae: 1.44838 |  0:00:30s\n",
      "epoch 160| loss: 1.41335 | val_0_mae: 1.53373 |  0:00:31s\n",
      "epoch 161| loss: 1.555   | val_0_mae: 1.45134 |  0:00:31s\n",
      "epoch 162| loss: 1.46989 | val_0_mae: 1.39777 |  0:00:31s\n",
      "epoch 163| loss: 1.53295 | val_0_mae: 1.56494 |  0:00:31s\n",
      "epoch 164| loss: 1.68389 | val_0_mae: 1.39034 |  0:00:31s\n",
      "epoch 165| loss: 1.3887  | val_0_mae: 1.39432 |  0:00:31s\n",
      "epoch 166| loss: 1.38627 | val_0_mae: 1.36443 |  0:00:32s\n",
      "epoch 167| loss: 1.37939 | val_0_mae: 1.36466 |  0:00:32s\n",
      "epoch 168| loss: 1.45955 | val_0_mae: 1.35616 |  0:00:32s\n",
      "epoch 169| loss: 1.35597 | val_0_mae: 1.41771 |  0:00:32s\n",
      "epoch 170| loss: 1.35262 | val_0_mae: 1.34527 |  0:00:32s\n",
      "epoch 171| loss: 1.40977 | val_0_mae: 1.36375 |  0:00:33s\n",
      "epoch 172| loss: 1.39753 | val_0_mae: 1.42979 |  0:00:33s\n",
      "epoch 173| loss: 1.55978 | val_0_mae: 1.32412 |  0:00:33s\n",
      "epoch 174| loss: 1.48218 | val_0_mae: 1.38603 |  0:00:33s\n",
      "epoch 175| loss: 1.38948 | val_0_mae: 1.43568 |  0:00:33s\n",
      "epoch 176| loss: 1.45131 | val_0_mae: 1.32892 |  0:00:34s\n",
      "epoch 177| loss: 1.36465 | val_0_mae: 1.44214 |  0:00:34s\n",
      "epoch 178| loss: 1.37573 | val_0_mae: 1.39531 |  0:00:34s\n",
      "epoch 179| loss: 1.4876  | val_0_mae: 1.34887 |  0:00:34s\n",
      "epoch 180| loss: 1.49398 | val_0_mae: 1.36866 |  0:00:34s\n",
      "epoch 181| loss: 1.3706  | val_0_mae: 1.34575 |  0:00:35s\n",
      "epoch 182| loss: 1.31093 | val_0_mae: 1.39058 |  0:00:35s\n",
      "epoch 183| loss: 1.48971 | val_0_mae: 1.40119 |  0:00:35s\n",
      "epoch 184| loss: 1.42255 | val_0_mae: 1.45751 |  0:00:35s\n",
      "epoch 185| loss: 1.4419  | val_0_mae: 1.43818 |  0:00:35s\n",
      "epoch 186| loss: 1.54285 | val_0_mae: 1.4563  |  0:00:36s\n",
      "epoch 187| loss: 1.43034 | val_0_mae: 1.57076 |  0:00:36s\n",
      "epoch 188| loss: 1.51684 | val_0_mae: 1.41098 |  0:00:36s\n",
      "epoch 189| loss: 1.50717 | val_0_mae: 1.48822 |  0:00:36s\n",
      "epoch 190| loss: 1.5557  | val_0_mae: 1.48016 |  0:00:36s\n",
      "epoch 191| loss: 1.54655 | val_0_mae: 1.369   |  0:00:37s\n",
      "epoch 192| loss: 1.56724 | val_0_mae: 1.58437 |  0:00:37s\n",
      "epoch 193| loss: 1.64211 | val_0_mae: 1.38804 |  0:00:37s\n",
      "epoch 194| loss: 1.32473 | val_0_mae: 1.39944 |  0:00:37s\n",
      "epoch 195| loss: 1.52753 | val_0_mae: 1.39034 |  0:00:37s\n",
      "epoch 196| loss: 1.43631 | val_0_mae: 1.42688 |  0:00:38s\n",
      "epoch 197| loss: 1.41899 | val_0_mae: 1.42885 |  0:00:38s\n",
      "epoch 198| loss: 1.54712 | val_0_mae: 1.51121 |  0:00:38s\n",
      "epoch 199| loss: 1.36529 | val_0_mae: 1.50981 |  0:00:38s\n",
      "epoch 200| loss: 1.69412 | val_0_mae: 1.57897 |  0:00:38s\n",
      "epoch 201| loss: 1.35187 | val_0_mae: 1.69196 |  0:00:39s\n",
      "epoch 202| loss: 1.97461 | val_0_mae: 1.82851 |  0:00:39s\n",
      "epoch 203| loss: 1.57024 | val_0_mae: 1.44331 |  0:00:39s\n",
      "epoch 204| loss: 1.61053 | val_0_mae: 1.7916  |  0:00:39s\n",
      "epoch 205| loss: 1.47662 | val_0_mae: 1.40769 |  0:00:39s\n",
      "epoch 206| loss: 1.44247 | val_0_mae: 1.38964 |  0:00:40s\n",
      "epoch 207| loss: 1.36665 | val_0_mae: 1.42892 |  0:00:40s\n",
      "epoch 208| loss: 1.33165 | val_0_mae: 1.33444 |  0:00:40s\n",
      "epoch 209| loss: 1.38975 | val_0_mae: 1.45564 |  0:00:40s\n",
      "epoch 210| loss: 1.31323 | val_0_mae: 1.34736 |  0:00:40s\n",
      "epoch 211| loss: 1.45876 | val_0_mae: 1.40059 |  0:00:41s\n",
      "epoch 212| loss: 1.32226 | val_0_mae: 1.406   |  0:00:41s\n",
      "epoch 213| loss: 1.3554  | val_0_mae: 1.41212 |  0:00:41s\n",
      "epoch 214| loss: 1.32938 | val_0_mae: 1.37537 |  0:00:41s\n",
      "epoch 215| loss: 1.35017 | val_0_mae: 1.34853 |  0:00:41s\n",
      "epoch 216| loss: 1.34594 | val_0_mae: 1.39842 |  0:00:42s\n",
      "epoch 217| loss: 1.46539 | val_0_mae: 1.48451 |  0:00:42s\n",
      "epoch 218| loss: 1.49286 | val_0_mae: 1.42097 |  0:00:42s\n",
      "epoch 219| loss: 1.53488 | val_0_mae: 1.60285 |  0:00:42s\n",
      "epoch 220| loss: 1.341   | val_0_mae: 1.45943 |  0:00:42s\n",
      "epoch 221| loss: 1.36333 | val_0_mae: 1.45099 |  0:00:43s\n",
      "epoch 222| loss: 1.88025 | val_0_mae: 1.65139 |  0:00:43s\n",
      "epoch 223| loss: 1.88788 | val_0_mae: 1.34411 |  0:00:43s\n",
      "\n",
      "Early stopping occured at epoch 223 with best_epoch = 173 and best_val_0_mae = 1.32412\n",
      "Best weights from best epoch are automatically used!\n",
      "XGB: 1.3296225073015602\n",
      "CAT: 1.3447970308839619\n",
      "TAB: 1.3241187700411687\n",
      "MHA: 2.555380703126412\n",
      "CNN_TINY: 3.1556127611738165\n",
      "CNN_BIG: 2.3466406212866837\n",
      "MEAN MHA+TINY+BIG: 2.0496175096049636\n",
      "MEAN XGB+CAT+TAB: 1.2915907581200783\n",
      "****************************************************************************************************\n",
      "============================================================\n",
      "Num Fold: 3\n",
      "Train segments: 3545 Val segments: 886\n",
      "============================================================\n",
      "Device used : cuda\n",
      "Training xgboost\n",
      "[19:18:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:18:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Training catboost\n",
      "Training TabNet\n",
      "epoch 0  | loss: 23.78559| val_0_mae: 25.24225|  0:00:00s\n",
      "epoch 1  | loss: 22.77904| val_0_mae: 21.4695 |  0:00:00s\n",
      "epoch 2  | loss: 22.18696| val_0_mae: 19.51273|  0:00:00s\n",
      "epoch 3  | loss: 21.56259| val_0_mae: 17.88168|  0:00:00s\n",
      "epoch 4  | loss: 20.7279 | val_0_mae: 15.92209|  0:00:01s\n",
      "epoch 5  | loss: 19.6098 | val_0_mae: 11.63085|  0:00:01s\n",
      "epoch 6  | loss: 18.28141| val_0_mae: 8.99267 |  0:00:01s\n",
      "epoch 7  | loss: 16.63657| val_0_mae: 8.70725 |  0:00:01s\n",
      "epoch 8  | loss: 14.61776| val_0_mae: 9.92913 |  0:00:01s\n",
      "epoch 9  | loss: 12.22774| val_0_mae: 10.0757 |  0:00:02s\n",
      "epoch 10 | loss: 9.52327 | val_0_mae: 8.70744 |  0:00:02s\n",
      "epoch 11 | loss: 6.70706 | val_0_mae: 10.32862|  0:00:02s\n",
      "epoch 12 | loss: 4.2355  | val_0_mae: 19.48233|  0:00:02s\n",
      "epoch 13 | loss: 3.25625 | val_0_mae: 20.12624|  0:00:02s\n",
      "epoch 14 | loss: 3.50015 | val_0_mae: 17.56334|  0:00:03s\n",
      "epoch 15 | loss: 3.18024 | val_0_mae: 16.72875|  0:00:03s\n",
      "epoch 16 | loss: 2.65823 | val_0_mae: 15.89989|  0:00:03s\n",
      "epoch 17 | loss: 2.58014 | val_0_mae: 14.92156|  0:00:03s\n",
      "epoch 18 | loss: 2.43194 | val_0_mae: 13.43842|  0:00:03s\n",
      "epoch 19 | loss: 2.35146 | val_0_mae: 13.09733|  0:00:04s\n",
      "epoch 20 | loss: 2.22349 | val_0_mae: 14.1762 |  0:00:04s\n",
      "epoch 21 | loss: 2.13665 | val_0_mae: 12.48982|  0:00:04s\n",
      "epoch 22 | loss: 2.22655 | val_0_mae: 12.38127|  0:00:04s\n",
      "epoch 23 | loss: 2.07114 | val_0_mae: 10.26508|  0:00:04s\n",
      "epoch 24 | loss: 2.18028 | val_0_mae: 10.86031|  0:00:05s\n",
      "epoch 25 | loss: 2.08414 | val_0_mae: 9.6692  |  0:00:05s\n",
      "epoch 26 | loss: 2.14875 | val_0_mae: 9.51049 |  0:00:05s\n",
      "epoch 27 | loss: 1.99414 | val_0_mae: 8.67458 |  0:00:05s\n",
      "epoch 28 | loss: 1.96112 | val_0_mae: 8.69785 |  0:00:05s\n",
      "epoch 29 | loss: 2.00564 | val_0_mae: 7.49897 |  0:00:06s\n",
      "epoch 30 | loss: 1.87871 | val_0_mae: 7.86285 |  0:00:06s\n",
      "epoch 31 | loss: 1.93675 | val_0_mae: 6.71327 |  0:00:06s\n",
      "epoch 32 | loss: 1.99888 | val_0_mae: 7.17133 |  0:00:06s\n",
      "epoch 33 | loss: 1.95789 | val_0_mae: 6.92842 |  0:00:06s\n",
      "epoch 34 | loss: 1.9048  | val_0_mae: 6.11751 |  0:00:07s\n",
      "epoch 35 | loss: 1.93889 | val_0_mae: 5.58531 |  0:00:07s\n",
      "epoch 36 | loss: 1.78273 | val_0_mae: 6.57289 |  0:00:07s\n",
      "epoch 37 | loss: 1.97786 | val_0_mae: 5.87839 |  0:00:07s\n",
      "epoch 38 | loss: 1.94342 | val_0_mae: 4.91952 |  0:00:07s\n",
      "epoch 39 | loss: 1.92579 | val_0_mae: 5.46139 |  0:00:08s\n",
      "epoch 40 | loss: 1.89563 | val_0_mae: 5.53576 |  0:00:08s\n",
      "epoch 41 | loss: 1.83313 | val_0_mae: 4.05663 |  0:00:08s\n",
      "epoch 42 | loss: 2.01331 | val_0_mae: 4.46685 |  0:00:08s\n",
      "epoch 43 | loss: 1.7736  | val_0_mae: 4.38055 |  0:00:08s\n",
      "epoch 44 | loss: 1.99192 | val_0_mae: 4.08561 |  0:00:09s\n",
      "epoch 45 | loss: 1.85942 | val_0_mae: 3.81921 |  0:00:09s\n",
      "epoch 46 | loss: 1.72387 | val_0_mae: 3.80844 |  0:00:09s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 | loss: 1.72541 | val_0_mae: 3.52827 |  0:00:09s\n",
      "epoch 48 | loss: 1.84266 | val_0_mae: 3.89097 |  0:00:09s\n",
      "epoch 49 | loss: 1.80922 | val_0_mae: 3.14945 |  0:00:10s\n",
      "epoch 50 | loss: 1.83096 | val_0_mae: 3.44316 |  0:00:10s\n",
      "epoch 51 | loss: 1.7106  | val_0_mae: 2.9579  |  0:00:10s\n",
      "epoch 52 | loss: 1.75237 | val_0_mae: 3.60106 |  0:00:10s\n",
      "epoch 53 | loss: 1.69632 | val_0_mae: 2.89464 |  0:00:10s\n",
      "epoch 54 | loss: 1.73111 | val_0_mae: 2.75798 |  0:00:10s\n",
      "epoch 55 | loss: 1.73566 | val_0_mae: 3.47462 |  0:00:11s\n",
      "epoch 56 | loss: 1.69017 | val_0_mae: 2.91556 |  0:00:11s\n",
      "epoch 57 | loss: 1.88517 | val_0_mae: 4.00029 |  0:00:11s\n",
      "epoch 58 | loss: 1.81293 | val_0_mae: 2.38161 |  0:00:11s\n",
      "epoch 59 | loss: 1.84698 | val_0_mae: 2.54127 |  0:00:11s\n",
      "epoch 60 | loss: 1.69498 | val_0_mae: 2.67789 |  0:00:12s\n",
      "epoch 61 | loss: 1.63405 | val_0_mae: 2.15853 |  0:00:12s\n",
      "epoch 62 | loss: 1.77243 | val_0_mae: 2.77614 |  0:00:12s\n",
      "epoch 63 | loss: 1.72795 | val_0_mae: 1.83431 |  0:00:12s\n",
      "epoch 64 | loss: 1.73843 | val_0_mae: 1.98924 |  0:00:13s\n",
      "epoch 65 | loss: 1.72006 | val_0_mae: 2.50997 |  0:00:13s\n",
      "epoch 66 | loss: 1.67137 | val_0_mae: 1.83405 |  0:00:13s\n",
      "epoch 67 | loss: 1.58647 | val_0_mae: 1.91073 |  0:00:13s\n",
      "epoch 68 | loss: 1.64677 | val_0_mae: 1.90237 |  0:00:13s\n",
      "epoch 69 | loss: 1.65806 | val_0_mae: 1.94873 |  0:00:14s\n",
      "epoch 70 | loss: 1.63644 | val_0_mae: 1.95934 |  0:00:14s\n",
      "epoch 71 | loss: 1.68932 | val_0_mae: 2.17113 |  0:00:14s\n",
      "epoch 72 | loss: 1.70438 | val_0_mae: 1.82414 |  0:00:14s\n",
      "epoch 73 | loss: 1.58468 | val_0_mae: 1.84728 |  0:00:14s\n",
      "epoch 74 | loss: 1.56145 | val_0_mae: 1.57867 |  0:00:14s\n",
      "epoch 75 | loss: 1.60859 | val_0_mae: 1.79208 |  0:00:15s\n",
      "epoch 76 | loss: 1.61761 | val_0_mae: 1.44806 |  0:00:15s\n",
      "epoch 77 | loss: 1.68596 | val_0_mae: 1.72506 |  0:00:15s\n",
      "epoch 78 | loss: 1.83023 | val_0_mae: 1.74207 |  0:00:15s\n",
      "epoch 79 | loss: 1.81672 | val_0_mae: 1.87446 |  0:00:16s\n",
      "epoch 80 | loss: 1.63095 | val_0_mae: 1.8371  |  0:00:16s\n",
      "epoch 81 | loss: 1.61323 | val_0_mae: 1.8325  |  0:00:16s\n",
      "epoch 82 | loss: 1.63275 | val_0_mae: 1.63945 |  0:00:16s\n",
      "epoch 83 | loss: 1.79301 | val_0_mae: 1.48645 |  0:00:16s\n",
      "epoch 84 | loss: 1.65868 | val_0_mae: 1.90493 |  0:00:17s\n",
      "epoch 85 | loss: 1.80714 | val_0_mae: 1.44035 |  0:00:17s\n",
      "epoch 86 | loss: 1.60958 | val_0_mae: 1.42439 |  0:00:17s\n",
      "epoch 87 | loss: 1.63357 | val_0_mae: 1.86494 |  0:00:17s\n",
      "epoch 88 | loss: 1.7703  | val_0_mae: 1.52388 |  0:00:17s\n",
      "epoch 89 | loss: 1.86328 | val_0_mae: 1.44517 |  0:00:17s\n",
      "epoch 90 | loss: 1.58883 | val_0_mae: 1.55113 |  0:00:18s\n",
      "epoch 91 | loss: 1.81007 | val_0_mae: 1.44385 |  0:00:18s\n",
      "epoch 92 | loss: 1.85307 | val_0_mae: 1.90726 |  0:00:18s\n",
      "epoch 93 | loss: 1.93183 | val_0_mae: 1.46867 |  0:00:18s\n",
      "epoch 94 | loss: 1.74436 | val_0_mae: 1.534   |  0:00:18s\n",
      "epoch 95 | loss: 1.75675 | val_0_mae: 1.61966 |  0:00:19s\n",
      "epoch 96 | loss: 1.69206 | val_0_mae: 1.62466 |  0:00:19s\n",
      "epoch 97 | loss: 1.69132 | val_0_mae: 1.53802 |  0:00:19s\n",
      "epoch 98 | loss: 1.55756 | val_0_mae: 1.70237 |  0:00:19s\n",
      "epoch 99 | loss: 1.56199 | val_0_mae: 1.46459 |  0:00:19s\n",
      "epoch 100| loss: 1.54201 | val_0_mae: 1.5743  |  0:00:20s\n",
      "epoch 101| loss: 1.53505 | val_0_mae: 1.45505 |  0:00:20s\n",
      "epoch 102| loss: 1.5291  | val_0_mae: 1.80649 |  0:00:20s\n",
      "epoch 103| loss: 1.63549 | val_0_mae: 1.55903 |  0:00:20s\n",
      "epoch 104| loss: 1.88306 | val_0_mae: 1.64847 |  0:00:20s\n",
      "epoch 105| loss: 2.03743 | val_0_mae: 2.02508 |  0:00:21s\n",
      "epoch 106| loss: 1.96875 | val_0_mae: 1.61481 |  0:00:21s\n",
      "epoch 107| loss: 1.69822 | val_0_mae: 1.79519 |  0:00:21s\n",
      "epoch 108| loss: 1.92041 | val_0_mae: 1.4828  |  0:00:21s\n",
      "epoch 109| loss: 1.63231 | val_0_mae: 1.68132 |  0:00:21s\n",
      "epoch 110| loss: 1.58425 | val_0_mae: 1.4114  |  0:00:22s\n",
      "epoch 111| loss: 1.61672 | val_0_mae: 1.46003 |  0:00:22s\n",
      "epoch 112| loss: 1.57028 | val_0_mae: 1.37331 |  0:00:22s\n",
      "epoch 113| loss: 1.66071 | val_0_mae: 1.50549 |  0:00:22s\n",
      "epoch 114| loss: 1.5489  | val_0_mae: 1.3813  |  0:00:22s\n",
      "epoch 115| loss: 1.56164 | val_0_mae: 1.42477 |  0:00:23s\n",
      "epoch 116| loss: 1.67061 | val_0_mae: 1.48381 |  0:00:23s\n",
      "epoch 117| loss: 1.75147 | val_0_mae: 1.61321 |  0:00:23s\n",
      "epoch 118| loss: 1.74084 | val_0_mae: 1.39334 |  0:00:23s\n",
      "epoch 119| loss: 1.76023 | val_0_mae: 1.44786 |  0:00:23s\n",
      "epoch 120| loss: 1.5801  | val_0_mae: 1.48805 |  0:00:24s\n",
      "epoch 121| loss: 1.73779 | val_0_mae: 1.44612 |  0:00:24s\n",
      "epoch 122| loss: 1.57089 | val_0_mae: 1.61129 |  0:00:24s\n",
      "epoch 123| loss: 1.58478 | val_0_mae: 1.50886 |  0:00:24s\n",
      "epoch 124| loss: 1.58159 | val_0_mae: 1.39624 |  0:00:24s\n",
      "epoch 125| loss: 1.53721 | val_0_mae: 1.3978  |  0:00:25s\n",
      "epoch 126| loss: 1.53898 | val_0_mae: 1.40761 |  0:00:25s\n",
      "epoch 127| loss: 1.58033 | val_0_mae: 1.37498 |  0:00:25s\n",
      "epoch 128| loss: 1.55631 | val_0_mae: 1.41223 |  0:00:25s\n",
      "epoch 129| loss: 1.49232 | val_0_mae: 1.3536  |  0:00:25s\n",
      "epoch 130| loss: 1.4834  | val_0_mae: 1.35977 |  0:00:26s\n",
      "epoch 131| loss: 1.64711 | val_0_mae: 1.48663 |  0:00:26s\n",
      "epoch 132| loss: 1.71346 | val_0_mae: 1.4395  |  0:00:26s\n",
      "epoch 133| loss: 1.76987 | val_0_mae: 1.39878 |  0:00:26s\n",
      "epoch 134| loss: 1.59307 | val_0_mae: 1.40528 |  0:00:26s\n",
      "epoch 135| loss: 1.51486 | val_0_mae: 1.37962 |  0:00:27s\n",
      "epoch 136| loss: 1.52999 | val_0_mae: 1.39651 |  0:00:27s\n",
      "epoch 137| loss: 1.50527 | val_0_mae: 1.38062 |  0:00:27s\n",
      "epoch 138| loss: 1.45615 | val_0_mae: 1.3919  |  0:00:27s\n",
      "epoch 139| loss: 1.55282 | val_0_mae: 1.36906 |  0:00:27s\n",
      "epoch 140| loss: 1.45902 | val_0_mae: 1.40375 |  0:00:27s\n",
      "epoch 141| loss: 1.56964 | val_0_mae: 1.46311 |  0:00:28s\n",
      "epoch 142| loss: 1.47276 | val_0_mae: 1.46942 |  0:00:28s\n",
      "epoch 143| loss: 1.53501 | val_0_mae: 1.36828 |  0:00:28s\n",
      "epoch 144| loss: 1.58618 | val_0_mae: 1.3653  |  0:00:28s\n",
      "epoch 145| loss: 1.50257 | val_0_mae: 1.37632 |  0:00:28s\n",
      "epoch 146| loss: 1.44719 | val_0_mae: 1.49265 |  0:00:29s\n",
      "epoch 147| loss: 1.55377 | val_0_mae: 1.37497 |  0:00:29s\n",
      "epoch 148| loss: 1.44415 | val_0_mae: 1.36959 |  0:00:29s\n",
      "epoch 149| loss: 1.61438 | val_0_mae: 1.37147 |  0:00:29s\n",
      "epoch 150| loss: 1.81534 | val_0_mae: 1.47755 |  0:00:29s\n",
      "epoch 151| loss: 1.61868 | val_0_mae: 1.44276 |  0:00:30s\n",
      "epoch 152| loss: 1.67026 | val_0_mae: 1.37761 |  0:00:30s\n",
      "epoch 153| loss: 1.7621  | val_0_mae: 1.52062 |  0:00:30s\n",
      "epoch 154| loss: 1.64693 | val_0_mae: 1.36919 |  0:00:30s\n",
      "epoch 155| loss: 1.70626 | val_0_mae: 1.50158 |  0:00:30s\n",
      "epoch 156| loss: 1.60125 | val_0_mae: 1.44131 |  0:00:31s\n",
      "epoch 157| loss: 1.63791 | val_0_mae: 1.41331 |  0:00:31s\n",
      "epoch 158| loss: 1.49438 | val_0_mae: 1.34533 |  0:00:31s\n",
      "epoch 159| loss: 1.45899 | val_0_mae: 1.37434 |  0:00:31s\n",
      "epoch 160| loss: 1.52676 | val_0_mae: 1.45061 |  0:00:31s\n",
      "epoch 161| loss: 1.45528 | val_0_mae: 1.34737 |  0:00:32s\n",
      "epoch 162| loss: 1.40667 | val_0_mae: 1.42936 |  0:00:32s\n",
      "epoch 163| loss: 1.48771 | val_0_mae: 1.42091 |  0:00:32s\n",
      "epoch 164| loss: 1.52689 | val_0_mae: 1.39081 |  0:00:32s\n",
      "epoch 165| loss: 1.45047 | val_0_mae: 1.44621 |  0:00:32s\n",
      "epoch 166| loss: 1.60061 | val_0_mae: 1.4138  |  0:00:33s\n",
      "epoch 167| loss: 1.44652 | val_0_mae: 1.34973 |  0:00:33s\n",
      "epoch 168| loss: 1.48805 | val_0_mae: 1.33826 |  0:00:33s\n",
      "epoch 169| loss: 1.51864 | val_0_mae: 1.40798 |  0:00:33s\n",
      "epoch 170| loss: 1.55313 | val_0_mae: 1.47389 |  0:00:33s\n",
      "epoch 171| loss: 1.55103 | val_0_mae: 1.35577 |  0:00:34s\n",
      "epoch 172| loss: 1.50712 | val_0_mae: 1.35742 |  0:00:34s\n",
      "epoch 173| loss: 1.45984 | val_0_mae: 1.39831 |  0:00:34s\n",
      "epoch 174| loss: 1.39569 | val_0_mae: 1.35348 |  0:00:34s\n",
      "epoch 175| loss: 1.47444 | val_0_mae: 1.35144 |  0:00:34s\n",
      "epoch 176| loss: 1.41732 | val_0_mae: 1.4221  |  0:00:35s\n",
      "epoch 177| loss: 1.49734 | val_0_mae: 1.38479 |  0:00:35s\n",
      "epoch 178| loss: 1.39785 | val_0_mae: 1.36895 |  0:00:35s\n",
      "epoch 179| loss: 1.46083 | val_0_mae: 1.4482  |  0:00:35s\n",
      "epoch 180| loss: 1.54729 | val_0_mae: 1.39018 |  0:00:35s\n",
      "epoch 181| loss: 1.47906 | val_0_mae: 1.34831 |  0:00:36s\n",
      "epoch 182| loss: 1.4602  | val_0_mae: 1.6077  |  0:00:36s\n",
      "epoch 183| loss: 1.67123 | val_0_mae: 1.35244 |  0:00:36s\n",
      "epoch 184| loss: 1.53004 | val_0_mae: 1.39417 |  0:00:36s\n",
      "epoch 185| loss: 1.50774 | val_0_mae: 1.43834 |  0:00:36s\n",
      "epoch 186| loss: 1.60516 | val_0_mae: 1.3399  |  0:00:37s\n",
      "epoch 187| loss: 1.40076 | val_0_mae: 1.37142 |  0:00:37s\n",
      "epoch 188| loss: 1.53303 | val_0_mae: 1.35641 |  0:00:37s\n",
      "epoch 189| loss: 1.58118 | val_0_mae: 1.33807 |  0:00:37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 190| loss: 1.51013 | val_0_mae: 1.42183 |  0:00:37s\n",
      "epoch 191| loss: 1.61511 | val_0_mae: 1.39986 |  0:00:38s\n",
      "epoch 192| loss: 1.60811 | val_0_mae: 1.81797 |  0:00:38s\n",
      "epoch 193| loss: 1.46983 | val_0_mae: 2.00723 |  0:00:38s\n",
      "epoch 194| loss: 1.59961 | val_0_mae: 1.6452  |  0:00:38s\n",
      "epoch 195| loss: 1.56231 | val_0_mae: 1.52899 |  0:00:38s\n",
      "epoch 196| loss: 1.49307 | val_0_mae: 1.44543 |  0:00:39s\n",
      "epoch 197| loss: 1.45945 | val_0_mae: 1.33044 |  0:00:39s\n",
      "epoch 198| loss: 1.42186 | val_0_mae: 1.41956 |  0:00:39s\n",
      "epoch 199| loss: 1.42815 | val_0_mae: 1.36009 |  0:00:39s\n",
      "epoch 200| loss: 1.41652 | val_0_mae: 1.34752 |  0:00:39s\n",
      "epoch 201| loss: 1.48263 | val_0_mae: 1.34228 |  0:00:40s\n",
      "epoch 202| loss: 1.44308 | val_0_mae: 1.36891 |  0:00:40s\n",
      "epoch 203| loss: 1.39566 | val_0_mae: 1.4718  |  0:00:40s\n",
      "epoch 204| loss: 1.49168 | val_0_mae: 1.38717 |  0:00:40s\n",
      "epoch 205| loss: 1.36371 | val_0_mae: 1.36308 |  0:00:40s\n",
      "epoch 206| loss: 1.42768 | val_0_mae: 1.40766 |  0:00:41s\n",
      "epoch 207| loss: 1.3954  | val_0_mae: 1.39102 |  0:00:41s\n",
      "epoch 208| loss: 1.4256  | val_0_mae: 1.38298 |  0:00:41s\n",
      "epoch 209| loss: 1.40888 | val_0_mae: 1.36472 |  0:00:41s\n",
      "epoch 210| loss: 1.37571 | val_0_mae: 1.40279 |  0:00:41s\n",
      "epoch 211| loss: 1.38106 | val_0_mae: 1.37094 |  0:00:42s\n",
      "epoch 212| loss: 1.36625 | val_0_mae: 1.45622 |  0:00:42s\n",
      "epoch 213| loss: 1.43707 | val_0_mae: 1.36698 |  0:00:42s\n",
      "epoch 214| loss: 1.42686 | val_0_mae: 1.39985 |  0:00:42s\n",
      "epoch 215| loss: 1.53613 | val_0_mae: 1.48191 |  0:00:42s\n",
      "epoch 216| loss: 1.51145 | val_0_mae: 1.37694 |  0:00:43s\n",
      "epoch 217| loss: 1.4068  | val_0_mae: 1.42818 |  0:00:43s\n",
      "epoch 218| loss: 1.56624 | val_0_mae: 1.39016 |  0:00:43s\n",
      "epoch 219| loss: 1.45791 | val_0_mae: 1.41371 |  0:00:43s\n",
      "epoch 220| loss: 1.55624 | val_0_mae: 1.36345 |  0:00:43s\n",
      "epoch 221| loss: 1.44607 | val_0_mae: 1.39165 |  0:00:44s\n",
      "epoch 222| loss: 1.40302 | val_0_mae: 1.4815  |  0:00:44s\n",
      "epoch 223| loss: 1.45227 | val_0_mae: 1.41169 |  0:00:44s\n",
      "epoch 224| loss: 1.43978 | val_0_mae: 1.46147 |  0:00:44s\n",
      "epoch 225| loss: 1.4768  | val_0_mae: 1.38943 |  0:00:44s\n",
      "epoch 226| loss: 1.71574 | val_0_mae: 1.59659 |  0:00:45s\n",
      "epoch 227| loss: 1.54998 | val_0_mae: 1.36615 |  0:00:45s\n",
      "epoch 228| loss: 1.39415 | val_0_mae: 1.41197 |  0:00:45s\n",
      "epoch 229| loss: 1.52587 | val_0_mae: 1.35736 |  0:00:45s\n",
      "epoch 230| loss: 1.39969 | val_0_mae: 1.42806 |  0:00:45s\n",
      "epoch 231| loss: 1.50976 | val_0_mae: 1.39921 |  0:00:46s\n",
      "epoch 232| loss: 1.40852 | val_0_mae: 1.36583 |  0:00:46s\n",
      "epoch 233| loss: 1.42861 | val_0_mae: 1.58944 |  0:00:46s\n",
      "epoch 234| loss: 1.43983 | val_0_mae: 1.36686 |  0:00:46s\n",
      "epoch 235| loss: 1.61939 | val_0_mae: 1.45731 |  0:00:46s\n",
      "epoch 236| loss: 1.42728 | val_0_mae: 1.46244 |  0:00:47s\n",
      "epoch 237| loss: 1.46975 | val_0_mae: 1.42785 |  0:00:47s\n",
      "epoch 238| loss: 1.50854 | val_0_mae: 1.4064  |  0:00:47s\n",
      "epoch 239| loss: 1.44768 | val_0_mae: 1.4363  |  0:00:47s\n",
      "epoch 240| loss: 1.71712 | val_0_mae: 1.44417 |  0:00:48s\n",
      "epoch 241| loss: 1.4248  | val_0_mae: 1.42623 |  0:00:48s\n",
      "epoch 242| loss: 1.51509 | val_0_mae: 1.40178 |  0:00:48s\n",
      "epoch 243| loss: 1.50032 | val_0_mae: 1.38224 |  0:00:48s\n",
      "epoch 244| loss: 1.40579 | val_0_mae: 1.41114 |  0:00:48s\n",
      "epoch 245| loss: 1.42778 | val_0_mae: 1.43773 |  0:00:49s\n",
      "epoch 246| loss: 1.5261  | val_0_mae: 1.48244 |  0:00:49s\n",
      "epoch 247| loss: 1.52642 | val_0_mae: 1.45073 |  0:00:49s\n",
      "\n",
      "Early stopping occured at epoch 247 with best_epoch = 197 and best_val_0_mae = 1.33044\n",
      "Best weights from best epoch are automatically used!\n",
      "XGB: 1.2806456301297058\n",
      "CAT: 1.3479541539993778\n",
      "TAB: 1.3304407134017182\n",
      "MHA: 2.4303925799616217\n",
      "CNN_TINY: 3.306434337182842\n",
      "CNN_BIG: 2.2964659202031594\n",
      "MEAN MHA+TINY+BIG: 1.9702616580323546\n",
      "MEAN XGB+CAT+TAB: 1.2716934649037537\n",
      "****************************************************************************************************\n",
      "============================================================\n",
      "Num Fold: 4\n",
      "Train segments: 3545 Val segments: 886\n",
      "============================================================\n",
      "Device used : cuda\n",
      "Training xgboost\n",
      "[19:19:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:19:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Training catboost\n",
      "Training TabNet\n",
      "epoch 0  | loss: 23.52119| val_0_mae: 26.23713|  0:00:00s\n",
      "epoch 1  | loss: 22.57913| val_0_mae: 22.68712|  0:00:00s\n",
      "epoch 2  | loss: 21.91867| val_0_mae: 20.90479|  0:00:00s\n",
      "epoch 3  | loss: 21.26364| val_0_mae: 20.07006|  0:00:00s\n",
      "epoch 4  | loss: 20.43887| val_0_mae: 17.02769|  0:00:01s\n",
      "epoch 5  | loss: 19.42245| val_0_mae: 16.4924 |  0:00:01s\n",
      "epoch 6  | loss: 18.08077| val_0_mae: 15.70664|  0:00:01s\n",
      "epoch 7  | loss: 16.26253| val_0_mae: 13.03933|  0:00:01s\n",
      "epoch 8  | loss: 14.11535| val_0_mae: 11.30511|  0:00:01s\n",
      "epoch 9  | loss: 11.6917 | val_0_mae: 10.46311|  0:00:02s\n",
      "epoch 10 | loss: 9.10452 | val_0_mae: 11.2486 |  0:00:02s\n",
      "epoch 11 | loss: 6.4407  | val_0_mae: 20.02662|  0:00:02s\n",
      "epoch 12 | loss: 4.20214 | val_0_mae: 25.66473|  0:00:02s\n",
      "epoch 13 | loss: 3.72957 | val_0_mae: 23.19732|  0:00:02s\n",
      "epoch 14 | loss: 3.64752 | val_0_mae: 20.66896|  0:00:03s\n",
      "epoch 15 | loss: 3.37669 | val_0_mae: 16.31257|  0:00:03s\n",
      "epoch 16 | loss: 2.8528  | val_0_mae: 13.98755|  0:00:03s\n",
      "epoch 17 | loss: 2.70426 | val_0_mae: 14.09898|  0:00:03s\n",
      "epoch 18 | loss: 2.32071 | val_0_mae: 13.2932 |  0:00:03s\n",
      "epoch 19 | loss: 2.48164 | val_0_mae: 12.98009|  0:00:04s\n",
      "epoch 20 | loss: 2.26362 | val_0_mae: 12.17663|  0:00:04s\n",
      "epoch 21 | loss: 2.07725 | val_0_mae: 10.72673|  0:00:04s\n",
      "epoch 22 | loss: 2.01697 | val_0_mae: 11.90789|  0:00:04s\n",
      "epoch 23 | loss: 2.08233 | val_0_mae: 11.56528|  0:00:04s\n",
      "epoch 24 | loss: 1.83368 | val_0_mae: 9.64207 |  0:00:05s\n",
      "epoch 25 | loss: 1.84058 | val_0_mae: 10.43207|  0:00:05s\n",
      "epoch 26 | loss: 2.05132 | val_0_mae: 10.09444|  0:00:05s\n",
      "epoch 27 | loss: 1.8457  | val_0_mae: 8.94489 |  0:00:05s\n",
      "epoch 28 | loss: 1.89231 | val_0_mae: 8.21525 |  0:00:05s\n",
      "epoch 29 | loss: 1.89669 | val_0_mae: 7.53287 |  0:00:06s\n",
      "epoch 30 | loss: 1.7948  | val_0_mae: 7.19163 |  0:00:06s\n",
      "epoch 31 | loss: 1.94078 | val_0_mae: 6.61155 |  0:00:06s\n",
      "epoch 32 | loss: 1.85015 | val_0_mae: 6.68601 |  0:00:06s\n",
      "epoch 33 | loss: 1.88122 | val_0_mae: 6.43261 |  0:00:06s\n",
      "epoch 34 | loss: 1.84025 | val_0_mae: 5.59317 |  0:00:07s\n",
      "epoch 35 | loss: 1.7795  | val_0_mae: 5.44022 |  0:00:07s\n",
      "epoch 36 | loss: 1.92373 | val_0_mae: 5.96688 |  0:00:07s\n",
      "epoch 37 | loss: 1.84357 | val_0_mae: 5.91928 |  0:00:07s\n",
      "epoch 38 | loss: 1.85551 | val_0_mae: 5.63341 |  0:00:08s\n",
      "epoch 39 | loss: 1.71384 | val_0_mae: 4.82706 |  0:00:08s\n",
      "epoch 40 | loss: 1.7951  | val_0_mae: 5.21572 |  0:00:08s\n",
      "epoch 41 | loss: 1.79027 | val_0_mae: 5.0253  |  0:00:08s\n",
      "epoch 42 | loss: 1.68072 | val_0_mae: 4.95527 |  0:00:08s\n",
      "epoch 43 | loss: 1.75068 | val_0_mae: 4.91951 |  0:00:08s\n",
      "epoch 44 | loss: 1.88876 | val_0_mae: 5.158   |  0:00:09s\n",
      "epoch 45 | loss: 1.74988 | val_0_mae: 3.52792 |  0:00:09s\n",
      "epoch 46 | loss: 1.71454 | val_0_mae: 3.91599 |  0:00:09s\n",
      "epoch 47 | loss: 1.68695 | val_0_mae: 3.53513 |  0:00:09s\n",
      "epoch 48 | loss: 1.63957 | val_0_mae: 4.13107 |  0:00:09s\n",
      "epoch 49 | loss: 1.67484 | val_0_mae: 2.57625 |  0:00:10s\n",
      "epoch 50 | loss: 1.80749 | val_0_mae: 3.26042 |  0:00:10s\n",
      "epoch 51 | loss: 1.64833 | val_0_mae: 3.36266 |  0:00:10s\n",
      "epoch 52 | loss: 1.69176 | val_0_mae: 3.50818 |  0:00:10s\n",
      "epoch 53 | loss: 1.81096 | val_0_mae: 3.58205 |  0:00:10s\n",
      "epoch 54 | loss: 1.73454 | val_0_mae: 2.419   |  0:00:11s\n",
      "epoch 55 | loss: 1.91448 | val_0_mae: 2.51192 |  0:00:11s\n",
      "epoch 56 | loss: 1.74549 | val_0_mae: 3.40001 |  0:00:11s\n",
      "epoch 57 | loss: 1.68896 | val_0_mae: 2.49374 |  0:00:11s\n",
      "epoch 58 | loss: 1.76953 | val_0_mae: 2.38849 |  0:00:11s\n",
      "epoch 59 | loss: 1.72754 | val_0_mae: 3.1434  |  0:00:12s\n",
      "epoch 60 | loss: 1.67468 | val_0_mae: 2.52219 |  0:00:12s\n",
      "epoch 61 | loss: 1.61953 | val_0_mae: 2.67348 |  0:00:12s\n",
      "epoch 62 | loss: 1.60372 | val_0_mae: 2.03574 |  0:00:12s\n",
      "epoch 63 | loss: 1.81373 | val_0_mae: 1.93494 |  0:00:12s\n",
      "epoch 64 | loss: 1.66223 | val_0_mae: 1.91811 |  0:00:13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65 | loss: 1.66181 | val_0_mae: 1.74484 |  0:00:13s\n",
      "epoch 66 | loss: 1.63035 | val_0_mae: 2.10875 |  0:00:13s\n",
      "epoch 67 | loss: 1.63717 | val_0_mae: 2.25269 |  0:00:13s\n",
      "epoch 68 | loss: 1.56326 | val_0_mae: 2.50211 |  0:00:14s\n",
      "epoch 69 | loss: 1.69684 | val_0_mae: 2.24847 |  0:00:14s\n",
      "epoch 70 | loss: 1.58366 | val_0_mae: 2.07946 |  0:00:14s\n",
      "epoch 71 | loss: 1.79062 | val_0_mae: 2.27482 |  0:00:14s\n",
      "epoch 72 | loss: 1.54613 | val_0_mae: 2.07494 |  0:00:14s\n",
      "epoch 73 | loss: 1.69961 | val_0_mae: 2.14863 |  0:00:14s\n",
      "epoch 74 | loss: 1.67966 | val_0_mae: 2.13456 |  0:00:15s\n",
      "epoch 75 | loss: 1.69212 | val_0_mae: 1.42296 |  0:00:15s\n",
      "epoch 76 | loss: 1.76024 | val_0_mae: 1.59236 |  0:00:15s\n",
      "epoch 77 | loss: 1.49638 | val_0_mae: 1.84424 |  0:00:15s\n",
      "epoch 78 | loss: 1.4975  | val_0_mae: 1.5413  |  0:00:15s\n",
      "epoch 79 | loss: 1.68893 | val_0_mae: 1.54238 |  0:00:16s\n",
      "epoch 80 | loss: 1.49459 | val_0_mae: 2.04976 |  0:00:16s\n",
      "epoch 81 | loss: 1.51149 | val_0_mae: 1.57709 |  0:00:16s\n",
      "epoch 82 | loss: 1.50908 | val_0_mae: 2.11752 |  0:00:16s\n",
      "epoch 83 | loss: 1.75805 | val_0_mae: 2.15319 |  0:00:16s\n",
      "epoch 84 | loss: 1.56049 | val_0_mae: 1.50784 |  0:00:17s\n",
      "epoch 85 | loss: 1.66469 | val_0_mae: 1.77045 |  0:00:17s\n",
      "epoch 86 | loss: 1.56627 | val_0_mae: 1.5926  |  0:00:17s\n",
      "epoch 87 | loss: 1.50797 | val_0_mae: 1.58    |  0:00:17s\n",
      "epoch 88 | loss: 1.50986 | val_0_mae: 1.76142 |  0:00:17s\n",
      "epoch 89 | loss: 1.73333 | val_0_mae: 1.42113 |  0:00:18s\n",
      "epoch 90 | loss: 1.68798 | val_0_mae: 1.87736 |  0:00:18s\n",
      "epoch 91 | loss: 1.65654 | val_0_mae: 1.41088 |  0:00:18s\n",
      "epoch 92 | loss: 1.57361 | val_0_mae: 1.54333 |  0:00:18s\n",
      "epoch 93 | loss: 1.61888 | val_0_mae: 1.68934 |  0:00:18s\n",
      "epoch 94 | loss: 1.53383 | val_0_mae: 1.46608 |  0:00:19s\n",
      "epoch 95 | loss: 1.51508 | val_0_mae: 1.51873 |  0:00:19s\n",
      "epoch 96 | loss: 1.41913 | val_0_mae: 1.44891 |  0:00:19s\n",
      "epoch 97 | loss: 1.61823 | val_0_mae: 1.41319 |  0:00:19s\n",
      "epoch 98 | loss: 1.57154 | val_0_mae: 1.4739  |  0:00:19s\n",
      "epoch 99 | loss: 1.55899 | val_0_mae: 1.408   |  0:00:20s\n",
      "epoch 100| loss: 1.51483 | val_0_mae: 1.48937 |  0:00:20s\n",
      "epoch 101| loss: 1.52351 | val_0_mae: 1.39374 |  0:00:20s\n",
      "epoch 102| loss: 1.49879 | val_0_mae: 1.47528 |  0:00:20s\n",
      "epoch 103| loss: 1.565   | val_0_mae: 1.47527 |  0:00:20s\n",
      "epoch 104| loss: 1.61332 | val_0_mae: 1.58483 |  0:00:21s\n",
      "epoch 105| loss: 1.60661 | val_0_mae: 1.72956 |  0:00:21s\n",
      "epoch 106| loss: 1.57174 | val_0_mae: 1.42883 |  0:00:21s\n",
      "epoch 107| loss: 1.62154 | val_0_mae: 1.79575 |  0:00:21s\n",
      "epoch 108| loss: 1.44281 | val_0_mae: 1.46356 |  0:00:21s\n",
      "epoch 109| loss: 1.57113 | val_0_mae: 1.5885  |  0:00:22s\n",
      "epoch 110| loss: 1.43812 | val_0_mae: 1.55967 |  0:00:22s\n",
      "epoch 111| loss: 1.50145 | val_0_mae: 1.4241  |  0:00:22s\n",
      "epoch 112| loss: 1.45721 | val_0_mae: 1.40401 |  0:00:22s\n",
      "epoch 113| loss: 1.4687  | val_0_mae: 1.51184 |  0:00:22s\n",
      "epoch 114| loss: 1.47343 | val_0_mae: 1.62922 |  0:00:23s\n",
      "epoch 115| loss: 1.39993 | val_0_mae: 1.42795 |  0:00:23s\n",
      "epoch 116| loss: 1.56447 | val_0_mae: 1.37466 |  0:00:23s\n",
      "epoch 117| loss: 1.77953 | val_0_mae: 1.54943 |  0:00:23s\n",
      "epoch 118| loss: 1.55969 | val_0_mae: 1.43983 |  0:00:23s\n",
      "epoch 119| loss: 1.64039 | val_0_mae: 1.6564  |  0:00:23s\n",
      "epoch 120| loss: 1.595   | val_0_mae: 1.49249 |  0:00:24s\n",
      "epoch 121| loss: 1.57469 | val_0_mae: 1.4487  |  0:00:24s\n",
      "epoch 122| loss: 1.6233  | val_0_mae: 1.47857 |  0:00:24s\n",
      "epoch 123| loss: 1.43274 | val_0_mae: 1.53138 |  0:00:24s\n",
      "epoch 124| loss: 1.46907 | val_0_mae: 1.38387 |  0:00:24s\n",
      "epoch 125| loss: 1.48826 | val_0_mae: 1.37631 |  0:00:25s\n",
      "epoch 126| loss: 1.41718 | val_0_mae: 1.54484 |  0:00:25s\n",
      "epoch 127| loss: 1.55972 | val_0_mae: 1.60434 |  0:00:25s\n",
      "epoch 128| loss: 1.46918 | val_0_mae: 1.40884 |  0:00:25s\n",
      "epoch 129| loss: 1.47635 | val_0_mae: 1.36269 |  0:00:25s\n",
      "epoch 130| loss: 1.42301 | val_0_mae: 1.35469 |  0:00:26s\n",
      "epoch 131| loss: 1.49955 | val_0_mae: 1.32846 |  0:00:26s\n",
      "epoch 132| loss: 1.4322  | val_0_mae: 1.33111 |  0:00:26s\n",
      "epoch 133| loss: 1.4486  | val_0_mae: 1.31565 |  0:00:26s\n",
      "epoch 134| loss: 1.4555  | val_0_mae: 1.38033 |  0:00:26s\n",
      "epoch 135| loss: 1.57059 | val_0_mae: 1.4705  |  0:00:27s\n",
      "epoch 136| loss: 1.49945 | val_0_mae: 1.51789 |  0:00:27s\n",
      "epoch 137| loss: 2.20071 | val_0_mae: 1.86042 |  0:00:27s\n",
      "epoch 138| loss: 1.81566 | val_0_mae: 1.32342 |  0:00:27s\n",
      "epoch 139| loss: 1.62374 | val_0_mae: 1.34859 |  0:00:27s\n",
      "epoch 140| loss: 1.41316 | val_0_mae: 1.34943 |  0:00:28s\n",
      "epoch 141| loss: 1.49136 | val_0_mae: 1.33838 |  0:00:28s\n",
      "epoch 142| loss: 1.48795 | val_0_mae: 1.37312 |  0:00:28s\n",
      "epoch 143| loss: 1.52896 | val_0_mae: 1.36466 |  0:00:28s\n",
      "epoch 144| loss: 1.43163 | val_0_mae: 1.45086 |  0:00:28s\n",
      "epoch 145| loss: 1.87695 | val_0_mae: 1.48219 |  0:00:29s\n",
      "epoch 146| loss: 1.54922 | val_0_mae: 1.33016 |  0:00:29s\n",
      "epoch 147| loss: 1.61036 | val_0_mae: 1.51661 |  0:00:29s\n",
      "epoch 148| loss: 1.51919 | val_0_mae: 1.36613 |  0:00:29s\n",
      "epoch 149| loss: 1.49902 | val_0_mae: 1.43546 |  0:00:29s\n",
      "epoch 150| loss: 1.83923 | val_0_mae: 1.53453 |  0:00:29s\n",
      "epoch 151| loss: 1.54589 | val_0_mae: 1.52942 |  0:00:30s\n",
      "epoch 152| loss: 1.53923 | val_0_mae: 1.4302  |  0:00:30s\n",
      "epoch 153| loss: 1.57384 | val_0_mae: 1.34216 |  0:00:30s\n",
      "epoch 154| loss: 1.39557 | val_0_mae: 1.37789 |  0:00:30s\n",
      "epoch 155| loss: 1.38007 | val_0_mae: 1.45521 |  0:00:30s\n",
      "epoch 156| loss: 1.3562  | val_0_mae: 1.45253 |  0:00:31s\n",
      "epoch 157| loss: 1.50938 | val_0_mae: 1.33173 |  0:00:31s\n",
      "epoch 158| loss: 1.38757 | val_0_mae: 1.42626 |  0:00:31s\n",
      "epoch 159| loss: 1.43152 | val_0_mae: 1.45742 |  0:00:31s\n",
      "epoch 160| loss: 1.54581 | val_0_mae: 1.37141 |  0:00:31s\n",
      "epoch 161| loss: 1.51051 | val_0_mae: 1.35084 |  0:00:32s\n",
      "epoch 162| loss: 1.4706  | val_0_mae: 1.40033 |  0:00:32s\n",
      "epoch 163| loss: 1.3844  | val_0_mae: 1.41947 |  0:00:32s\n",
      "epoch 164| loss: 1.41037 | val_0_mae: 1.33892 |  0:00:32s\n",
      "epoch 165| loss: 1.53639 | val_0_mae: 1.35957 |  0:00:32s\n",
      "epoch 166| loss: 1.54045 | val_0_mae: 1.37905 |  0:00:33s\n",
      "epoch 167| loss: 1.4651  | val_0_mae: 1.33231 |  0:00:33s\n",
      "epoch 168| loss: 1.38568 | val_0_mae: 1.36691 |  0:00:33s\n",
      "epoch 169| loss: 1.32833 | val_0_mae: 1.42796 |  0:00:33s\n",
      "epoch 170| loss: 1.38801 | val_0_mae: 1.32062 |  0:00:33s\n",
      "epoch 171| loss: 1.38326 | val_0_mae: 1.31338 |  0:00:34s\n",
      "epoch 172| loss: 1.46043 | val_0_mae: 1.38356 |  0:00:34s\n",
      "epoch 173| loss: 1.38301 | val_0_mae: 1.32747 |  0:00:34s\n",
      "epoch 174| loss: 1.47021 | val_0_mae: 1.34824 |  0:00:34s\n",
      "epoch 175| loss: 1.36971 | val_0_mae: 1.4147  |  0:00:34s\n",
      "epoch 176| loss: 1.51954 | val_0_mae: 1.30173 |  0:00:35s\n",
      "epoch 177| loss: 1.40143 | val_0_mae: 1.35334 |  0:00:35s\n",
      "epoch 178| loss: 1.34351 | val_0_mae: 1.32575 |  0:00:35s\n",
      "epoch 179| loss: 1.38283 | val_0_mae: 1.30944 |  0:00:35s\n",
      "epoch 180| loss: 1.36144 | val_0_mae: 1.3743  |  0:00:35s\n",
      "epoch 181| loss: 1.37902 | val_0_mae: 1.42809 |  0:00:36s\n",
      "epoch 182| loss: 1.40542 | val_0_mae: 1.39192 |  0:00:36s\n",
      "epoch 183| loss: 1.44038 | val_0_mae: 1.30842 |  0:00:36s\n",
      "epoch 184| loss: 1.43098 | val_0_mae: 1.33357 |  0:00:36s\n",
      "epoch 185| loss: 1.4835  | val_0_mae: 1.49129 |  0:00:36s\n",
      "epoch 186| loss: 1.6216  | val_0_mae: 1.3088  |  0:00:37s\n",
      "epoch 187| loss: 1.61076 | val_0_mae: 1.3574  |  0:00:37s\n",
      "epoch 188| loss: 1.4601  | val_0_mae: 1.35431 |  0:00:37s\n",
      "epoch 189| loss: 1.69125 | val_0_mae: 1.36698 |  0:00:37s\n",
      "epoch 190| loss: 1.52952 | val_0_mae: 1.55043 |  0:00:37s\n",
      "epoch 191| loss: 1.71808 | val_0_mae: 1.32454 |  0:00:38s\n",
      "epoch 192| loss: 1.47561 | val_0_mae: 1.42797 |  0:00:38s\n",
      "epoch 193| loss: 1.63621 | val_0_mae: 1.38704 |  0:00:38s\n",
      "epoch 194| loss: 1.40107 | val_0_mae: 1.33086 |  0:00:38s\n",
      "epoch 195| loss: 1.37049 | val_0_mae: 1.41703 |  0:00:38s\n",
      "epoch 196| loss: 1.33409 | val_0_mae: 1.30606 |  0:00:39s\n",
      "epoch 197| loss: 1.62628 | val_0_mae: 1.32823 |  0:00:39s\n",
      "epoch 198| loss: 1.51374 | val_0_mae: 1.45964 |  0:00:39s\n",
      "epoch 199| loss: 1.48595 | val_0_mae: 1.38854 |  0:00:39s\n",
      "epoch 200| loss: 1.36893 | val_0_mae: 1.28777 |  0:00:39s\n",
      "epoch 201| loss: 1.28124 | val_0_mae: 1.29219 |  0:00:40s\n",
      "epoch 202| loss: 1.29617 | val_0_mae: 1.30557 |  0:00:40s\n",
      "epoch 203| loss: 1.28648 | val_0_mae: 1.27943 |  0:00:40s\n",
      "epoch 204| loss: 1.39179 | val_0_mae: 1.32155 |  0:00:40s\n",
      "epoch 205| loss: 1.36984 | val_0_mae: 1.34316 |  0:00:40s\n",
      "epoch 206| loss: 1.26986 | val_0_mae: 1.31429 |  0:00:41s\n",
      "epoch 207| loss: 1.35633 | val_0_mae: 1.4192  |  0:00:41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 208| loss: 1.54596 | val_0_mae: 1.41662 |  0:00:41s\n",
      "epoch 209| loss: 1.39509 | val_0_mae: 1.52095 |  0:00:41s\n",
      "epoch 210| loss: 1.49003 | val_0_mae: 1.37913 |  0:00:41s\n",
      "epoch 211| loss: 1.39831 | val_0_mae: 1.34951 |  0:00:42s\n",
      "epoch 212| loss: 1.34879 | val_0_mae: 1.34539 |  0:00:42s\n",
      "epoch 213| loss: 1.28637 | val_0_mae: 1.44044 |  0:00:42s\n",
      "epoch 214| loss: 1.30908 | val_0_mae: 1.34438 |  0:00:42s\n",
      "epoch 215| loss: 1.34929 | val_0_mae: 1.32617 |  0:00:42s\n",
      "epoch 216| loss: 1.32801 | val_0_mae: 1.32728 |  0:00:43s\n",
      "epoch 217| loss: 1.27159 | val_0_mae: 1.27399 |  0:00:43s\n",
      "epoch 218| loss: 1.36814 | val_0_mae: 1.31212 |  0:00:43s\n",
      "epoch 219| loss: 1.36163 | val_0_mae: 1.29237 |  0:00:43s\n",
      "epoch 220| loss: 1.29115 | val_0_mae: 1.2627  |  0:00:43s\n",
      "epoch 221| loss: 1.33392 | val_0_mae: 1.28326 |  0:00:44s\n",
      "epoch 222| loss: 1.31318 | val_0_mae: 1.29855 |  0:00:44s\n",
      "epoch 223| loss: 1.38136 | val_0_mae: 1.30239 |  0:00:44s\n",
      "epoch 224| loss: 1.45051 | val_0_mae: 1.3703  |  0:00:44s\n",
      "epoch 225| loss: 1.38392 | val_0_mae: 1.37178 |  0:00:44s\n",
      "epoch 226| loss: 1.55464 | val_0_mae: 1.34444 |  0:00:45s\n",
      "epoch 227| loss: 1.39679 | val_0_mae: 1.34992 |  0:00:45s\n",
      "epoch 228| loss: 1.35565 | val_0_mae: 1.31078 |  0:00:45s\n",
      "epoch 229| loss: 1.32716 | val_0_mae: 1.37687 |  0:00:45s\n",
      "epoch 230| loss: 1.32158 | val_0_mae: 1.39894 |  0:00:45s\n",
      "epoch 231| loss: 1.37934 | val_0_mae: 1.37142 |  0:00:46s\n",
      "epoch 232| loss: 1.33001 | val_0_mae: 1.31249 |  0:00:46s\n",
      "epoch 233| loss: 1.3059  | val_0_mae: 1.30439 |  0:00:46s\n",
      "epoch 234| loss: 1.32259 | val_0_mae: 1.39614 |  0:00:46s\n",
      "epoch 235| loss: 1.35224 | val_0_mae: 1.3987  |  0:00:46s\n",
      "epoch 236| loss: 1.46572 | val_0_mae: 1.37775 |  0:00:47s\n",
      "epoch 237| loss: 1.29044 | val_0_mae: 1.31543 |  0:00:47s\n",
      "epoch 238| loss: 1.35297 | val_0_mae: 1.3164  |  0:00:47s\n",
      "epoch 239| loss: 1.3755  | val_0_mae: 1.3362  |  0:00:47s\n",
      "epoch 240| loss: 1.25992 | val_0_mae: 1.35263 |  0:00:47s\n",
      "epoch 241| loss: 1.28078 | val_0_mae: 1.35541 |  0:00:47s\n",
      "epoch 242| loss: 1.2969  | val_0_mae: 1.31511 |  0:00:48s\n",
      "epoch 243| loss: 1.36715 | val_0_mae: 1.33526 |  0:00:48s\n",
      "epoch 244| loss: 1.30665 | val_0_mae: 1.30719 |  0:00:48s\n",
      "epoch 245| loss: 1.24857 | val_0_mae: 1.42361 |  0:00:48s\n",
      "epoch 246| loss: 1.30402 | val_0_mae: 1.34363 |  0:00:48s\n",
      "epoch 247| loss: 1.29492 | val_0_mae: 1.38376 |  0:00:49s\n",
      "epoch 248| loss: 1.31654 | val_0_mae: 1.28727 |  0:00:49s\n",
      "epoch 249| loss: 1.31091 | val_0_mae: 1.38528 |  0:00:49s\n",
      "Stop training because you reached max_epochs = 250 with best_epoch = 220 and best_val_0_mae = 1.2627\n",
      "Best weights from best epoch are automatically used!\n",
      "XGB: 1.2652471413050372\n",
      "CAT: 1.2844130754923906\n",
      "TAB: 1.2626952271496408\n",
      "MHA: 2.4325455942155743\n",
      "CNN_TINY: 3.1967265733634362\n",
      "CNN_BIG: 2.3301508310722348\n",
      "MEAN MHA+TINY+BIG: 1.989095957883748\n",
      "MEAN XGB+CAT+TAB: 1.22292594947907\n",
      "****************************************************************************************************\n",
      "============================================================\n",
      "Num Fold: 5\n",
      "Train segments: 3545 Val segments: 886\n",
      "============================================================\n",
      "Device used : cuda\n",
      "Training xgboost\n",
      "[19:20:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[19:20:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.2.0/src/objective/regression_obj.cu:174: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Training catboost\n",
      "Training TabNet\n",
      "epoch 0  | loss: 23.86529| val_0_mae: 26.75467|  0:00:00s\n",
      "epoch 1  | loss: 22.88507| val_0_mae: 21.66933|  0:00:00s\n",
      "epoch 2  | loss: 22.30345| val_0_mae: 20.36256|  0:00:00s\n",
      "epoch 3  | loss: 21.7061 | val_0_mae: 19.5008 |  0:00:00s\n",
      "epoch 4  | loss: 20.91305| val_0_mae: 18.05597|  0:00:01s\n",
      "epoch 5  | loss: 19.88937| val_0_mae: 16.67365|  0:00:01s\n",
      "epoch 6  | loss: 18.61155| val_0_mae: 13.36063|  0:00:01s\n",
      "epoch 7  | loss: 17.00583| val_0_mae: 10.54709|  0:00:01s\n",
      "epoch 8  | loss: 15.08405| val_0_mae: 9.58689 |  0:00:01s\n",
      "epoch 9  | loss: 12.77687| val_0_mae: 9.88172 |  0:00:02s\n",
      "epoch 10 | loss: 10.22466| val_0_mae: 8.31022 |  0:00:02s\n",
      "epoch 11 | loss: 7.32472 | val_0_mae: 19.30497|  0:00:02s\n",
      "epoch 12 | loss: 4.48576 | val_0_mae: 20.21734|  0:00:02s\n",
      "epoch 13 | loss: 3.17819 | val_0_mae: 23.64076|  0:00:02s\n",
      "epoch 14 | loss: 3.40671 | val_0_mae: 24.65117|  0:00:03s\n",
      "epoch 15 | loss: 3.08714 | val_0_mae: 22.45602|  0:00:03s\n",
      "epoch 16 | loss: 2.8872  | val_0_mae: 18.42081|  0:00:03s\n",
      "epoch 17 | loss: 2.73249 | val_0_mae: 16.79545|  0:00:03s\n",
      "epoch 18 | loss: 2.61841 | val_0_mae: 18.2249 |  0:00:03s\n",
      "epoch 19 | loss: 2.49657 | val_0_mae: 14.62082|  0:00:04s\n",
      "epoch 20 | loss: 2.4321  | val_0_mae: 13.98476|  0:00:04s\n",
      "epoch 21 | loss: 2.45279 | val_0_mae: 14.0229 |  0:00:04s\n",
      "epoch 22 | loss: 2.31649 | val_0_mae: 11.74575|  0:00:04s\n",
      "epoch 23 | loss: 2.26246 | val_0_mae: 10.21348|  0:00:04s\n",
      "epoch 24 | loss: 2.14991 | val_0_mae: 9.55689 |  0:00:04s\n",
      "epoch 25 | loss: 2.17571 | val_0_mae: 9.87163 |  0:00:05s\n",
      "epoch 26 | loss: 2.16719 | val_0_mae: 8.46045 |  0:00:05s\n",
      "epoch 27 | loss: 2.23846 | val_0_mae: 7.78766 |  0:00:05s\n",
      "epoch 28 | loss: 2.03425 | val_0_mae: 7.8837  |  0:00:05s\n",
      "epoch 29 | loss: 2.17654 | val_0_mae: 9.8414  |  0:00:05s\n",
      "epoch 30 | loss: 2.09962 | val_0_mae: 7.51101 |  0:00:06s\n",
      "epoch 31 | loss: 2.0827  | val_0_mae: 7.18456 |  0:00:06s\n",
      "epoch 32 | loss: 1.98212 | val_0_mae: 7.57986 |  0:00:06s\n",
      "epoch 33 | loss: 1.92931 | val_0_mae: 8.03889 |  0:00:06s\n",
      "epoch 34 | loss: 1.98446 | val_0_mae: 6.53258 |  0:00:06s\n",
      "epoch 35 | loss: 1.9738  | val_0_mae: 6.77786 |  0:00:07s\n",
      "epoch 36 | loss: 2.00311 | val_0_mae: 6.17564 |  0:00:07s\n",
      "epoch 37 | loss: 1.98736 | val_0_mae: 6.54182 |  0:00:07s\n",
      "epoch 38 | loss: 1.91595 | val_0_mae: 6.72399 |  0:00:07s\n",
      "epoch 39 | loss: 1.93134 | val_0_mae: 6.1109  |  0:00:07s\n",
      "epoch 40 | loss: 2.05062 | val_0_mae: 5.51774 |  0:00:08s\n",
      "epoch 41 | loss: 1.85605 | val_0_mae: 5.78701 |  0:00:08s\n",
      "epoch 42 | loss: 1.96571 | val_0_mae: 4.45968 |  0:00:08s\n",
      "epoch 43 | loss: 1.83341 | val_0_mae: 4.74226 |  0:00:08s\n",
      "epoch 44 | loss: 1.94091 | val_0_mae: 3.58534 |  0:00:08s\n",
      "epoch 45 | loss: 1.87633 | val_0_mae: 4.559   |  0:00:09s\n",
      "epoch 46 | loss: 1.79496 | val_0_mae: 4.09051 |  0:00:09s\n",
      "epoch 47 | loss: 1.79423 | val_0_mae: 3.9958  |  0:00:09s\n",
      "epoch 48 | loss: 1.73316 | val_0_mae: 4.13503 |  0:00:09s\n",
      "epoch 49 | loss: 1.76227 | val_0_mae: 4.32367 |  0:00:09s\n",
      "epoch 50 | loss: 1.78858 | val_0_mae: 3.78735 |  0:00:10s\n",
      "epoch 51 | loss: 1.83678 | val_0_mae: 3.31989 |  0:00:10s\n",
      "epoch 52 | loss: 1.74627 | val_0_mae: 3.32412 |  0:00:10s\n",
      "epoch 53 | loss: 1.83972 | val_0_mae: 3.94975 |  0:00:10s\n",
      "epoch 54 | loss: 1.68337 | val_0_mae: 3.35712 |  0:00:10s\n",
      "epoch 55 | loss: 1.84702 | val_0_mae: 3.74903 |  0:00:11s\n",
      "epoch 56 | loss: 1.77927 | val_0_mae: 3.63797 |  0:00:11s\n",
      "epoch 57 | loss: 1.69013 | val_0_mae: 2.94609 |  0:00:11s\n",
      "epoch 58 | loss: 1.65164 | val_0_mae: 2.94578 |  0:00:11s\n",
      "epoch 59 | loss: 1.69629 | val_0_mae: 3.13982 |  0:00:11s\n",
      "epoch 60 | loss: 1.81973 | val_0_mae: 2.70738 |  0:00:12s\n",
      "epoch 61 | loss: 1.68102 | val_0_mae: 2.97671 |  0:00:12s\n",
      "epoch 62 | loss: 1.60252 | val_0_mae: 2.91662 |  0:00:12s\n",
      "epoch 63 | loss: 1.55098 | val_0_mae: 2.78531 |  0:00:12s\n",
      "epoch 64 | loss: 1.61901 | val_0_mae: 2.228   |  0:00:12s\n",
      "epoch 65 | loss: 1.67143 | val_0_mae: 2.29504 |  0:00:13s\n",
      "epoch 66 | loss: 1.68783 | val_0_mae: 2.25258 |  0:00:13s\n",
      "epoch 67 | loss: 1.58783 | val_0_mae: 2.20446 |  0:00:13s\n",
      "epoch 68 | loss: 1.53804 | val_0_mae: 2.19819 |  0:00:13s\n",
      "epoch 69 | loss: 1.58608 | val_0_mae: 2.56361 |  0:00:13s\n",
      "epoch 70 | loss: 1.53576 | val_0_mae: 2.00445 |  0:00:14s\n",
      "epoch 71 | loss: 1.59931 | val_0_mae: 2.11641 |  0:00:14s\n",
      "epoch 72 | loss: 1.52264 | val_0_mae: 2.55664 |  0:00:14s\n",
      "epoch 73 | loss: 1.52671 | val_0_mae: 2.08367 |  0:00:14s\n",
      "epoch 74 | loss: 1.56277 | val_0_mae: 2.12884 |  0:00:14s\n",
      "epoch 75 | loss: 1.57506 | val_0_mae: 1.89073 |  0:00:15s\n",
      "epoch 76 | loss: 1.54457 | val_0_mae: 1.91107 |  0:00:15s\n",
      "epoch 77 | loss: 1.57603 | val_0_mae: 1.89106 |  0:00:15s\n",
      "epoch 78 | loss: 1.51244 | val_0_mae: 2.24302 |  0:00:15s\n",
      "epoch 79 | loss: 1.44478 | val_0_mae: 1.65383 |  0:00:15s\n",
      "epoch 80 | loss: 1.4888  | val_0_mae: 1.79461 |  0:00:16s\n",
      "epoch 81 | loss: 1.47081 | val_0_mae: 1.86652 |  0:00:16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82 | loss: 1.39308 | val_0_mae: 1.64761 |  0:00:16s\n",
      "epoch 83 | loss: 1.57265 | val_0_mae: 1.80698 |  0:00:16s\n",
      "epoch 84 | loss: 1.52312 | val_0_mae: 1.80363 |  0:00:16s\n",
      "epoch 85 | loss: 1.5166  | val_0_mae: 2.07074 |  0:00:17s\n",
      "epoch 86 | loss: 1.50842 | val_0_mae: 1.82598 |  0:00:17s\n",
      "epoch 87 | loss: 1.42281 | val_0_mae: 1.78481 |  0:00:17s\n",
      "epoch 88 | loss: 1.49789 | val_0_mae: 1.95925 |  0:00:17s\n",
      "epoch 89 | loss: 1.51717 | val_0_mae: 1.67806 |  0:00:17s\n",
      "epoch 90 | loss: 1.42859 | val_0_mae: 1.50202 |  0:00:18s\n",
      "epoch 91 | loss: 1.53976 | val_0_mae: 1.77604 |  0:00:18s\n",
      "epoch 92 | loss: 1.41454 | val_0_mae: 1.60058 |  0:00:18s\n",
      "epoch 93 | loss: 1.44079 | val_0_mae: 1.72213 |  0:00:18s\n",
      "epoch 94 | loss: 1.50996 | val_0_mae: 1.50526 |  0:00:18s\n",
      "epoch 95 | loss: 1.51539 | val_0_mae: 1.632   |  0:00:19s\n",
      "epoch 96 | loss: 1.67967 | val_0_mae: 1.52282 |  0:00:19s\n",
      "epoch 97 | loss: 1.5476  | val_0_mae: 1.45723 |  0:00:19s\n",
      "epoch 98 | loss: 1.58044 | val_0_mae: 1.5312  |  0:00:19s\n",
      "epoch 99 | loss: 1.48039 | val_0_mae: 1.66669 |  0:00:19s\n",
      "epoch 100| loss: 1.52394 | val_0_mae: 1.69315 |  0:00:20s\n",
      "epoch 101| loss: 1.43076 | val_0_mae: 1.43045 |  0:00:20s\n",
      "epoch 102| loss: 1.53269 | val_0_mae: 1.58697 |  0:00:20s\n",
      "epoch 103| loss: 1.50936 | val_0_mae: 1.57584 |  0:00:20s\n",
      "epoch 104| loss: 1.58841 | val_0_mae: 1.52845 |  0:00:20s\n",
      "epoch 105| loss: 1.55443 | val_0_mae: 1.6219  |  0:00:21s\n",
      "epoch 106| loss: 1.4679  | val_0_mae: 1.70489 |  0:00:21s\n",
      "epoch 107| loss: 1.43988 | val_0_mae: 1.55095 |  0:00:21s\n",
      "epoch 108| loss: 1.43928 | val_0_mae: 1.59065 |  0:00:21s\n",
      "epoch 109| loss: 1.49982 | val_0_mae: 1.47587 |  0:00:21s\n",
      "epoch 110| loss: 1.44259 | val_0_mae: 1.56203 |  0:00:22s\n",
      "epoch 111| loss: 1.44668 | val_0_mae: 1.48837 |  0:00:22s\n",
      "epoch 112| loss: 1.59217 | val_0_mae: 1.50554 |  0:00:22s\n",
      "epoch 113| loss: 1.53538 | val_0_mae: 1.61145 |  0:00:22s\n",
      "epoch 114| loss: 1.70763 | val_0_mae: 1.64689 |  0:00:22s\n",
      "epoch 115| loss: 1.62292 | val_0_mae: 1.51348 |  0:00:23s\n",
      "epoch 116| loss: 1.53704 | val_0_mae: 1.69166 |  0:00:23s\n",
      "epoch 117| loss: 1.5894  | val_0_mae: 1.55426 |  0:00:23s\n",
      "epoch 118| loss: 1.47209 | val_0_mae: 1.67932 |  0:00:23s\n",
      "epoch 119| loss: 1.42228 | val_0_mae: 1.4848  |  0:00:23s\n",
      "epoch 120| loss: 1.51253 | val_0_mae: 1.46858 |  0:00:24s\n",
      "epoch 121| loss: 1.53526 | val_0_mae: 1.58114 |  0:00:24s\n",
      "epoch 122| loss: 1.4937  | val_0_mae: 1.44243 |  0:00:24s\n",
      "epoch 123| loss: 1.45331 | val_0_mae: 1.58817 |  0:00:24s\n",
      "epoch 124| loss: 1.4825  | val_0_mae: 1.58777 |  0:00:24s\n",
      "epoch 125| loss: 1.53612 | val_0_mae: 1.52106 |  0:00:25s\n",
      "epoch 126| loss: 1.55338 | val_0_mae: 1.53429 |  0:00:25s\n",
      "epoch 127| loss: 1.4154  | val_0_mae: 1.58147 |  0:00:25s\n",
      "epoch 128| loss: 1.62499 | val_0_mae: 1.4395  |  0:00:25s\n",
      "epoch 129| loss: 1.5255  | val_0_mae: 1.58628 |  0:00:25s\n",
      "epoch 130| loss: 1.47307 | val_0_mae: 1.47056 |  0:00:26s\n",
      "epoch 131| loss: 1.49987 | val_0_mae: 1.43669 |  0:00:26s\n",
      "epoch 132| loss: 1.44191 | val_0_mae: 1.5076  |  0:00:26s\n",
      "epoch 133| loss: 1.43976 | val_0_mae: 1.52469 |  0:00:26s\n",
      "epoch 134| loss: 1.63391 | val_0_mae: 1.67162 |  0:00:26s\n",
      "epoch 135| loss: 1.5635  | val_0_mae: 1.4465  |  0:00:27s\n",
      "epoch 136| loss: 1.5716  | val_0_mae: 1.71626 |  0:00:27s\n",
      "epoch 137| loss: 1.53015 | val_0_mae: 1.53855 |  0:00:27s\n",
      "epoch 138| loss: 1.47599 | val_0_mae: 1.71713 |  0:00:27s\n",
      "epoch 139| loss: 1.53514 | val_0_mae: 1.5196  |  0:00:27s\n",
      "epoch 140| loss: 1.51689 | val_0_mae: 1.59459 |  0:00:28s\n",
      "epoch 141| loss: 1.42989 | val_0_mae: 1.57518 |  0:00:28s\n",
      "epoch 142| loss: 1.5874  | val_0_mae: 1.54439 |  0:00:28s\n",
      "epoch 143| loss: 1.48903 | val_0_mae: 1.49901 |  0:00:28s\n",
      "epoch 144| loss: 1.43931 | val_0_mae: 1.45465 |  0:00:28s\n",
      "epoch 145| loss: 1.4706  | val_0_mae: 1.41704 |  0:00:29s\n",
      "epoch 146| loss: 1.38231 | val_0_mae: 1.4701  |  0:00:29s\n",
      "epoch 147| loss: 1.40852 | val_0_mae: 1.4238  |  0:00:29s\n",
      "epoch 148| loss: 1.38422 | val_0_mae: 1.43439 |  0:00:29s\n",
      "epoch 149| loss: 1.38865 | val_0_mae: 1.50029 |  0:00:29s\n",
      "epoch 150| loss: 1.34814 | val_0_mae: 1.40789 |  0:00:30s\n",
      "epoch 151| loss: 1.36659 | val_0_mae: 1.64965 |  0:00:30s\n",
      "epoch 152| loss: 1.34653 | val_0_mae: 1.52031 |  0:00:30s\n",
      "epoch 153| loss: 1.61053 | val_0_mae: 1.57984 |  0:00:30s\n",
      "epoch 154| loss: 1.46647 | val_0_mae: 1.75256 |  0:00:30s\n",
      "epoch 155| loss: 1.55382 | val_0_mae: 1.49696 |  0:00:31s\n",
      "epoch 156| loss: 1.57972 | val_0_mae: 1.43651 |  0:00:31s\n",
      "epoch 157| loss: 1.49239 | val_0_mae: 1.67727 |  0:00:31s\n",
      "epoch 158| loss: 1.50378 | val_0_mae: 1.58888 |  0:00:31s\n",
      "epoch 159| loss: 1.42009 | val_0_mae: 1.4228  |  0:00:31s\n",
      "epoch 160| loss: 1.46648 | val_0_mae: 1.41413 |  0:00:31s\n",
      "epoch 161| loss: 1.38266 | val_0_mae: 1.54976 |  0:00:32s\n",
      "epoch 162| loss: 1.40581 | val_0_mae: 1.54703 |  0:00:32s\n",
      "epoch 163| loss: 1.50736 | val_0_mae: 1.49785 |  0:00:32s\n",
      "epoch 164| loss: 1.59643 | val_0_mae: 1.476   |  0:00:32s\n",
      "epoch 165| loss: 1.49378 | val_0_mae: 1.4914  |  0:00:32s\n",
      "epoch 166| loss: 1.70435 | val_0_mae: 1.57177 |  0:00:33s\n",
      "epoch 167| loss: 1.44611 | val_0_mae: 1.60547 |  0:00:33s\n",
      "epoch 168| loss: 1.42072 | val_0_mae: 1.52777 |  0:00:33s\n",
      "epoch 169| loss: 1.78335 | val_0_mae: 1.66665 |  0:00:33s\n",
      "epoch 170| loss: 1.62721 | val_0_mae: 1.65691 |  0:00:33s\n",
      "epoch 171| loss: 1.63404 | val_0_mae: 1.46928 |  0:00:34s\n",
      "epoch 172| loss: 1.51115 | val_0_mae: 1.65116 |  0:00:34s\n",
      "epoch 173| loss: 1.53529 | val_0_mae: 1.56185 |  0:00:34s\n",
      "epoch 174| loss: 1.59809 | val_0_mae: 1.48451 |  0:00:34s\n",
      "epoch 175| loss: 1.44766 | val_0_mae: 1.62821 |  0:00:34s\n",
      "epoch 176| loss: 1.4078  | val_0_mae: 1.47256 |  0:00:35s\n",
      "epoch 177| loss: 1.45752 | val_0_mae: 1.53522 |  0:00:35s\n",
      "epoch 178| loss: 1.58042 | val_0_mae: 1.62955 |  0:00:35s\n",
      "epoch 179| loss: 1.38956 | val_0_mae: 1.45431 |  0:00:35s\n",
      "epoch 180| loss: 1.46655 | val_0_mae: 1.39376 |  0:00:35s\n",
      "epoch 181| loss: 1.3468  | val_0_mae: 1.3957  |  0:00:36s\n",
      "epoch 182| loss: 1.41996 | val_0_mae: 1.52909 |  0:00:36s\n",
      "epoch 183| loss: 1.3963  | val_0_mae: 1.45271 |  0:00:36s\n",
      "epoch 184| loss: 1.36901 | val_0_mae: 1.36995 |  0:00:36s\n",
      "epoch 185| loss: 1.41287 | val_0_mae: 1.47295 |  0:00:36s\n",
      "epoch 186| loss: 1.39997 | val_0_mae: 1.55277 |  0:00:37s\n",
      "epoch 187| loss: 1.37562 | val_0_mae: 1.44113 |  0:00:37s\n",
      "epoch 188| loss: 1.3494  | val_0_mae: 1.51853 |  0:00:37s\n",
      "epoch 189| loss: 1.40524 | val_0_mae: 1.41221 |  0:00:37s\n",
      "epoch 190| loss: 1.4483  | val_0_mae: 1.41327 |  0:00:37s\n",
      "epoch 191| loss: 1.44038 | val_0_mae: 1.41578 |  0:00:38s\n",
      "epoch 192| loss: 1.35385 | val_0_mae: 1.47841 |  0:00:38s\n",
      "epoch 193| loss: 1.4424  | val_0_mae: 1.42414 |  0:00:38s\n",
      "epoch 194| loss: 1.34335 | val_0_mae: 1.3658  |  0:00:38s\n",
      "epoch 195| loss: 1.35311 | val_0_mae: 1.46133 |  0:00:38s\n",
      "epoch 196| loss: 1.33017 | val_0_mae: 1.39266 |  0:00:39s\n",
      "epoch 197| loss: 1.46903 | val_0_mae: 1.49022 |  0:00:39s\n",
      "epoch 198| loss: 1.38739 | val_0_mae: 1.37793 |  0:00:39s\n",
      "epoch 199| loss: 1.42437 | val_0_mae: 1.42338 |  0:00:39s\n",
      "epoch 200| loss: 1.39718 | val_0_mae: 1.39145 |  0:00:39s\n",
      "epoch 201| loss: 1.38718 | val_0_mae: 1.40314 |  0:00:40s\n",
      "epoch 202| loss: 1.38046 | val_0_mae: 1.47787 |  0:00:40s\n",
      "epoch 203| loss: 1.43448 | val_0_mae: 1.47234 |  0:00:40s\n",
      "epoch 204| loss: 1.40129 | val_0_mae: 1.41434 |  0:00:40s\n",
      "epoch 205| loss: 1.38356 | val_0_mae: 1.4456  |  0:00:40s\n",
      "epoch 206| loss: 1.41004 | val_0_mae: 1.40315 |  0:00:41s\n",
      "epoch 207| loss: 1.35834 | val_0_mae: 1.64914 |  0:00:41s\n",
      "epoch 208| loss: 1.51815 | val_0_mae: 1.74292 |  0:00:41s\n",
      "epoch 209| loss: 1.40049 | val_0_mae: 1.4169  |  0:00:41s\n",
      "epoch 210| loss: 1.47057 | val_0_mae: 1.63187 |  0:00:41s\n",
      "epoch 211| loss: 1.57998 | val_0_mae: 1.42757 |  0:00:42s\n",
      "epoch 212| loss: 1.37902 | val_0_mae: 1.46854 |  0:00:42s\n",
      "epoch 213| loss: 1.43033 | val_0_mae: 1.57322 |  0:00:42s\n",
      "epoch 214| loss: 1.35436 | val_0_mae: 1.48507 |  0:00:42s\n",
      "epoch 215| loss: 1.43775 | val_0_mae: 1.54516 |  0:00:42s\n",
      "epoch 216| loss: 1.52727 | val_0_mae: 1.53692 |  0:00:43s\n",
      "epoch 217| loss: 1.35893 | val_0_mae: 1.43608 |  0:00:43s\n",
      "epoch 218| loss: 1.37464 | val_0_mae: 1.44511 |  0:00:43s\n",
      "epoch 219| loss: 1.31169 | val_0_mae: 1.52758 |  0:00:43s\n",
      "epoch 220| loss: 1.41087 | val_0_mae: 1.43968 |  0:00:43s\n",
      "epoch 221| loss: 1.45218 | val_0_mae: 1.62213 |  0:00:44s\n",
      "epoch 222| loss: 1.3394  | val_0_mae: 1.50278 |  0:00:44s\n",
      "epoch 223| loss: 1.3773  | val_0_mae: 1.45353 |  0:00:44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 224| loss: 1.35147 | val_0_mae: 1.48563 |  0:00:44s\n",
      "epoch 225| loss: 1.40088 | val_0_mae: 1.66367 |  0:00:44s\n",
      "epoch 226| loss: 1.63903 | val_0_mae: 1.90657 |  0:00:44s\n",
      "epoch 227| loss: 1.43629 | val_0_mae: 1.66951 |  0:00:45s\n",
      "epoch 228| loss: 1.44589 | val_0_mae: 1.48735 |  0:00:45s\n",
      "epoch 229| loss: 1.41744 | val_0_mae: 1.44452 |  0:00:45s\n",
      "epoch 230| loss: 1.44204 | val_0_mae: 1.4173  |  0:00:45s\n",
      "epoch 231| loss: 1.42981 | val_0_mae: 1.41251 |  0:00:45s\n",
      "epoch 232| loss: 1.35106 | val_0_mae: 1.59074 |  0:00:46s\n",
      "epoch 233| loss: 1.35329 | val_0_mae: 1.57337 |  0:00:46s\n",
      "epoch 234| loss: 1.38538 | val_0_mae: 1.51191 |  0:00:46s\n",
      "epoch 235| loss: 1.37456 | val_0_mae: 1.45224 |  0:00:46s\n",
      "epoch 236| loss: 1.38847 | val_0_mae: 1.45116 |  0:00:46s\n",
      "epoch 237| loss: 1.36349 | val_0_mae: 1.45407 |  0:00:47s\n",
      "epoch 238| loss: 1.34568 | val_0_mae: 1.50101 |  0:00:47s\n",
      "epoch 239| loss: 1.31456 | val_0_mae: 1.48201 |  0:00:47s\n",
      "epoch 240| loss: 1.51422 | val_0_mae: 1.54792 |  0:00:47s\n",
      "epoch 241| loss: 1.51483 | val_0_mae: 1.48494 |  0:00:47s\n",
      "epoch 242| loss: 1.43247 | val_0_mae: 1.45419 |  0:00:48s\n",
      "epoch 243| loss: 1.46587 | val_0_mae: 1.38085 |  0:00:48s\n",
      "epoch 244| loss: 1.33085 | val_0_mae: 1.39288 |  0:00:48s\n",
      "\n",
      "Early stopping occured at epoch 244 with best_epoch = 194 and best_val_0_mae = 1.3658\n",
      "Best weights from best epoch are automatically used!\n",
      "XGB: 1.4070924840944625\n",
      "CAT: 1.4223215657399004\n",
      "TAB: 1.3657978561242894\n",
      "MHA: 2.476705462200903\n",
      "CNN_TINY: 3.284694757674946\n",
      "CNN_BIG: 2.4551889471218917\n",
      "MEAN MHA+TINY+BIG: 2.033015138954099\n",
      "MEAN XGB+CAT+TAB: 1.348857868123806\n",
      "****************************************************************************************************\n",
      "XGB Score: 1.329, CAT Score: 1.3528, TAB Score: 1.3179, ENSEM Score: 1.2851\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# 7. Training L2 Stack Models\n",
    "\n",
    "save = True\n",
    "train_columns = [col for col in df_train_l2.columns if col not in ['y_true', 'segment_id']]\n",
    "list_models_xgb, list_models_cat, list_models_tab = [], [], []\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=12)\n",
    "list_segments_train = list(unique_segments_id_train)\n",
    "dict_predictions = {'segment_id' : [], 'y_val_true' : [], \n",
    "                    'y_val_xgb_pred': [], 'y_val_cat_pred': [],  'y_val_tab_pred': []}\n",
    "\n",
    "for num_fold, (train_index, val_index) in enumerate(kf.split(df_train_l2,\n",
    "                                                     np.zeros(len(list_segments_train)))):\n",
    "    \n",
    "    X_train, y_train = df_train_l2[train_columns].loc[train_index], df_train_l2['y_true'].loc[train_index]\n",
    "    X_val, y_val = df_train_l2[train_columns].loc[val_index], df_train_l2['y_true'].loc[val_index]\n",
    "    \n",
    "    print('=='*30)\n",
    "    print(f'Num Fold: {num_fold + 1}')\n",
    "    print(f'Train segments: {len(train_index)} Val segments: {len(val_index)}')\n",
    "    print('=='*30)\n",
    "    \n",
    "    model_xgb = xgb.XGBRegressor(objective='reg:linear', booster='gbtree', tree_method='gpu_hist', #booster=gblinear\n",
    "                                 learning_rate=0.05, max_depth=4, n_estimators=200, \n",
    "                                 alpha=0.5, gamma=2.0,\n",
    "                                 subsample=0.8, colsample_bytree=0.9, min_child_weight=0.0,\n",
    "                                 nthread=-1, \n",
    "                                 seed=12)\n",
    "    \n",
    "    model_cat = catboost.CatBoostRegressor(iterations=500, learning_rate=0.05, depth=5, \n",
    "                                 eval_metric='MAE', colsample_bylevel=0.7, bagging_temperature=0.2,\n",
    "                                 early_stopping_rounds=100, l2_leaf_reg=10,\n",
    "                                 random_seed=12)\n",
    "    \n",
    "    model_tabnet = TabNetRegressor()\n",
    "    \n",
    "    ######\n",
    "    print('Training xgboost')\n",
    "    model_xgb.fit(X_train, y_train, verbose=False) \n",
    "    y_val_xgb_pred = model_xgb.predict(X_val)\n",
    "    print('Training catboost')\n",
    "    model_cat.fit(X_train, y_train, verbose=False) \n",
    "    y_val_cat_pred = model_cat.predict(X_val)\n",
    "    print('Training TabNet')\n",
    "    model_tabnet.fit(\n",
    "      X_train.values, \n",
    "      np.expand_dims(y_train.values, -1),\n",
    "      eval_set=[(X_val.values, np.expand_dims(y_val.values, -1))],\n",
    "      patience=50,\n",
    "      max_epochs=250,\n",
    "      eval_metric=[TabnetMAE],\n",
    "      loss_fn=torch.nn.L1Loss()\n",
    "    )\n",
    "    y_val_tab_pred = model_tabnet.predict(X_val.values).squeeze()\n",
    "    ##\n",
    "    X_val['y_true'] = y_val\n",
    "    X_val['tab'] = y_val_tab_pred\n",
    "    X_val['xgb'] = y_val_xgb_pred\n",
    "    X_val['cat'] = y_val_cat_pred\n",
    "    X_val = X_val.reset_index(drop=True)\n",
    "    ######\n",
    "\n",
    "    print(f\"XGB: {np.mean(np.abs(X_val['y_true'] - X_val['xgb']))}\")\n",
    "    print(f\"CAT: {np.mean(np.abs(X_val['y_true'] - X_val['cat']))}\")\n",
    "    print(f\"TAB: {np.mean(np.abs(X_val['y_true'] - X_val['tab']))}\")\n",
    "    print(f\"MHA: {np.mean(np.abs(X_val['y_true'] - X_val['mha_pred_mean']))}\")\n",
    "    print(f\"CNN_TINY: {np.mean(np.abs(X_val['y_true'] - X_val['cnn_tiny_pred_mean']))}\")\n",
    "    print(f\"CNN_BIG: {np.mean(np.abs(X_val['y_true'] - X_val['cnn_big_pred_mean']))}\")\n",
    "    print(f\"MEAN MHA+TINY+BIG: {np.mean(np.abs(X_val['y_true'] - ((X_val['mha_pred_mean']+X_val['cnn_tiny_pred_mean']+X_val['cnn_big_pred_mean'])/3)))}\")\n",
    "    print(f\"MEAN XGB+CAT+TAB: {np.mean(np.abs(X_val['y_true'] - ((X_val['xgb'] + X_val['cat'] + X_val['tab'])/3)))}\")\n",
    "    \n",
    "    dict_predictions['segment_id'].append(list(df_train_l2['segment_id'].loc[val_index]))\n",
    "    dict_predictions['y_val_true'].append(list(df_train_l2['y_true'].loc[val_index]))\n",
    "    dict_predictions['y_val_xgb_pred'].append(y_val_xgb_pred)\n",
    "    dict_predictions['y_val_cat_pred'].append(y_val_cat_pred)\n",
    "    dict_predictions['y_val_tab_pred'].append(y_val_tab_pred)\n",
    "    \n",
    "    list_models_xgb.append(model_xgb)\n",
    "    list_models_cat.append(model_cat)\n",
    "    list_models_tab.append(model_tabnet)\n",
    "    print('**********'*10)\n",
    "\n",
    "df_cv = pd.DataFrame({\n",
    "    'segment_id' : np.concatenate(dict_predictions['segment_id']),\n",
    "    'y_val_true' : np.concatenate(dict_predictions['y_val_true']),\n",
    "    'y_val_xgb_pred' : np.concatenate(dict_predictions['y_val_xgb_pred']),\n",
    "    'y_val_cat_pred' : np.concatenate(dict_predictions['y_val_cat_pred']),\n",
    "    'y_val_tab_pred' : np.concatenate(dict_predictions['y_val_tab_pred'])\n",
    "})\n",
    "\n",
    "val_xgb_score = np.round(np.mean(np.abs(df_cv['y_val_true'] - df_cv['y_val_xgb_pred'])), 4)\n",
    "val_cat_score = np.round(np.mean(np.abs(df_cv['y_val_true'] - df_cv['y_val_cat_pred'])), 4)\n",
    "val_tab_score = np.round(np.mean(np.abs(df_cv['y_val_true'] - df_cv['y_val_tab_pred'])), 4)\n",
    "val_ensem_score = np.round(np.mean(np.abs(df_cv['y_val_true'] - ((df_cv['y_val_xgb_pred'] + df_cv['y_val_cat_pred'] + df_cv['y_val_tab_pred'])/3))), 4)\n",
    "\n",
    "print(f\"XGB Score: {val_xgb_score}, CAT Score: {val_cat_score}, TAB Score: {val_tab_score}, ENSEM Score: {val_ensem_score}\")\n",
    "print('**********'*10)\n",
    "\n",
    "if save:\n",
    "    experiment_name = f\"{val_ensem_score}_{datetime.today().strftime('%d-%m-%Y-%H-%M-%S')}\"\n",
    "    path = f'./StackedModels/{experiment_name}'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        for fold, model in enumerate(zip(list_models_xgb, list_models_cat, list_models_tab)):\n",
    "            pickle.dump(model[0], open(f'{path}/xgb_{fold}.pickle', 'wb')) \n",
    "            pickle.dump(model[1], open(f'{path}/cat_{fold}.pickle', 'wb'))\n",
    "            torch.save(model[2], f'{path}/tabnet_{fold}')\n",
    "\n",
    "# v0.1 - Without conf\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.9784, CAT Score: 1.9637, ENSEM Score: 1.9602\n",
    "# ****************************************************************************************************\n",
    "\n",
    "# v0.2 - With conf\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.9357, CAT Score: 1.904, ENSEM Score: 1.9069\n",
    "# ****************************************************************************************************\n",
    "\n",
    "# v0.3 - With conf & wavenet with test stats\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.9537, CAT Score: 1.9416, ENSEM Score: 1.9339\n",
    "# ****************************************************************************************************\n",
    "\n",
    "# v0.4 - With stft\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.9531, CAT Score: 1.9438, ENSEM Score: 1.9325\n",
    "# ****************************************************************************************************\n",
    "\n",
    "# v0.5 - With lgbm stft\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.7674, CAT Score: 1.7744, ENSEM Score: 1.7552\n",
    "# ****************************************************************************************************\n",
    "\n",
    "# v0.6 - Add tabnet\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.7766, CAT Score: 1.7809, TAB Score: 1.744, ENSEM Score: 1.7143\n",
    "# ****************************************************************************************************\n",
    "\n",
    "# v0.7 - Updated Models with new tta\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.5708, CAT Score: 1.5869, TAB Score: 1.6282, ENSEM Score: 1.5391\n",
    "# ****************************************************************************************************\n",
    "\n",
    "# v0.8 - Updated Models with new tta + predictions tabular\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.3259, CAT Score: 1.3528, TAB Score: 1.3179, ENSEM Score: 1.2849\n",
    "# ****************************************************************************************************\n",
    "\n",
    "# v0.9 - Updated Models with new tta + predictions tabular + hipertune xgb\n",
    "# ****************************************************************************************************\n",
    "# XGB Score: 1.329, CAT Score: 1.3528, TAB Score: 1.3179, ENSEM Score: 1.2851\n",
    "# ****************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
